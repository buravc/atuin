diff --git a/Cargo.lock b/Cargo.lock
index 5688d2dc..0ad597a6 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -27,18 +27,6 @@ dependencies = [
  "generic-array",
 ]
 
-[[package]]
-name = "ahash"
-version = "0.8.11"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e89da841a80418a9b391ebaea17f5c112ffaaa96f621d2c285b5174da76b9011"
-dependencies = [
- "cfg-if",
- "once_cell",
- "version_check",
- "zerocopy 0.7.35",
-]
-
 [[package]]
 name = "aho-corasick"
 version = "1.1.3"
@@ -153,12 +141,6 @@ dependencies = [
  "x11rb",
 ]
 
-[[package]]
-name = "arc-swap"
-version = "1.7.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "69f7f8c3906b62b754cd5326047894316021dcfe5a194c8ea52bdd94934a3457"
-
 [[package]]
 name = "argon2"
 version = "0.5.3"
@@ -232,8 +214,6 @@ dependencies = [
  "atuin-history",
  "atuin-kv",
  "atuin-scripts",
- "atuin-server",
- "atuin-server-postgres",
  "clap",
  "clap_complete",
  "clap_complete_nushell",
@@ -294,9 +274,9 @@ dependencies = [
  "minspan",
  "palette",
  "pretty_assertions",
- "rand 0.8.5",
+ "rand",
  "regex",
- "reqwest 0.11.27",
+ "reqwest",
  "rmp",
  "rusty_paserk",
  "rusty_paseto",
@@ -356,7 +336,7 @@ dependencies = [
  "prost",
  "prost-types",
  "protox",
- "rand 0.8.5",
+ "rand",
  "time",
  "tokio",
  "tokio-stream",
@@ -377,7 +357,7 @@ dependencies = [
  "atuin-common",
  "crypto_secretbox",
  "eyre",
- "rand 0.8.5",
+ "rand",
  "rmp",
  "serde",
  "tokio",
@@ -390,7 +370,7 @@ dependencies = [
  "atuin-client",
  "crossterm",
  "divan",
- "rand 0.8.5",
+ "rand",
  "serde",
  "time",
  "unicode-segmentation",
@@ -434,65 +414,6 @@ dependencies = [
  "uuid",
 ]
 
-[[package]]
-name = "atuin-server"
-version = "18.6.1"
-dependencies = [
- "argon2",
- "async-trait",
- "atuin-common",
- "atuin-server-database",
- "axum",
- "axum-server",
- "config",
- "eyre",
- "fs-err",
- "metrics",
- "metrics-exporter-prometheus",
- "postmark",
- "rand 0.8.5",
- "reqwest 0.11.27",
- "rustls 0.23.25",
- "semver",
- "serde",
- "serde_json",
- "time",
- "tokio",
- "tower 0.5.2",
- "tower-http",
- "tracing",
-]
-
-[[package]]
-name = "atuin-server-database"
-version = "18.6.1"
-dependencies = [
- "async-trait",
- "atuin-common",
- "eyre",
- "serde",
- "time",
- "tracing",
-]
-
-[[package]]
-name = "atuin-server-postgres"
-version = "18.6.1"
-dependencies = [
- "async-trait",
- "atuin-common",
- "atuin-server-database",
- "eyre",
- "futures-util",
- "metrics",
- "serde",
- "sqlx",
- "time",
- "tracing",
- "url",
- "uuid",
-]
-
 [[package]]
 name = "autocfg"
 version = "1.4.0"
@@ -512,8 +433,6 @@ dependencies = [
  "http 1.3.1",
  "http-body 1.0.1",
  "http-body-util",
- "hyper 1.6.0",
- "hyper-util",
  "itoa",
  "matchit",
  "memchr",
@@ -522,15 +441,10 @@ dependencies = [
  "pin-project-lite",
  "rustversion",
  "serde",
- "serde_json",
- "serde_path_to_error",
- "serde_urlencoded",
  "sync_wrapper 1.0.2",
- "tokio",
  "tower 0.5.2",
  "tower-layer",
  "tower-service",
- "tracing",
 ]
 
 [[package]]
@@ -551,29 +465,6 @@ dependencies = [
  "sync_wrapper 1.0.2",
  "tower-layer",
  "tower-service",
- "tracing",
-]
-
-[[package]]
-name = "axum-server"
-version = "0.7.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "495c05f60d6df0093e8fb6e74aa5846a0ad06abaf96d76166283720bf740f8ab"
-dependencies = [
- "arc-swap",
- "bytes",
- "fs-err",
- "http 1.3.1",
- "http-body 1.0.1",
- "hyper 1.6.0",
- "hyper-util",
- "pin-project-lite",
- "rustls 0.23.25",
- "rustls-pemfile 2.2.0",
- "rustls-pki-types",
- "tokio",
- "tokio-rustls 0.26.2",
- "tower-service",
 ]
 
 [[package]]
@@ -729,12 +620,6 @@ version = "0.1.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "fd16c4719339c4530435d38e511904438d07cce7950afa3718a84ac36c10e89e"
 
-[[package]]
-name = "cfg_aliases"
-version = "0.2.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "613afe47fcd5fac7ccf1db93babcb082c5994d996f20b8b159f2ad1658eb5724"
-
 [[package]]
 name = "chacha20"
 version = "0.9.1"
@@ -1056,7 +941,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1bfb12502f3fc46cca1bb51ac28df9d618d813cdc3d2f25b9fe775a34af26bb3"
 dependencies = [
  "generic-array",
- "rand_core 0.6.4",
+ "rand_core",
  "typenum",
 ]
 
@@ -1506,7 +1391,6 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1f89bda4c2a21204059a977ed3bfe746677dfd137b83c339e702b0ac91d482aa"
 dependencies = [
  "autocfg",
- "tokio",
 ]
 
 [[package]]
@@ -1647,10 +1531,8 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "c4567c8db10ae91089c99af84c68c38da3ec2f087c3f82960bcdbf3656b6f4d7"
 dependencies = [
  "cfg-if",
- "js-sys",
  "libc",
  "wasi 0.11.0+wasi-snapshot-preview1",
- "wasm-bindgen",
 ]
 
 [[package]]
@@ -1660,11 +1542,9 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "73fea8450eea4bac3940448fb7ae50d91f034f941199fcd9d909a5a07aa455f0"
 dependencies = [
  "cfg-if",
- "js-sys",
  "libc",
  "r-efi",
  "wasi 0.14.2+wasi-0.2.4",
- "wasm-bindgen",
 ]
 
 [[package]]
@@ -1717,15 +1597,6 @@ version = "0.12.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8a9ee70c43aaf417c914396645a0fa852624801b24ebb7ae78fe8272889ac888"
 
-[[package]]
-name = "hashbrown"
-version = "0.13.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "33ff8ae62cd3a9102e5637afc8452c55acf3844001bd5374e0b0bd7b6616c038"
-dependencies = [
- "ahash",
-]
-
 [[package]]
 name = "hashbrown"
 version = "0.14.5"
@@ -1758,12 +1629,6 @@ version = "0.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2304e00983f87ffb38b55b444b5e3b60a884b5d30c0fca7d82fe33449bbe55ea"
 
-[[package]]
-name = "hermit-abi"
-version = "0.3.9"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d231dfb89cfffdbc30e7fc41579ed6066ad03abda9e567ccafae602b97ec5024"
-
 [[package]]
 name = "hex"
 version = "0.4.3"
@@ -1927,25 +1792,7 @@ dependencies = [
  "hyper 0.14.32",
  "rustls 0.21.12",
  "tokio",
- "tokio-rustls 0.24.1",
-]
-
-[[package]]
-name = "hyper-rustls"
-version = "0.27.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2d191583f3da1305256f22463b9bb0471acad48a4e534a5218b9963e9c1f59b2"
-dependencies = [
- "futures-util",
- "http 1.3.1",
- "hyper 1.6.0",
- "hyper-util",
- "rustls 0.23.25",
- "rustls-pki-types",
- "tokio",
- "tokio-rustls 0.26.2",
- "tower-service",
- "webpki-roots",
+ "tokio-rustls",
 ]
 
 [[package]]
@@ -2431,15 +2278,6 @@ dependencies = [
  "hashbrown 0.15.2",
 ]
 
-[[package]]
-name = "mach2"
-version = "0.4.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "19b955cdeb2a02b9117f121ce63aa52d08ade45de53e48fe6a38b39c10f6f709"
-dependencies = [
- "libc",
-]
-
 [[package]]
 name = "matchers"
 version = "0.1.0"
@@ -2471,61 +2309,6 @@ version = "2.7.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "78ca9ab1a0babb1e7d5695e3530886289c18cf2f87ec19a575a0abdce112e3a3"
 
-[[package]]
-name = "metrics"
-version = "0.21.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "fde3af1a009ed76a778cb84fdef9e7dbbdf5775ae3e4cc1f434a6a307f6f76c5"
-dependencies = [
- "ahash",
- "metrics-macros",
- "portable-atomic",
-]
-
-[[package]]
-name = "metrics-exporter-prometheus"
-version = "0.12.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1d4fa7ce7c4862db464a37b0b31d89bca874562f034bd7993895572783d02950"
-dependencies = [
- "base64 0.21.7",
- "hyper 0.14.32",
- "indexmap 1.9.3",
- "ipnet",
- "metrics",
- "metrics-util",
- "quanta",
- "thiserror 1.0.69",
- "tokio",
- "tracing",
-]
-
-[[package]]
-name = "metrics-macros"
-version = "0.7.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "38b4faf00617defe497754acde3024865bc143d44a86799b24e191ecff91354f"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn",
-]
-
-[[package]]
-name = "metrics-util"
-version = "0.15.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4de2ed6e491ed114b40b732e4d1659a9d53992ebd87490c44a6ffe23739d973e"
-dependencies = [
- "crossbeam-epoch",
- "crossbeam-utils",
- "hashbrown 0.13.1",
- "metrics",
- "num_cpus",
- "quanta",
- "sketches-ddsketch",
-]
-
 [[package]]
 name = "miette"
 version = "7.5.0"
@@ -2612,7 +2395,7 @@ checksum = "ab2156c4fce2f8df6c499cc1c763e4394b7482525bf2a9701c9d79d215f519e4"
 dependencies = [
  "bitflags 2.9.0",
  "cfg-if",
- "cfg_aliases 0.1.1",
+ "cfg_aliases",
  "libc",
 ]
 
@@ -2675,7 +2458,7 @@ dependencies = [
  "num-integer",
  "num-iter",
  "num-traits",
- "rand 0.8.5",
+ "rand",
  "smallvec",
  "zeroize",
 ]
@@ -2716,16 +2499,6 @@ dependencies = [
  "libm",
 ]
 
-[[package]]
-name = "num_cpus"
-version = "1.16.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4161fcb6d602d4d2081af7c3a45852d875a03dd337a6bfdd6e06407b61342a43"
-dependencies = [
- "hermit-abi",
- "libc",
-]
-
 [[package]]
 name = "num_threads"
 version = "0.1.7"
@@ -2950,7 +2723,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "346f04948ba92c43e8469c1ee6736c7563d71012b17d40745260fe106aac2166"
 dependencies = [
  "base64ct",
- "rand_core 0.6.4",
+ "rand_core",
  "subtle",
 ]
 
@@ -3027,7 +2800,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "3c80231409c20246a13fddb31776fb942c38553c51e871f8cbd687a4cfb5843d"
 dependencies = [
  "phf_shared",
- "rand 0.8.5",
+ "rand",
 ]
 
 [[package]]
@@ -3141,24 +2914,6 @@ version = "1.11.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "350e9b48cbc6b0e028b0473b114454c6316e57336ee184ceab6e53f72c178b3e"
 
-[[package]]
-name = "postmark"
-version = "0.11.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "68343f3a4bf4afeae32c7f4bee549823034c6f754d7b718d8c2459aa1a15406e"
-dependencies = [
- "async-trait",
- "bytes",
- "http 1.3.1",
- "reqwest 0.12.15",
- "serde",
- "serde_json",
- "thiserror 1.0.69",
- "time",
- "typed-builder",
- "url",
-]
-
 [[package]]
 name = "powerfmt"
 version = "0.2.0"
@@ -3171,7 +2926,7 @@ version = "0.2.21"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "85eae3c4ed2f50dcfe72643da4befc30deadb458a9b590d720cde2f2b1e97da9"
 dependencies = [
- "zerocopy 0.8.23",
+ "zerocopy",
 ]
 
 [[package]]
@@ -3294,22 +3049,6 @@ dependencies = [
  "thiserror 2.0.12",
 ]
 
-[[package]]
-name = "quanta"
-version = "0.11.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a17e662a7a8291a865152364c20c7abc5e60486ab2001e8ec10b24862de0b9ab"
-dependencies = [
- "crossbeam-utils",
- "libc",
- "mach2",
- "once_cell",
- "raw-cpuid",
- "wasi 0.11.0+wasi-snapshot-preview1",
- "web-sys",
- "winapi",
-]
-
 [[package]]
 name = "quick-xml"
 version = "0.37.2"
@@ -3319,60 +3058,6 @@ dependencies = [
  "memchr",
 ]
 
-[[package]]
-name = "quinn"
-version = "0.11.7"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c3bd15a6f2967aef83887dcb9fec0014580467e33720d073560cf015a5683012"
-dependencies = [
- "bytes",
- "cfg_aliases 0.2.1",
- "pin-project-lite",
- "quinn-proto",
- "quinn-udp",
- "rustc-hash 2.1.1",
- "rustls 0.23.25",
- "socket2",
- "thiserror 2.0.12",
- "tokio",
- "tracing",
- "web-time",
-]
-
-[[package]]
-name = "quinn-proto"
-version = "0.11.10"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b820744eb4dc9b57a3398183639c511b5a26d2ed702cedd3febaa1393caa22cc"
-dependencies = [
- "bytes",
- "getrandom 0.3.2",
- "rand 0.9.0",
- "ring",
- "rustc-hash 2.1.1",
- "rustls 0.23.25",
- "rustls-pki-types",
- "slab",
- "thiserror 2.0.12",
- "tinyvec",
- "tracing",
- "web-time",
-]
-
-[[package]]
-name = "quinn-udp"
-version = "0.5.10"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e46f3055866785f6b92bc6164b76be02ca8f2eb4b002c0354b28cf4c119e5944"
-dependencies = [
- "cfg_aliases 0.2.1",
- "libc",
- "once_cell",
- "socket2",
- "tracing",
- "windows-sys 0.59.0",
-]
-
 [[package]]
 name = "quote"
 version = "1.0.40"
@@ -3395,19 +3080,8 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "34af8d1a0e25924bc5b7c43c079c942339d8f0a8b57c39049bef581b46327404"
 dependencies = [
  "libc",
- "rand_chacha 0.3.1",
- "rand_core 0.6.4",
-]
-
-[[package]]
-name = "rand"
-version = "0.9.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3779b94aeb87e8bd4e834cee3650289ee9e0d5677f976ecdb6d219e5f4f6cd94"
-dependencies = [
- "rand_chacha 0.9.0",
- "rand_core 0.9.3",
- "zerocopy 0.8.23",
+ "rand_chacha",
+ "rand_core",
 ]
 
 [[package]]
@@ -3417,17 +3091,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e6c10a63a0fa32252be49d21e7709d4d4baf8d231c2dbce1eaa8141b9b127d88"
 dependencies = [
  "ppv-lite86",
- "rand_core 0.6.4",
-]
-
-[[package]]
-name = "rand_chacha"
-version = "0.9.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d3022b5f1df60f26e1ffddd6c66e8aa15de382ae63b3a0c1bfc0e4d3e3f325cb"
-dependencies = [
- "ppv-lite86",
- "rand_core 0.9.3",
+ "rand_core",
 ]
 
 [[package]]
@@ -3439,15 +3103,6 @@ dependencies = [
  "getrandom 0.2.15",
 ]
 
-[[package]]
-name = "rand_core"
-version = "0.9.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "99d9a13982dcf210057a8a78572b2217b667c3beacbf3a0d8b454f6f82837d38"
-dependencies = [
- "getrandom 0.3.2",
-]
-
 [[package]]
 name = "ratatui"
 version = "0.29.0"
@@ -3469,15 +3124,6 @@ dependencies = [
  "unicode-width 0.2.0",
 ]
 
-[[package]]
-name = "raw-cpuid"
-version = "10.7.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6c297679cb867470fa8c9f67dbba74a78d78e3e98d7cf2b08d6d71540f797332"
-dependencies = [
- "bitflags 1.3.2",
-]
-
 [[package]]
 name = "rayon"
 version = "1.10.0"
@@ -3583,7 +3229,7 @@ dependencies = [
  "http 0.2.12",
  "http-body 0.4.6",
  "hyper 0.14.32",
- "hyper-rustls 0.24.2",
+ "hyper-rustls",
  "ipnet",
  "js-sys",
  "log",
@@ -3600,7 +3246,7 @@ dependencies = [
  "sync_wrapper 0.1.2",
  "system-configuration",
  "tokio",
- "tokio-rustls 0.24.1",
+ "tokio-rustls",
  "tower-service",
  "url",
  "wasm-bindgen",
@@ -3609,49 +3255,6 @@ dependencies = [
  "winreg",
 ]
 
-[[package]]
-name = "reqwest"
-version = "0.12.15"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d19c46a6fdd48bc4dab94b6103fccc55d34c67cc0ad04653aad4ea2a07cd7bbb"
-dependencies = [
- "base64 0.22.1",
- "bytes",
- "futures-core",
- "futures-util",
- "http 1.3.1",
- "http-body 1.0.1",
- "http-body-util",
- "hyper 1.6.0",
- "hyper-rustls 0.27.5",
- "hyper-util",
- "ipnet",
- "js-sys",
- "log",
- "mime",
- "once_cell",
- "percent-encoding",
- "pin-project-lite",
- "quinn",
- "rustls 0.23.25",
- "rustls-pemfile 2.2.0",
- "rustls-pki-types",
- "serde",
- "serde_json",
- "serde_urlencoded",
- "sync_wrapper 1.0.2",
- "tokio",
- "tokio-rustls 0.26.2",
- "tower 0.5.2",
- "tower-service",
- "url",
- "wasm-bindgen",
- "wasm-bindgen-futures",
- "web-sys",
- "webpki-roots",
- "windows-registry",
-]
-
 [[package]]
 name = "ring"
 version = "0.17.14"
@@ -3701,7 +3304,7 @@ dependencies = [
  "num-traits",
  "pkcs1",
  "pkcs8",
- "rand_core 0.6.4",
+ "rand_core",
  "signature",
  "spki",
  "subtle",
@@ -3739,12 +3342,6 @@ version = "1.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "08d43f7aa6b08d49f382cde6a7982047c3426db949b1424bc4b7ec9ae12c6ce2"
 
-[[package]]
-name = "rustc-hash"
-version = "2.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "357703d41365b4b27c590e3ed91eabb1b663f07c4c084095e60cbed4362dff0d"
-
 [[package]]
 name = "rustc_version"
 version = "0.4.1"
@@ -3841,9 +3438,6 @@ name = "rustls-pki-types"
 version = "1.11.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "917ce264624a4b4db1c364dcc35bfca9ded014d0a958cd47ad3e960e988ea51c"
-dependencies = [
- "web-time",
-]
 
 [[package]]
 name = "rustls-webpki"
@@ -3888,7 +3482,7 @@ dependencies = [
  "digest",
  "ed25519-dalek",
  "generic-array",
- "rand 0.8.5",
+ "rand",
  "rusty_paseto",
  "serde",
  "sha2",
@@ -3908,7 +3502,7 @@ dependencies = [
  "ed25519-dalek",
  "hex",
  "iso8601",
- "rand_core 0.6.4",
+ "rand_core",
  "ring",
  "thiserror 1.0.69",
  "time",
@@ -4016,16 +3610,6 @@ dependencies = [
  "serde",
 ]
 
-[[package]]
-name = "serde_path_to_error"
-version = "0.1.17"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "59fab13f937fa393d08645bf3a84bdfe86e296747b506ada67bb15f10f218b2a"
-dependencies = [
- "itoa",
- "serde",
-]
-
 [[package]]
 name = "serde_regex"
 version = "1.1.0"
@@ -4170,7 +3754,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "77549399552de45a898a580c1b41d445bf730df867cc44e6c0233bbc4b8329de"
 dependencies = [
  "digest",
- "rand_core 0.6.4",
+ "rand_core",
 ]
 
 [[package]]
@@ -4185,12 +3769,6 @@ version = "1.0.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "56199f7ddabf13fe5074ce809e7d3f42b42ae711800501b5b16ea82ad029c39d"
 
-[[package]]
-name = "sketches-ddsketch"
-version = "0.2.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "85636c14b73d81f541e525f585c0a2109e6744e1565b5c1668e31c70c10ed65c"
-
 [[package]]
 name = "slab"
 version = "0.4.9"
@@ -4367,7 +3945,7 @@ dependencies = [
  "memchr",
  "once_cell",
  "percent-encoding",
- "rand 0.8.5",
+ "rand",
  "rsa",
  "serde",
  "sha1",
@@ -4407,7 +3985,7 @@ dependencies = [
  "md-5",
  "memchr",
  "once_cell",
- "rand 0.8.5",
+ "rand",
  "serde",
  "serde_json",
  "sha2",
@@ -4526,9 +4104,6 @@ name = "sync_wrapper"
 version = "1.0.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0bf256ce5efdfa370213c1dabab5935a12e49f2c58d15e9eac2870d3b4f27263"
-dependencies = [
- "futures-core",
-]
 
 [[package]]
 name = "synstructure"
@@ -4713,8 +4288,8 @@ dependencies = [
  "hmac",
  "once_cell",
  "pbkdf2",
- "rand 0.8.5",
- "rustc-hash 1.1.0",
+ "rand",
+ "rustc-hash",
  "sha2",
  "thiserror 1.0.69",
  "unicode-normalization",
@@ -4786,16 +4361,6 @@ dependencies = [
  "tokio",
 ]
 
-[[package]]
-name = "tokio-rustls"
-version = "0.26.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8e727b36a1a0e8b74c376ac2211e40c2c8af09fb4013c60d910495810f008e9b"
-dependencies = [
- "rustls 0.23.25",
- "tokio",
-]
-
 [[package]]
 name = "tokio-stream"
 version = "0.1.17"
@@ -4920,7 +4485,7 @@ dependencies = [
  "indexmap 1.9.3",
  "pin-project",
  "pin-project-lite",
- "rand 0.8.5",
+ "rand",
  "slab",
  "tokio",
  "tokio-util",
@@ -4939,26 +4504,8 @@ dependencies = [
  "futures-util",
  "pin-project-lite",
  "sync_wrapper 1.0.2",
- "tokio",
- "tower-layer",
- "tower-service",
- "tracing",
-]
-
-[[package]]
-name = "tower-http"
-version = "0.6.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "403fa3b783d4b626a8ad51d766ab03cb6d2dbfc46b1c5d4448395e6628dc9697"
-dependencies = [
- "bitflags 2.9.0",
- "bytes",
- "http 1.3.1",
- "http-body 1.0.1",
- "pin-project-lite",
  "tower-layer",
  "tower-service",
- "tracing",
 ]
 
 [[package]]
@@ -5491,35 +5038,6 @@ version = "0.1.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "76840935b766e1b0a05c0066835fb9ec80071d4c09a16f6bd5f7e655e3c14c38"
 
-[[package]]
-name = "windows-registry"
-version = "0.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4286ad90ddb45071efd1a66dfa43eb02dd0dfbae1545ad6cc3c51cf34d7e8ba3"
-dependencies = [
- "windows-result",
- "windows-strings",
- "windows-targets 0.53.0",
-]
-
-[[package]]
-name = "windows-result"
-version = "0.3.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c64fd11a4fd95df68efcfee5f44a294fe71b8bc6a91993e2791938abcc712252"
-dependencies = [
- "windows-link",
-]
-
-[[package]]
-name = "windows-strings"
-version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "87fa48cc5d406560701792be122a10132491cff9d0aeb23583cc2dcafc847319"
-dependencies = [
- "windows-link",
-]
-
 [[package]]
 name = "windows-sys"
 version = "0.48.0"
@@ -5571,29 +5089,13 @@ dependencies = [
  "windows_aarch64_gnullvm 0.52.6",
  "windows_aarch64_msvc 0.52.6",
  "windows_i686_gnu 0.52.6",
- "windows_i686_gnullvm 0.52.6",
+ "windows_i686_gnullvm",
  "windows_i686_msvc 0.52.6",
  "windows_x86_64_gnu 0.52.6",
  "windows_x86_64_gnullvm 0.52.6",
  "windows_x86_64_msvc 0.52.6",
 ]
 
-[[package]]
-name = "windows-targets"
-version = "0.53.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b1e4c7e8ceaaf9cb7d7507c974735728ab453b67ef8f18febdd7c11fe59dca8b"
-dependencies = [
- "windows_aarch64_gnullvm 0.53.0",
- "windows_aarch64_msvc 0.53.0",
- "windows_i686_gnu 0.53.0",
- "windows_i686_gnullvm 0.53.0",
- "windows_i686_msvc 0.53.0",
- "windows_x86_64_gnu 0.53.0",
- "windows_x86_64_gnullvm 0.53.0",
- "windows_x86_64_msvc 0.53.0",
-]
-
 [[package]]
 name = "windows_aarch64_gnullvm"
 version = "0.48.5"
@@ -5606,12 +5108,6 @@ version = "0.52.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "32a4622180e7a0ec044bb555404c800bc9fd9ec262ec147edd5989ccd0c02cd3"
 
-[[package]]
-name = "windows_aarch64_gnullvm"
-version = "0.53.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "86b8d5f90ddd19cb4a147a5fa63ca848db3df085e25fee3cc10b39b6eebae764"
-
 [[package]]
 name = "windows_aarch64_msvc"
 version = "0.48.5"
@@ -5624,12 +5120,6 @@ version = "0.52.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "09ec2a7bb152e2252b53fa7803150007879548bc709c039df7627cabbd05d469"
 
-[[package]]
-name = "windows_aarch64_msvc"
-version = "0.53.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c7651a1f62a11b8cbd5e0d42526e55f2c99886c77e007179efff86c2b137e66c"
-
 [[package]]
 name = "windows_i686_gnu"
 version = "0.48.5"
@@ -5642,24 +5132,12 @@ version = "0.52.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8e9b5ad5ab802e97eb8e295ac6720e509ee4c243f69d781394014ebfe8bbfa0b"
 
-[[package]]
-name = "windows_i686_gnu"
-version = "0.53.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c1dc67659d35f387f5f6c479dc4e28f1d4bb90ddd1a5d3da2e5d97b42d6272c3"
-
 [[package]]
 name = "windows_i686_gnullvm"
 version = "0.52.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0eee52d38c090b3caa76c563b86c3a4bd71ef1a819287c19d586d7334ae8ed66"
 
-[[package]]
-name = "windows_i686_gnullvm"
-version = "0.53.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "9ce6ccbdedbf6d6354471319e781c0dfef054c81fbc7cf83f338a4296c0cae11"
-
 [[package]]
 name = "windows_i686_msvc"
 version = "0.48.5"
@@ -5672,12 +5150,6 @@ version = "0.52.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "240948bc05c5e7c6dabba28bf89d89ffce3e303022809e73deaefe4f6ec56c66"
 
-[[package]]
-name = "windows_i686_msvc"
-version = "0.53.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "581fee95406bb13382d2f65cd4a908ca7b1e4c2f1917f143ba16efe98a589b5d"
-
 [[package]]
 name = "windows_x86_64_gnu"
 version = "0.48.5"
@@ -5690,12 +5162,6 @@ version = "0.52.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "147a5c80aabfbf0c7d901cb5895d1de30ef2907eb21fbbab29ca94c5b08b1a78"
 
-[[package]]
-name = "windows_x86_64_gnu"
-version = "0.53.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2e55b5ac9ea33f2fc1716d1742db15574fd6fc8dadc51caab1c16a3d3b4190ba"
-
 [[package]]
 name = "windows_x86_64_gnullvm"
 version = "0.48.5"
@@ -5708,12 +5174,6 @@ version = "0.52.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "24d5b23dc417412679681396f2b49f3de8c1473deb516bd34410872eff51ed0d"
 
-[[package]]
-name = "windows_x86_64_gnullvm"
-version = "0.53.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "0a6e035dd0599267ce1ee132e51c27dd29437f63325753051e71dd9e42406c57"
-
 [[package]]
 name = "windows_x86_64_msvc"
 version = "0.48.5"
@@ -5726,12 +5186,6 @@ version = "0.52.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "589f6da84c646204747d1270a2a5661ea66ed1cced2631d546fdfb155959f9ec"
 
-[[package]]
-name = "windows_x86_64_msvc"
-version = "0.53.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "271414315aff87387382ec3d271b52d7ae78726f5d44ac98b4f4030c91880486"
-
 [[package]]
 name = "winnow"
 version = "0.7.4"
@@ -5839,33 +5293,13 @@ dependencies = [
  "synstructure",
 ]
 
-[[package]]
-name = "zerocopy"
-version = "0.7.35"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1b9b4fd18abc82b8136838da5d50bae7bdea537c574d8dc1a34ed098d6c166f0"
-dependencies = [
- "zerocopy-derive 0.7.35",
-]
-
 [[package]]
 name = "zerocopy"
 version = "0.8.23"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "fd97444d05a4328b90e75e503a34bad781f14e28a823ad3557f0750df1ebcbc6"
 dependencies = [
- "zerocopy-derive 0.8.23",
-]
-
-[[package]]
-name = "zerocopy-derive"
-version = "0.7.35"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "fa4f8080344d4671fb4e831a13ad1e68092748387dfc4f55e356242fae12ce3e"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn",
+ "zerocopy-derive",
 ]
 
 [[package]]
diff --git a/crates/atuin-client/Cargo.toml b/crates/atuin-client/Cargo.toml
index 5a98f797..e8d86e1c 100644
--- a/crates/atuin-client/Cargo.toml
+++ b/crates/atuin-client/Cargo.toml
@@ -13,7 +13,7 @@ repository = { workspace = true }
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [features]
-default = ["sync", "daemon"]
+default = ["daemon"]
 sync = ["urlencoding", "reqwest", "sha2", "hex"]
 daemon = []
 check-update = []
diff --git a/crates/atuin-client/src/api_client.rs b/crates/atuin-client/src/api_client.rs
deleted file mode 100644
index 0bd16c50..00000000
--- a/crates/atuin-client/src/api_client.rs
+++ /dev/null
@@ -1,442 +0,0 @@
-use std::collections::HashMap;
-use std::env;
-use std::time::Duration;
-
-use eyre::{Result, bail};
-use reqwest::{
-    Response, StatusCode, Url,
-    header::{AUTHORIZATION, HeaderMap, USER_AGENT},
-};
-
-use atuin_common::{
-    api::{ATUIN_CARGO_VERSION, ATUIN_HEADER_VERSION, ATUIN_VERSION},
-    record::{EncryptedData, HostId, Record, RecordIdx},
-};
-use atuin_common::{
-    api::{
-        AddHistoryRequest, ChangePasswordRequest, CountResponse, DeleteHistoryRequest,
-        ErrorResponse, LoginRequest, LoginResponse, MeResponse, RegisterResponse,
-        SendVerificationResponse, StatusResponse, SyncHistoryResponse, VerificationTokenRequest,
-        VerificationTokenResponse,
-    },
-    record::RecordStatus,
-};
-
-use semver::Version;
-use time::OffsetDateTime;
-use time::format_description::well_known::Rfc3339;
-
-use crate::{history::History, sync::hash_str, utils::get_host_user};
-
-static APP_USER_AGENT: &str = concat!("atuin/", env!("CARGO_PKG_VERSION"),);
-
-pub struct Client<'a> {
-    sync_addr: &'a str,
-    client: reqwest::Client,
-}
-
-pub async fn register(
-    address: &str,
-    username: &str,
-    email: &str,
-    password: &str,
-) -> Result<RegisterResponse> {
-    let mut map = HashMap::new();
-    map.insert("username", username);
-    map.insert("email", email);
-    map.insert("password", password);
-
-    let url = format!("{address}/user/{username}");
-    let resp = reqwest::get(url).await?;
-
-    if resp.status().is_success() {
-        bail!("username already in use");
-    }
-
-    let url = format!("{address}/register");
-    let client = reqwest::Client::new();
-    let resp = client
-        .post(url)
-        .header(USER_AGENT, APP_USER_AGENT)
-        .header(ATUIN_HEADER_VERSION, ATUIN_CARGO_VERSION)
-        .json(&map)
-        .send()
-        .await?;
-    let resp = handle_resp_error(resp).await?;
-
-    if !ensure_version(&resp)? {
-        bail!("could not register user due to version mismatch");
-    }
-
-    let session = resp.json::<RegisterResponse>().await?;
-    Ok(session)
-}
-
-pub async fn login(address: &str, req: LoginRequest) -> Result<LoginResponse> {
-    let url = format!("{address}/login");
-    let client = reqwest::Client::new();
-
-    let resp = client
-        .post(url)
-        .header(USER_AGENT, APP_USER_AGENT)
-        .json(&req)
-        .send()
-        .await?;
-    let resp = handle_resp_error(resp).await?;
-
-    if !ensure_version(&resp)? {
-        bail!("Could not login due to version mismatch");
-    }
-
-    let session = resp.json::<LoginResponse>().await?;
-    Ok(session)
-}
-
-#[cfg(feature = "check-update")]
-pub async fn latest_version() -> Result<Version> {
-    use atuin_common::api::IndexResponse;
-
-    let url = "https://api.atuin.sh";
-    let client = reqwest::Client::new();
-
-    let resp = client
-        .get(url)
-        .header(USER_AGENT, APP_USER_AGENT)
-        .send()
-        .await?;
-    let resp = handle_resp_error(resp).await?;
-
-    let index = resp.json::<IndexResponse>().await?;
-    let version = Version::parse(index.version.as_str())?;
-
-    Ok(version)
-}
-
-pub fn ensure_version(response: &Response) -> Result<bool> {
-    let version = response.headers().get(ATUIN_HEADER_VERSION);
-
-    let version = if let Some(version) = version {
-        match version.to_str() {
-            Ok(v) => Version::parse(v),
-            Err(e) => bail!("failed to parse server version: {:?}", e),
-        }
-    } else {
-        bail!("Server not reporting its version: it is either too old or unhealthy");
-    }?;
-
-    // If the client is newer than the server
-    if version.major < ATUIN_VERSION.major {
-        println!(
-            "Atuin version mismatch! In order to successfully sync, the server needs to run a newer version of Atuin"
-        );
-        println!("Client: {}", ATUIN_CARGO_VERSION);
-        println!("Server: {}", version);
-
-        return Ok(false);
-    }
-
-    Ok(true)
-}
-
-async fn handle_resp_error(resp: Response) -> Result<Response> {
-    let status = resp.status();
-
-    if status == StatusCode::SERVICE_UNAVAILABLE {
-        bail!(
-            "Service unavailable: check https://status.atuin.sh (or get in touch with your host)"
-        );
-    }
-
-    if status == StatusCode::TOO_MANY_REQUESTS {
-        bail!("Rate limited; please wait before doing that again");
-    }
-
-    if !status.is_success() {
-        if let Ok(error) = resp.json::<ErrorResponse>().await {
-            let reason = error.reason;
-
-            if status.is_client_error() {
-                bail!("Invalid request to the service: {status} - {reason}.")
-            }
-
-            bail!(
-                "There was an error with the atuin sync service, server error {status}: {reason}.\nIf the problem persists, contact the host"
-            )
-        }
-
-        bail!(
-            "There was an error with the atuin sync service: Status {status:?}.\nIf the problem persists, contact the host"
-        )
-    }
-
-    Ok(resp)
-}
-
-impl<'a> Client<'a> {
-    pub fn new(
-        sync_addr: &'a str,
-        session_token: &str,
-        connect_timeout: u64,
-        timeout: u64,
-    ) -> Result<Self> {
-        let mut headers = HeaderMap::new();
-        headers.insert(AUTHORIZATION, format!("Token {session_token}").parse()?);
-
-        // used for semver server check
-        headers.insert(ATUIN_HEADER_VERSION, ATUIN_CARGO_VERSION.parse()?);
-
-        Ok(Client {
-            sync_addr,
-            client: reqwest::Client::builder()
-                .user_agent(APP_USER_AGENT)
-                .default_headers(headers)
-                .connect_timeout(Duration::new(connect_timeout, 0))
-                .timeout(Duration::new(timeout, 0))
-                .build()?,
-        })
-    }
-
-    pub async fn count(&self) -> Result<i64> {
-        let url = format!("{}/sync/count", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self.client.get(url).send().await?;
-        let resp = handle_resp_error(resp).await?;
-
-        if !ensure_version(&resp)? {
-            bail!("could not sync due to version mismatch");
-        }
-
-        if resp.status() != StatusCode::OK {
-            bail!("failed to get count (are you logged in?)");
-        }
-
-        let count = resp.json::<CountResponse>().await?;
-
-        Ok(count.count)
-    }
-
-    pub async fn status(&self) -> Result<StatusResponse> {
-        let url = format!("{}/sync/status", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self.client.get(url).send().await?;
-        let resp = handle_resp_error(resp).await?;
-
-        if !ensure_version(&resp)? {
-            bail!("could not sync due to version mismatch");
-        }
-
-        let status = resp.json::<StatusResponse>().await?;
-
-        Ok(status)
-    }
-
-    pub async fn me(&self) -> Result<MeResponse> {
-        let url = format!("{}/api/v0/me", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self.client.get(url).send().await?;
-        let resp = handle_resp_error(resp).await?;
-
-        let status = resp.json::<MeResponse>().await?;
-
-        Ok(status)
-    }
-
-    pub async fn get_history(
-        &self,
-        sync_ts: OffsetDateTime,
-        history_ts: OffsetDateTime,
-        host: Option<String>,
-    ) -> Result<SyncHistoryResponse> {
-        let host = host.unwrap_or_else(|| hash_str(&get_host_user()));
-
-        let url = format!(
-            "{}/sync/history?sync_ts={}&history_ts={}&host={}",
-            self.sync_addr,
-            urlencoding::encode(sync_ts.format(&Rfc3339)?.as_str()),
-            urlencoding::encode(history_ts.format(&Rfc3339)?.as_str()),
-            host,
-        );
-
-        let resp = self.client.get(url).send().await?;
-        let resp = handle_resp_error(resp).await?;
-
-        let history = resp.json::<SyncHistoryResponse>().await?;
-        Ok(history)
-    }
-
-    pub async fn post_history(&self, history: &[AddHistoryRequest]) -> Result<()> {
-        let url = format!("{}/history", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self.client.post(url).json(history).send().await?;
-        handle_resp_error(resp).await?;
-
-        Ok(())
-    }
-
-    pub async fn delete_history(&self, h: History) -> Result<()> {
-        let url = format!("{}/history", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self
-            .client
-            .delete(url)
-            .json(&DeleteHistoryRequest {
-                client_id: h.id.to_string(),
-            })
-            .send()
-            .await?;
-
-        handle_resp_error(resp).await?;
-
-        Ok(())
-    }
-
-    pub async fn delete_store(&self) -> Result<()> {
-        let url = format!("{}/api/v0/store", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self.client.delete(url).send().await?;
-
-        handle_resp_error(resp).await?;
-
-        Ok(())
-    }
-
-    pub async fn post_records(&self, records: &[Record<EncryptedData>]) -> Result<()> {
-        let url = format!("{}/api/v0/record", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        debug!("uploading {} records to {url}", records.len());
-
-        let resp = self.client.post(url).json(records).send().await?;
-        handle_resp_error(resp).await?;
-
-        Ok(())
-    }
-
-    pub async fn next_records(
-        &self,
-        host: HostId,
-        tag: String,
-        start: RecordIdx,
-        count: u64,
-    ) -> Result<Vec<Record<EncryptedData>>> {
-        debug!(
-            "fetching record/s from host {}/{}/{}",
-            host.0.to_string(),
-            tag,
-            start
-        );
-
-        let url = format!(
-            "{}/api/v0/record/next?host={}&tag={}&count={}&start={}",
-            self.sync_addr, host.0, tag, count, start
-        );
-
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self.client.get(url).send().await?;
-        let resp = handle_resp_error(resp).await?;
-
-        let records = resp.json::<Vec<Record<EncryptedData>>>().await?;
-
-        Ok(records)
-    }
-
-    pub async fn record_status(&self) -> Result<RecordStatus> {
-        let url = format!("{}/api/v0/record", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self.client.get(url).send().await?;
-        let resp = handle_resp_error(resp).await?;
-
-        if !ensure_version(&resp)? {
-            bail!("could not sync records due to version mismatch");
-        }
-
-        let index = resp.json().await?;
-
-        debug!("got remote index {:?}", index);
-
-        Ok(index)
-    }
-
-    pub async fn delete(&self) -> Result<()> {
-        let url = format!("{}/account", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self.client.delete(url).send().await?;
-
-        if resp.status() == 403 {
-            bail!("invalid login details");
-        } else if resp.status() == 200 {
-            Ok(())
-        } else {
-            bail!("Unknown error");
-        }
-    }
-
-    pub async fn change_password(
-        &self,
-        current_password: String,
-        new_password: String,
-    ) -> Result<()> {
-        let url = format!("{}/account/password", self.sync_addr);
-        let url = Url::parse(url.as_str())?;
-
-        let resp = self
-            .client
-            .patch(url)
-            .json(&ChangePasswordRequest {
-                current_password,
-                new_password,
-            })
-            .send()
-            .await?;
-
-        if resp.status() == 401 {
-            bail!("current password is incorrect")
-        } else if resp.status() == 403 {
-            bail!("invalid login details");
-        } else if resp.status() == 200 {
-            Ok(())
-        } else {
-            bail!("Unknown error");
-        }
-    }
-
-    // Either request a verification email if token is null, or validate a token
-    pub async fn verify(&self, token: Option<String>) -> Result<(bool, bool)> {
-        // could dedupe this a bit, but it's simple at the moment
-        let (email_sent, verified) = if let Some(token) = token {
-            let url = format!("{}/api/v0/account/verify", self.sync_addr);
-            let url = Url::parse(url.as_str())?;
-
-            let resp = self
-                .client
-                .post(url)
-                .json(&VerificationTokenRequest { token })
-                .send()
-                .await?;
-            let resp = handle_resp_error(resp).await?;
-            let resp = resp.json::<VerificationTokenResponse>().await?;
-
-            (false, resp.verified)
-        } else {
-            let url = format!("{}/api/v0/account/send-verification", self.sync_addr);
-            let url = Url::parse(url.as_str())?;
-
-            let resp = self.client.post(url).send().await?;
-            let resp = handle_resp_error(resp).await?;
-            let resp = resp.json::<SendVerificationResponse>().await?;
-
-            (resp.email_sent, resp.verified)
-        };
-
-        Ok((email_sent, verified))
-    }
-}
diff --git a/crates/atuin-client/src/lib.rs b/crates/atuin-client/src/lib.rs
index 443ff3f8..b7595d4c 100644
--- a/crates/atuin-client/src/lib.rs
+++ b/crates/atuin-client/src/lib.rs
@@ -3,20 +3,12 @@
 #[macro_use]
 extern crate log;
 
-#[cfg(feature = "sync")]
-pub mod api_client;
-#[cfg(feature = "sync")]
-pub mod sync;
-
 pub mod database;
 pub mod encryption;
 pub mod history;
 pub mod import;
-pub mod login;
-pub mod logout;
 pub mod ordering;
 pub mod record;
-pub mod register;
 pub mod secrets;
 pub mod settings;
 pub mod theme;
diff --git a/crates/atuin-client/src/login.rs b/crates/atuin-client/src/login.rs
deleted file mode 100644
index 78168c7e..00000000
--- a/crates/atuin-client/src/login.rs
+++ /dev/null
@@ -1,94 +0,0 @@
-use std::path::PathBuf;
-
-use atuin_common::api::LoginRequest;
-use eyre::{Context, Result, bail};
-use tokio::fs::File;
-use tokio::io::AsyncWriteExt;
-
-use crate::{
-    api_client,
-    encryption::{Key, decode_key, encode_key, load_key},
-    record::{sqlite_store::SqliteStore, store::Store},
-    settings::Settings,
-};
-
-pub async fn login(
-    settings: &Settings,
-    store: &SqliteStore,
-    username: String,
-    password: String,
-    key: String,
-) -> Result<String> {
-    // try parse the key as a mnemonic...
-    let key = match bip39::Mnemonic::from_phrase(&key, bip39::Language::English) {
-        Ok(mnemonic) => encode_key(Key::from_slice(mnemonic.entropy()))?,
-        Err(err) => {
-            match err.downcast_ref::<bip39::ErrorKind>() {
-                Some(err) => {
-                    match err {
-                        // assume they copied in the base64 key
-                        bip39::ErrorKind::InvalidWord => key,
-                        bip39::ErrorKind::InvalidChecksum => {
-                            bail!("key mnemonic was not valid")
-                        }
-                        bip39::ErrorKind::InvalidKeysize(_)
-                        | bip39::ErrorKind::InvalidWordLength(_)
-                        | bip39::ErrorKind::InvalidEntropyLength(_, _) => {
-                            bail!("key was not the correct length")
-                        }
-                    }
-                }
-                _ => {
-                    // unknown error. assume they copied the base64 key
-                    key
-                }
-            }
-        }
-    };
-
-    let key_path = settings.key_path.as_str();
-    let key_path = PathBuf::from(key_path);
-
-    if !key_path.exists() {
-        if decode_key(key.clone()).is_err() {
-            bail!("the specified key was invalid");
-        }
-
-        let mut file = File::create(key_path).await?;
-        file.write_all(key.as_bytes()).await?;
-    } else {
-        // we now know that the user has logged in specifying a key, AND that the key path
-        // exists
-
-        // 1. check if the saved key and the provided key match. if so, nothing to do.
-        // 2. if not, re-encrypt the local history and overwrite the key
-        let current_key: [u8; 32] = load_key(settings)?.into();
-
-        let encoded = key.clone(); // gonna want to save it in a bit
-        let new_key: [u8; 32] = decode_key(key)
-            .context("could not decode provided key - is not valid base64")?
-            .into();
-
-        if new_key != current_key {
-            println!("\nRe-encrypting local store with new key");
-
-            store.re_encrypt(&current_key, &new_key).await?;
-
-            println!("Writing new key");
-            let mut file = File::create(key_path).await?;
-            file.write_all(encoded.as_bytes()).await?;
-        }
-    }
-
-    let session = api_client::login(
-        settings.sync_address.as_str(),
-        LoginRequest { username, password },
-    )
-    .await?;
-
-    let session_path = settings.session_path.as_str();
-    let mut file = File::create(session_path).await?;
-    file.write_all(session.session.as_bytes()).await?;
-
-    Ok(session.session)
-}
diff --git a/crates/atuin-client/src/logout.rs b/crates/atuin-client/src/logout.rs
deleted file mode 100644
index fe1a4d23..00000000
--- a/crates/atuin-client/src/logout.rs
+++ /dev/null
@@ -1,17 +0,0 @@
-use eyre::{Context, Result};
-use fs_err::remove_file;
-
-use crate::settings::Settings;
-
-pub fn logout(settings: &Settings) -> Result<()> {
-    let session_path = settings.session_path.as_str();
-
-    if settings.logged_in() {
-        remove_file(session_path).context("Failed to remove session file")?;
-        println!("You have logged out!");
-    } else {
-        println!("You are not logged in");
-    }
-
-    Ok(())
-}
diff --git a/crates/atuin-client/src/record/mod.rs b/crates/atuin-client/src/record/mod.rs
index c40fd395..9ac2c541 100644
--- a/crates/atuin-client/src/record/mod.rs
+++ b/crates/atuin-client/src/record/mod.rs
@@ -1,6 +1,3 @@
 pub mod encryption;
 pub mod sqlite_store;
 pub mod store;
-
-#[cfg(feature = "sync")]
-pub mod sync;
diff --git a/crates/atuin-client/src/record/sync.rs b/crates/atuin-client/src/record/sync.rs
deleted file mode 100644
index 1c6b0e01..00000000
--- a/crates/atuin-client/src/record/sync.rs
+++ /dev/null
@@ -1,616 +0,0 @@
-// do a sync :O
-use std::{cmp::Ordering, fmt::Write};
-
-use eyre::Result;
-use thiserror::Error;
-
-use super::store::Store;
-use crate::{api_client::Client, settings::Settings};
-
-use atuin_common::record::{Diff, HostId, RecordId, RecordIdx, RecordStatus};
-use indicatif::{ProgressBar, ProgressState, ProgressStyle};
-
-#[derive(Error, Debug)]
-pub enum SyncError {
-    #[error("the local store is ahead of the remote, but for another host. has remote lost data?")]
-    LocalAheadOtherHost,
-
-    #[error("an issue with the local database occurred: {msg:?}")]
-    LocalStoreError { msg: String },
-
-    #[error("something has gone wrong with the sync logic: {msg:?}")]
-    SyncLogicError { msg: String },
-
-    #[error("operational error: {msg:?}")]
-    OperationalError { msg: String },
-
-    #[error("a request to the sync server failed: {msg:?}")]
-    RemoteRequestError { msg: String },
-}
-
-#[derive(Debug, Eq, PartialEq)]
-pub enum Operation {
-    // Either upload or download until the states matches the below
-    Upload {
-        local: RecordIdx,
-        remote: Option<RecordIdx>,
-        host: HostId,
-        tag: String,
-    },
-    Download {
-        local: Option<RecordIdx>,
-        remote: RecordIdx,
-        host: HostId,
-        tag: String,
-    },
-    Noop {
-        host: HostId,
-        tag: String,
-    },
-}
-
-pub async fn diff(
-    settings: &Settings,
-    store: &impl Store,
-) -> Result<(Vec<Diff>, RecordStatus), SyncError> {
-    let client = Client::new(
-        &settings.sync_address,
-        settings
-            .session_token()
-            .map_err(|e| SyncError::RemoteRequestError { msg: e.to_string() })?
-            .as_str(),
-        settings.network_connect_timeout,
-        settings.network_timeout,
-    )
-    .map_err(|e| SyncError::OperationalError { msg: e.to_string() })?;
-
-    let local_index = store
-        .status()
-        .await
-        .map_err(|e| SyncError::LocalStoreError { msg: e.to_string() })?;
-
-    let remote_index = client
-        .record_status()
-        .await
-        .map_err(|e| SyncError::RemoteRequestError { msg: e.to_string() })?;
-
-    let diff = local_index.diff(&remote_index);
-
-    Ok((diff, remote_index))
-}
-
-// Take a diff, along with a local store, and resolve it into a set of operations.
-// With the store as context, we can determine if a tail exists locally or not and therefore if it needs uploading or download.
-// In theory this could be done as a part of the diffing stage, but it's easier to reason
-// about and test this way
-pub async fn operations(
-    diffs: Vec<Diff>,
-    _store: &impl Store,
-) -> Result<Vec<Operation>, SyncError> {
-    let mut operations = Vec::with_capacity(diffs.len());
-
-    for diff in diffs {
-        let op = match (diff.local, diff.remote) {
-            // We both have it! Could be either. Compare.
-            (Some(local), Some(remote)) => match local.cmp(&remote) {
-                Ordering::Equal => Operation::Noop {
-                    host: diff.host,
-                    tag: diff.tag,
-                },
-                Ordering::Greater => Operation::Upload {
-                    local,
-                    remote: Some(remote),
-                    host: diff.host,
-                    tag: diff.tag,
-                },
-                Ordering::Less => Operation::Download {
-                    local: Some(local),
-                    remote,
-                    host: diff.host,
-                    tag: diff.tag,
-                },
-            },
-
-            // Remote has it, we don't. Gotta be download
-            (None, Some(remote)) => Operation::Download {
-                local: None,
-                remote,
-                host: diff.host,
-                tag: diff.tag,
-            },
-
-            // We have it, remote doesn't. Gotta be upload.
-            (Some(local), None) => Operation::Upload {
-                local,
-                remote: None,
-                host: diff.host,
-                tag: diff.tag,
-            },
-
-            // something is pretty fucked.
-            (None, None) => {
-                return Err(SyncError::SyncLogicError {
-                    msg: String::from(
-                        "diff has nothing for local or remote - (host, tag) does not exist",
-                    ),
-                });
-            }
-        };
-
-        operations.push(op);
-    }
-
-    // sort them - purely so we have a stable testing order, and can rely on
-    // same input = same output
-    // We can sort by ID so long as we continue to use UUIDv7 or something
-    // with the same properties
-
-    operations.sort_by_key(|op| match op {
-        Operation::Noop { host, tag } => (0, *host, tag.clone()),
-
-        Operation::Upload { host, tag, .. } => (1, *host, tag.clone()),
-
-        Operation::Download { host, tag, .. } => (2, *host, tag.clone()),
-    });
-
-    Ok(operations)
-}
-
-async fn sync_upload(
-    store: &impl Store,
-    client: &Client<'_>,
-    host: HostId,
-    tag: String,
-    local: RecordIdx,
-    remote: Option<RecordIdx>,
-) -> Result<i64, SyncError> {
-    let remote = remote.unwrap_or(0);
-    let expected = local - remote;
-    let upload_page_size = 100;
-    let mut progress = 0;
-
-    let pb = ProgressBar::new(expected);
-    pb.set_style(ProgressStyle::with_template("{spinner:.green} [{elapsed_precise}] [{wide_bar:.cyan/blue}] {human_pos}/{human_len} ({eta})")
-        .unwrap()
-        .with_key("eta", |state: &ProgressState, w: &mut dyn Write| write!(w, "{:.1}s", state.eta().as_secs_f64()).unwrap())
-        .progress_chars("#>-"));
-
-    println!(
-        "Uploading {} records to {}/{}",
-        expected,
-        host.0.as_simple(),
-        tag
-    );
-
-    // preload with the first entry if remote does not know of this store
-    loop {
-        let page = store
-            .next(host, tag.as_str(), remote + progress, upload_page_size)
-            .await
-            .map_err(|e| {
-                error!("failed to read upload page: {e:?}");
-
-                SyncError::LocalStoreError { msg: e.to_string() }
-            })?;
-
-        client.post_records(&page).await.map_err(|e| {
-            error!("failed to post records: {e:?}");
-
-            SyncError::RemoteRequestError { msg: e.to_string() }
-        })?;
-
-        pb.set_position(progress);
-        progress += page.len() as u64;
-
-        if progress >= expected {
-            break;
-        }
-    }
-
-    pb.finish_with_message("Uploaded records");
-
-    Ok(progress as i64)
-}
-
-async fn sync_download(
-    store: &impl Store,
-    client: &Client<'_>,
-    host: HostId,
-    tag: String,
-    local: Option<RecordIdx>,
-    remote: RecordIdx,
-) -> Result<Vec<RecordId>, SyncError> {
-    let local = local.unwrap_or(0);
-    let expected = remote - local;
-    let download_page_size = 100;
-    let mut progress = 0;
-    let mut ret = Vec::new();
-
-    println!(
-        "Downloading {} records from {}/{}",
-        expected,
-        host.0.as_simple(),
-        tag
-    );
-
-    let pb = ProgressBar::new(expected);
-    pb.set_style(ProgressStyle::with_template("{spinner:.green} [{elapsed_precise}] [{wide_bar:.cyan/blue}] {human_pos}/{human_len} ({eta})")
-        .unwrap()
-        .with_key("eta", |state: &ProgressState, w: &mut dyn Write| write!(w, "{:.1}s", state.eta().as_secs_f64()).unwrap())
-        .progress_chars("#>-"));
-
-    // preload with the first entry if remote does not know of this store
-    loop {
-        let page = client
-            .next_records(host, tag.clone(), local + progress, download_page_size)
-            .await
-            .map_err(|e| SyncError::RemoteRequestError { msg: e.to_string() })?;
-
-        store
-            .push_batch(page.iter())
-            .await
-            .map_err(|e| SyncError::LocalStoreError { msg: e.to_string() })?;
-
-        ret.extend(page.iter().map(|f| f.id));
-
-        pb.set_position(progress);
-        progress += page.len() as u64;
-
-        if progress >= expected {
-            break;
-        }
-    }
-
-    pb.finish_with_message("Downloaded records");
-
-    Ok(ret)
-}
-
-pub async fn sync_remote(
-    operations: Vec<Operation>,
-    local_store: &impl Store,
-    settings: &Settings,
-) -> Result<(i64, Vec<RecordId>), SyncError> {
-    let client = Client::new(
-        &settings.sync_address,
-        settings
-            .session_token()
-            .map_err(|e| SyncError::RemoteRequestError { msg: e.to_string() })?
-            .as_str(),
-        settings.network_connect_timeout,
-        settings.network_timeout,
-    )
-    .expect("failed to create client");
-
-    let mut uploaded = 0;
-    let mut downloaded = Vec::new();
-
-    // this can totally run in parallel, but lets get it working first
-    for i in operations {
-        match i {
-            Operation::Upload {
-                host,
-                tag,
-                local,
-                remote,
-            } => uploaded += sync_upload(local_store, &client, host, tag, local, remote).await?,
-
-            Operation::Download {
-                host,
-                tag,
-                local,
-                remote,
-            } => {
-                let mut d = sync_download(local_store, &client, host, tag, local, remote).await?;
-                downloaded.append(&mut d)
-            }
-
-            Operation::Noop { .. } => continue,
-        }
-    }
-
-    Ok((uploaded, downloaded))
-}
-
-pub async fn sync(
-    settings: &Settings,
-    store: &impl Store,
-) -> Result<(i64, Vec<RecordId>), SyncError> {
-    let (diff, _) = diff(settings, store).await?;
-    let operations = operations(diff, store).await?;
-    let (uploaded, downloaded) = sync_remote(operations, store, settings).await?;
-
-    Ok((uploaded, downloaded))
-}
-
-#[cfg(test)]
-mod tests {
-    use atuin_common::record::{Diff, EncryptedData, HostId, Record};
-    use pretty_assertions::assert_eq;
-
-    use crate::{
-        record::{
-            encryption::PASETO_V4,
-            sqlite_store::SqliteStore,
-            store::Store,
-            sync::{self, Operation},
-        },
-        settings::test_local_timeout,
-    };
-
-    fn test_record() -> Record<EncryptedData> {
-        Record::builder()
-            .host(atuin_common::record::Host::new(HostId(
-                atuin_common::utils::uuid_v7(),
-            )))
-            .version("v1".into())
-            .tag(atuin_common::utils::uuid_v7().simple().to_string())
-            .data(EncryptedData {
-                data: String::new(),
-                content_encryption_key: String::new(),
-            })
-            .idx(0)
-            .build()
-    }
-
-    // Take a list of local records, and a list of remote records.
-    // Return the local database, and a diff of local/remote, ready to build
-    // ops
-    async fn build_test_diff(
-        local_records: Vec<Record<EncryptedData>>,
-        remote_records: Vec<Record<EncryptedData>>,
-    ) -> (SqliteStore, Vec<Diff>) {
-        let local_store = SqliteStore::new(":memory:", test_local_timeout())
-            .await
-            .expect("failed to open in memory sqlite");
-        let remote_store = SqliteStore::new(":memory:", test_local_timeout())
-            .await
-            .expect("failed to open in memory sqlite"); // "remote"
-
-        for i in local_records {
-            local_store.push(&i).await.unwrap();
-        }
-
-        for i in remote_records {
-            remote_store.push(&i).await.unwrap();
-        }
-
-        let local_index = local_store.status().await.unwrap();
-        let remote_index = remote_store.status().await.unwrap();
-
-        let diff = local_index.diff(&remote_index);
-
-        (local_store, diff)
-    }
-
-    #[tokio::test]
-    async fn test_basic_diff() {
-        // a diff where local is ahead of remote. nothing else.
-
-        let record = test_record();
-        let (store, diff) = build_test_diff(vec![record.clone()], vec![]).await;
-
-        assert_eq!(diff.len(), 1);
-
-        let operations = sync::operations(diff, &store).await.unwrap();
-
-        assert_eq!(operations.len(), 1);
-
-        assert_eq!(
-            operations[0],
-            Operation::Upload {
-                host: record.host.id,
-                tag: record.tag,
-                local: record.idx,
-                remote: None,
-            }
-        );
-    }
-
-    #[tokio::test]
-    async fn build_two_way_diff() {
-        // a diff where local is ahead of remote for one, and remote for
-        // another. One upload, one download
-
-        let shared_record = test_record();
-        let remote_ahead = test_record();
-
-        let local_ahead = shared_record
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-
-        assert_eq!(local_ahead.idx, 1);
-
-        let local = vec![shared_record.clone(), local_ahead.clone()]; // local knows about the already synced, and something newer in the same store
-        let remote = vec![shared_record.clone(), remote_ahead.clone()]; // remote knows about the already-synced, and one new record in a new store
-
-        let (store, diff) = build_test_diff(local, remote).await;
-        let operations = sync::operations(diff, &store).await.unwrap();
-
-        assert_eq!(operations.len(), 2);
-
-        assert_eq!(
-            operations,
-            vec![
-                // Or in otherwords, local is ahead by one
-                Operation::Upload {
-                    host: local_ahead.host.id,
-                    tag: local_ahead.tag,
-                    local: 1,
-                    remote: Some(0),
-                },
-                // Or in other words, remote knows of a record in an entirely new store (tag)
-                Operation::Download {
-                    host: remote_ahead.host.id,
-                    tag: remote_ahead.tag,
-                    local: None,
-                    remote: 0,
-                },
-            ]
-        );
-    }
-
-    #[tokio::test]
-    async fn build_complex_diff() {
-        // One shared, ahead but known only by remote
-        // One known only by local
-        // One known only by remote
-
-        let shared_record = test_record();
-        let local_only = test_record();
-
-        let local_only_20 = test_record();
-        let local_only_21 = local_only_20
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-        let local_only_22 = local_only_21
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-        let local_only_23 = local_only_22
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-
-        let remote_only = test_record();
-
-        let remote_only_20 = test_record();
-        let remote_only_21 = remote_only_20
-            .append(vec![2, 3, 2])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-        let remote_only_22 = remote_only_21
-            .append(vec![2, 3, 2])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-        let remote_only_23 = remote_only_22
-            .append(vec![2, 3, 2])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-        let remote_only_24 = remote_only_23
-            .append(vec![2, 3, 2])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-
-        let second_shared = test_record();
-        let second_shared_remote_ahead = second_shared
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-        let second_shared_remote_ahead2 = second_shared_remote_ahead
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-
-        let third_shared = test_record();
-        let third_shared_local_ahead = third_shared
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-        let third_shared_local_ahead2 = third_shared_local_ahead
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-
-        let fourth_shared = test_record();
-        let fourth_shared_remote_ahead = fourth_shared
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-        let fourth_shared_remote_ahead2 = fourth_shared_remote_ahead
-            .append(vec![1, 2, 3])
-            .encrypt::<PASETO_V4>(&[0; 32]);
-
-        let local = vec![
-            shared_record.clone(),
-            second_shared.clone(),
-            third_shared.clone(),
-            fourth_shared.clone(),
-            fourth_shared_remote_ahead.clone(),
-            // single store, only local has it
-            local_only.clone(),
-            // bigger store, also only known by local
-            local_only_20.clone(),
-            local_only_21.clone(),
-            local_only_22.clone(),
-            local_only_23.clone(),
-            // another shared store, but local is ahead on this one
-            third_shared_local_ahead.clone(),
-            third_shared_local_ahead2.clone(),
-        ];
-
-        let remote = vec![
-            remote_only.clone(),
-            remote_only_20.clone(),
-            remote_only_21.clone(),
-            remote_only_22.clone(),
-            remote_only_23.clone(),
-            remote_only_24.clone(),
-            shared_record.clone(),
-            second_shared.clone(),
-            third_shared.clone(),
-            second_shared_remote_ahead.clone(),
-            second_shared_remote_ahead2.clone(),
-            fourth_shared.clone(),
-            fourth_shared_remote_ahead.clone(),
-            fourth_shared_remote_ahead2.clone(),
-        ]; // remote knows about the already-synced, and one new record in a new store
-
-        let (store, diff) = build_test_diff(local, remote).await;
-        let operations = sync::operations(diff, &store).await.unwrap();
-
-        assert_eq!(operations.len(), 7);
-
-        let mut result_ops = vec![
-            // We started with a shared record, but the remote knows of two newer records in the
-            // same store
-            Operation::Download {
-                local: Some(0),
-                remote: 2,
-                host: second_shared_remote_ahead.host.id,
-                tag: second_shared_remote_ahead.tag,
-            },
-            // We have a shared record, local knows of the first two but not the last
-            Operation::Download {
-                local: Some(1),
-                remote: 2,
-                host: fourth_shared_remote_ahead2.host.id,
-                tag: fourth_shared_remote_ahead2.tag,
-            },
-            // Remote knows of a store with a single record that local does not have
-            Operation::Download {
-                local: None,
-                remote: 0,
-                host: remote_only.host.id,
-                tag: remote_only.tag,
-            },
-            // Remote knows of a store with a bunch of records that local does not have
-            Operation::Download {
-                local: None,
-                remote: 4,
-                host: remote_only_20.host.id,
-                tag: remote_only_20.tag,
-            },
-            // Local knows of a record in a store that remote does not have
-            Operation::Upload {
-                local: 0,
-                remote: None,
-                host: local_only.host.id,
-                tag: local_only.tag,
-            },
-            // Local knows of 4 records in a store that remote does not have
-            Operation::Upload {
-                local: 3,
-                remote: None,
-                host: local_only_20.host.id,
-                tag: local_only_20.tag,
-            },
-            // Local knows of 2 more records in a shared store that remote only has one of
-            Operation::Upload {
-                local: 2,
-                remote: Some(0),
-                host: third_shared.host.id,
-                tag: third_shared.tag,
-            },
-        ];
-
-        result_ops.sort_by_key(|op| match op {
-            Operation::Noop { host, tag } => (0, *host, tag.clone()),
-
-            Operation::Upload { host, tag, .. } => (1, *host, tag.clone()),
-
-            Operation::Download { host, tag, .. } => (2, *host, tag.clone()),
-        });
-
-        assert_eq!(result_ops, operations);
-    }
-}
diff --git a/crates/atuin-client/src/register.rs b/crates/atuin-client/src/register.rs
deleted file mode 100644
index dae01efd..00000000
--- a/crates/atuin-client/src/register.rs
+++ /dev/null
@@ -1,23 +0,0 @@
-use eyre::Result;
-use tokio::fs::File;
-use tokio::io::AsyncWriteExt;
-
-use crate::{api_client, settings::Settings};
-
-pub async fn register(
-    settings: &Settings,
-    username: String,
-    email: String,
-    password: String,
-) -> Result<String> {
-    let session =
-        api_client::register(settings.sync_address.as_str(), &username, &email, &password).await?;
-
-    let path = settings.session_path.as_str();
-    let mut file = File::create(path).await?;
-    file.write_all(session.session.as_bytes()).await?;
-
-    let _key = crate::encryption::load_key(settings)?;
-
-    Ok(session.session)
-}
diff --git a/crates/atuin-client/src/settings.rs b/crates/atuin-client/src/settings.rs
index d1f37e39..64013e1d 100644
--- a/crates/atuin-client/src/settings.rs
+++ b/crates/atuin-client/src/settings.rs
@@ -10,7 +10,6 @@ use config::{
 };
 use eyre::{Context, Error, Result, bail, eyre};
 use fs_err::{File, create_dir_all};
-use humantime::parse_duration;
 use regex::RegexSet;
 use semver::Version;
 use serde::{Deserialize, Serialize};
@@ -23,7 +22,6 @@ use time::{
 use uuid::Uuid;
 
 pub const HISTORY_PAGE_SIZE: i64 = 100;
-pub const LAST_SYNC_FILENAME: &str = "last_sync_time";
 pub const LAST_VERSION_CHECK_FILENAME: &str = "last_version_check_time";
 pub const LATEST_VERSION_FILENAME: &str = "latest_version";
 pub const HOST_ID_FILENAME: &str = "host_id";
@@ -327,11 +325,6 @@ impl Default for Stats {
     }
 }
 
-#[derive(Clone, Debug, Deserialize, Default, Serialize)]
-pub struct Sync {
-    pub records: bool,
-}
-
 #[derive(Clone, Debug, Deserialize, Default, Serialize)]
 pub struct Keys {
     pub scroll_exits: bool,
@@ -448,14 +441,10 @@ pub struct Settings {
     pub dialect: Dialect,
     pub timezone: Timezone,
     pub style: Style,
-    pub auto_sync: bool,
     pub update_check: bool,
-    pub sync_address: String,
-    pub sync_frequency: String,
     pub db_path: String,
     pub record_store_path: String,
     pub key_path: String,
-    pub session_path: String,
     pub search_mode: SearchMode,
     pub filter_mode: Option<FilterMode>,
     pub filter_mode_shell_up_key_binding: Option<FilterMode>,
@@ -489,8 +478,6 @@ pub struct Settings {
     pub workspaces: bool,
     pub ctrl_n_shortcuts: bool,
 
-    pub network_connect_timeout: u64,
-    pub network_timeout: u64,
     pub local_timeout: f64,
     pub enter_accept: bool,
     pub smart_sort: bool,
@@ -498,9 +485,6 @@ pub struct Settings {
     #[serde(default)]
     pub stats: Stats,
 
-    #[serde(default)]
-    pub sync: Sync,
-
     #[serde(default)]
     pub keys: Keys,
 
@@ -582,18 +566,10 @@ impl Settings {
         }
     }
 
-    pub fn save_sync_time() -> Result<()> {
-        Settings::save_current_time(LAST_SYNC_FILENAME)
-    }
-
     pub fn save_version_check_time() -> Result<()> {
         Settings::save_current_time(LAST_VERSION_CHECK_FILENAME)
     }
 
-    pub fn last_sync() -> Result<OffsetDateTime> {
-        Settings::load_time_from_file(LAST_SYNC_FILENAME)
-    }
-
     pub fn last_version_check() -> Result<OffsetDateTime> {
         Settings::load_time_from_file(LAST_VERSION_CHECK_FILENAME)
     }
@@ -615,39 +591,6 @@ impl Settings {
         Some(HostId(uuid))
     }
 
-    pub fn should_sync(&self) -> Result<bool> {
-        if !self.auto_sync || !PathBuf::from(self.session_path.as_str()).exists() {
-            return Ok(false);
-        }
-
-        if self.sync_frequency == "0" {
-            return Ok(true);
-        }
-
-        match parse_duration(self.sync_frequency.as_str()) {
-            Ok(d) => {
-                let d = time::Duration::try_from(d)?;
-                Ok(OffsetDateTime::now_utc() - Settings::last_sync()? >= d)
-            }
-            Err(e) => Err(eyre!("failed to check sync: {}", e)),
-        }
-    }
-
-    pub fn logged_in(&self) -> bool {
-        let session_path = self.session_path.as_str();
-
-        PathBuf::from(session_path).exists()
-    }
-
-    pub fn session_token(&self) -> Result<String> {
-        if !self.logged_in() {
-            return Err(eyre!("Tried to load session; not logged in"));
-        }
-
-        let session_path = self.session_path.as_str();
-        Ok(fs_err::read_to_string(session_path)?)
-    }
-
     #[cfg(feature = "check-update")]
     fn needs_update_check(&self) -> Result<bool> {
         let last_check = Settings::last_version_check()?;
@@ -681,10 +624,6 @@ impl Settings {
             return Ok(version);
         }
 
-        #[cfg(feature = "sync")]
-        let latest = crate::api_client::latest_version().await.unwrap_or(current);
-
-        #[cfg(not(feature = "sync"))]
         let latest = current;
 
         let latest_encoded = latest.to_string();
@@ -870,7 +809,6 @@ impl Settings {
         settings.db_path = Self::expand_path(settings.db_path)?;
         settings.record_store_path = Self::expand_path(settings.record_store_path)?;
         settings.key_path = Self::expand_path(settings.key_path)?;
-        settings.session_path = Self::expand_path(settings.session_path)?;
 
         Ok(settings)
     }
@@ -886,12 +824,7 @@ impl Settings {
     }
 
     pub fn paths_ok(&self) -> bool {
-        let paths = [
-            &self.db_path,
-            &self.record_store_path,
-            &self.key_path,
-            &self.session_path,
-        ];
+        let paths = [&self.db_path, &self.record_store_path, &self.key_path];
         paths.iter().all(|p| !utils::broken_symlink(p))
     }
 }
diff --git a/crates/atuin-client/src/sync.rs b/crates/atuin-client/src/sync.rs
deleted file mode 100644
index ac63c2c0..00000000
--- a/crates/atuin-client/src/sync.rs
+++ /dev/null
@@ -1,213 +0,0 @@
-use std::collections::HashSet;
-use std::iter::FromIterator;
-
-use eyre::Result;
-
-use atuin_common::api::AddHistoryRequest;
-use crypto_secretbox::Key;
-use time::OffsetDateTime;
-
-use crate::{
-    api_client,
-    database::Database,
-    encryption::{decrypt, encrypt, load_key},
-    settings::Settings,
-};
-
-pub fn hash_str(string: &str) -> String {
-    use sha2::{Digest, Sha256};
-    let mut hasher = Sha256::new();
-    hasher.update(string.as_bytes());
-    hex::encode(hasher.finalize())
-}
-
-// Currently sync is kinda naive, and basically just pages backwards through
-// history. This means newly added stuff shows up properly! We also just use
-// the total count in each database to indicate whether a sync is needed.
-// I think this could be massively improved! If we had a way of easily
-// indicating count per time period (hour, day, week, year, etc) then we can
-// easily pinpoint where we are missing data and what needs downloading. Start
-// with year, then find the week, then the day, then the hour, then download it
-// all! The current naive approach will do for now.
-
-// Check if remote has things we don't, and if so, download them.
-// Returns (num downloaded, total local)
-async fn sync_download(
-    key: &Key,
-    force: bool,
-    client: &api_client::Client<'_>,
-    db: &impl Database,
-) -> Result<(i64, i64)> {
-    debug!("starting sync download");
-
-    let remote_status = client.status().await?;
-    let remote_count = remote_status.count;
-
-    // useful to ensure we don't even save something that hasn't yet been synced + deleted
-    let remote_deleted =
-        HashSet::<&str>::from_iter(remote_status.deleted.iter().map(String::as_str));
-
-    let initial_local = db.history_count(true).await?;
-    let mut local_count = initial_local;
-
-    let mut last_sync = if force {
-        OffsetDateTime::UNIX_EPOCH
-    } else {
-        Settings::last_sync()?
-    };
-
-    let mut last_timestamp = OffsetDateTime::UNIX_EPOCH;
-
-    let host = if force { Some(String::from("")) } else { None };
-
-    while remote_count > local_count {
-        let page = client
-            .get_history(last_sync, last_timestamp, host.clone())
-            .await?;
-
-        let history: Vec<_> = page
-            .history
-            .iter()
-            // TODO: handle deletion earlier in this chain
-            .map(|h| serde_json::from_str(h).expect("invalid base64"))
-            .map(|h| decrypt(h, key).expect("failed to decrypt history! check your key"))
-            .map(|mut h| {
-                if remote_deleted.contains(h.id.0.as_str()) {
-                    h.deleted_at = Some(time::OffsetDateTime::now_utc());
-                    h.command = String::from("");
-                }
-
-                h
-            })
-            .collect();
-
-        db.save_bulk(&history).await?;
-
-        local_count = db.history_count(true).await?;
-        let remote_page_size = std::cmp::max(remote_status.page_size, 0) as usize;
-
-        if history.len() < remote_page_size {
-            break;
-        }
-
-        let page_last = history
-            .last()
-            .expect("could not get last element of page")
-            .timestamp;
-
-        // in the case of a small sync frequency, it's possible for history to
-        // be "lost" between syncs. In this case we need to rewind the sync
-        // timestamps
-        if page_last == last_timestamp {
-            last_timestamp = OffsetDateTime::UNIX_EPOCH;
-            last_sync -= time::Duration::hours(1);
-        } else {
-            last_timestamp = page_last;
-        }
-    }
-
-    for i in remote_status.deleted {
-        // we will update the stored history to have this data
-        // pretty much everything can be nullified
-        match db.load(i.as_str()).await? {
-            Some(h) => {
-                db.delete(h).await?;
-            }
-            _ => {
-                info!(
-                    "could not delete history with id {}, not found locally",
-                    i.as_str()
-                );
-            }
-        }
-    }
-
-    Ok((local_count - initial_local, local_count))
-}
-
-// Check if we have things remote doesn't, and if so, upload them
-async fn sync_upload(
-    key: &Key,
-    _force: bool,
-    client: &api_client::Client<'_>,
-    db: &impl Database,
-) -> Result<()> {
-    debug!("starting sync upload");
-
-    let remote_status = client.status().await?;
-    let remote_deleted: HashSet<String> = HashSet::from_iter(remote_status.deleted.clone());
-
-    let initial_remote_count = client.count().await?;
-    let mut remote_count = initial_remote_count;
-
-    let local_count = db.history_count(true).await?;
-
-    debug!("remote has {}, we have {}", remote_count, local_count);
-
-    // first just try the most recent set
-    let mut cursor = OffsetDateTime::now_utc();
-
-    while local_count > remote_count {
-        let last = db.before(cursor, remote_status.page_size).await?;
-        let mut buffer = Vec::new();
-
-        if last.is_empty() {
-            break;
-        }
-
-        for i in last {
-            let data = encrypt(&i, key)?;
-            let data = serde_json::to_string(&data)?;
-
-            let add_hist = AddHistoryRequest {
-                id: i.id.to_string(),
-                timestamp: i.timestamp,
-                data,
-                hostname: hash_str(&i.hostname),
-            };
-
-            buffer.push(add_hist);
-        }
-
-        // anything left over outside of the 100 block size
-        client.post_history(&buffer).await?;
-        cursor = buffer.last().unwrap().timestamp;
-        remote_count = client.count().await?;
-
-        debug!("upload cursor: {:?}", cursor);
-    }
-
-    let deleted = db.deleted().await?;
-
-    for i in deleted {
-        if remote_deleted.contains(&i.id.to_string()) {
-            continue;
-        }
-
-        info!("deleting {} on remote", i.id);
-        client.delete_history(i).await?;
-    }
-
-    Ok(())
-}
-
-pub async fn sync(settings: &Settings, force: bool, db: &impl Database) -> Result<()> {
-    let client = api_client::Client::new(
-        &settings.sync_address,
-        settings.session_token()?.as_str(),
-        settings.network_connect_timeout,
-        settings.network_timeout,
-    )?;
-
-    Settings::save_sync_time()?;
-
-    let key = load_key(settings)?; // encryption key
-
-    sync_upload(&key, force, &client, db).await?;
-
-    let download = sync_download(&key, force, &client, db).await?;
-
-    debug!("sync downloaded {}", download.0);
-
-    Ok(())
-}
diff --git a/crates/atuin-daemon/src/server.rs b/crates/atuin-daemon/src/server.rs
index efed0ee3..2f6b61d1 100644
--- a/crates/atuin-daemon/src/server.rs
+++ b/crates/atuin-daemon/src/server.rs
@@ -19,8 +19,6 @@ use crate::history::history_server::{History as HistorySvc, HistoryServer};
 
 use crate::history::{EndHistoryReply, EndHistoryRequest, StartHistoryReply, StartHistoryRequest};
 
-mod sync;
-
 #[derive(Debug)]
 pub struct HistoryService {
     // A store for WIP history
@@ -258,13 +256,5 @@ pub async fn listen(
 
     let history = HistoryService::new(history_store.clone(), history_db.clone());
 
-    // start services
-    tokio::spawn(sync::worker(
-        settings.clone(),
-        store,
-        history_store,
-        history_db,
-    ));
-
     start_server(settings, history).await
 }
diff --git a/crates/atuin-daemon/src/server/sync.rs b/crates/atuin-daemon/src/server/sync.rs
deleted file mode 100644
index 3aa5dec3..00000000
--- a/crates/atuin-daemon/src/server/sync.rs
+++ /dev/null
@@ -1,88 +0,0 @@
-use eyre::Result;
-use rand::Rng;
-use tokio::time::{self, MissedTickBehavior};
-
-use atuin_client::database::Sqlite as HistoryDatabase;
-use atuin_client::{
-    encryption,
-    history::store::HistoryStore,
-    record::{sqlite_store::SqliteStore, sync},
-    settings::Settings,
-};
-
-use atuin_dotfiles::store::{AliasStore, var::VarStore};
-
-pub async fn worker(
-    settings: Settings,
-    store: SqliteStore,
-    history_store: HistoryStore,
-    history_db: HistoryDatabase,
-) -> Result<()> {
-    tracing::info!("booting sync worker");
-
-    let encryption_key: [u8; 32] = encryption::load_key(&settings)?.into();
-    let host_id = Settings::host_id().expect("failed to get host_id");
-    let alias_store = AliasStore::new(store.clone(), host_id, encryption_key);
-    let var_store = VarStore::new(store.clone(), host_id, encryption_key);
-
-    // Don't backoff by more than 30 mins (with a random jitter of up to 1 min)
-    let max_interval: f64 = 60.0 * 30.0 + rand::thread_rng().gen_range(0.0..60.0);
-
-    let mut ticker = time::interval(time::Duration::from_secs(settings.daemon.sync_frequency));
-
-    // IMPORTANT: without this, if we miss ticks because a sync takes ages or is otherwise delayed,
-    // we may end up running a lot of syncs in a hot loop. No bueno!
-    ticker.set_missed_tick_behavior(MissedTickBehavior::Skip);
-
-    loop {
-        ticker.tick().await;
-        tracing::info!("sync worker tick");
-
-        if !settings.logged_in() {
-            tracing::debug!("not logged in, skipping sync tick");
-            continue;
-        }
-
-        let res = sync::sync(&settings, &store).await;
-
-        if let Err(e) = res {
-            tracing::error!("sync tick failed with {e}");
-
-            let mut rng = rand::thread_rng();
-
-            let mut new_interval = ticker.period().as_secs_f64() * rng.gen_range(2.0..2.2);
-
-            if new_interval > max_interval {
-                new_interval = max_interval;
-            }
-
-            ticker = time::interval(time::Duration::from_secs(new_interval as u64));
-            ticker.reset_after(time::Duration::from_secs(new_interval as u64));
-
-            tracing::error!("backing off, next sync tick in {new_interval}");
-        } else {
-            let (uploaded, downloaded) = res.unwrap();
-
-            tracing::info!(
-                uploaded = ?uploaded,
-                downloaded = ?downloaded,
-                "sync complete"
-            );
-
-            history_store
-                .incremental_build(&history_db, &downloaded)
-                .await?;
-
-            alias_store.build().await?;
-            var_store.build().await?;
-
-            // Reset backoff on success
-            if ticker.period().as_secs() != settings.daemon.sync_frequency {
-                ticker = time::interval(time::Duration::from_secs(settings.daemon.sync_frequency));
-            }
-
-            // store sync time
-            tokio::task::spawn_blocking(Settings::save_sync_time).await??;
-        }
-    }
-}
diff --git a/crates/atuin-server-database/Cargo.toml b/crates/atuin-server-database/Cargo.toml
deleted file mode 100644
index e3e38e3f..00000000
--- a/crates/atuin-server-database/Cargo.toml
+++ /dev/null
@@ -1,19 +0,0 @@
-[package]
-name = "atuin-server-database"
-edition = "2024"
-description = "server database library for atuin"
-
-version = { workspace = true }
-authors = { workspace = true }
-license = { workspace = true }
-homepage = { workspace = true }
-repository = { workspace = true }
-
-[dependencies]
-atuin-common = { path = "../atuin-common", version = "18.6.1" }
-
-tracing = { workspace = true }
-time = { workspace = true }
-eyre = { workspace = true }
-serde = { workspace = true }
-async-trait = { workspace = true }
diff --git a/crates/atuin-server-database/src/calendar.rs b/crates/atuin-server-database/src/calendar.rs
deleted file mode 100644
index 2229667b..00000000
--- a/crates/atuin-server-database/src/calendar.rs
+++ /dev/null
@@ -1,18 +0,0 @@
-// Calendar data
-
-use serde::{Deserialize, Serialize};
-use time::Month;
-
-pub enum TimePeriod {
-    Year,
-    Month { year: i32 },
-    Day { year: i32, month: Month },
-}
-
-#[derive(Debug, Serialize, Deserialize)]
-pub struct TimePeriodInfo {
-    pub count: u64,
-
-    // TODO: Use this for merkle tree magic
-    pub hash: String,
-}
diff --git a/crates/atuin-server-database/src/lib.rs b/crates/atuin-server-database/src/lib.rs
deleted file mode 100644
index 1c577f59..00000000
--- a/crates/atuin-server-database/src/lib.rs
+++ /dev/null
@@ -1,178 +0,0 @@
-#![forbid(unsafe_code)]
-
-pub mod calendar;
-pub mod models;
-
-use std::{
-    collections::HashMap,
-    fmt::{Debug, Display},
-    ops::Range,
-};
-
-use self::{
-    calendar::{TimePeriod, TimePeriodInfo},
-    models::{History, NewHistory, NewSession, NewUser, Session, User},
-};
-use async_trait::async_trait;
-use atuin_common::record::{EncryptedData, HostId, Record, RecordIdx, RecordStatus};
-use serde::{Serialize, de::DeserializeOwned};
-use time::{Date, Duration, Month, OffsetDateTime, Time, UtcOffset};
-use tracing::instrument;
-
-#[derive(Debug)]
-pub enum DbError {
-    NotFound,
-    Other(eyre::Report),
-}
-
-impl Display for DbError {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        write!(f, "{self:?}")
-    }
-}
-
-impl<T: std::error::Error + Into<time::error::Error>> From<T> for DbError {
-    fn from(value: T) -> Self {
-        DbError::Other(value.into().into())
-    }
-}
-
-impl std::error::Error for DbError {}
-
-pub type DbResult<T> = Result<T, DbError>;
-
-#[async_trait]
-pub trait Database: Sized + Clone + Send + Sync + 'static {
-    type Settings: Debug + Clone + DeserializeOwned + Serialize + Send + Sync + 'static;
-    async fn new(settings: &Self::Settings) -> DbResult<Self>;
-
-    async fn get_session(&self, token: &str) -> DbResult<Session>;
-    async fn get_session_user(&self, token: &str) -> DbResult<User>;
-    async fn add_session(&self, session: &NewSession) -> DbResult<()>;
-
-    async fn get_user(&self, username: &str) -> DbResult<User>;
-    async fn get_user_session(&self, u: &User) -> DbResult<Session>;
-    async fn add_user(&self, user: &NewUser) -> DbResult<i64>;
-
-    async fn user_verified(&self, id: i64) -> DbResult<bool>;
-    async fn verify_user(&self, id: i64) -> DbResult<()>;
-    async fn user_verification_token(&self, id: i64) -> DbResult<String>;
-
-    async fn update_user_password(&self, u: &User) -> DbResult<()>;
-
-    async fn total_history(&self) -> DbResult<i64>;
-    async fn count_history(&self, user: &User) -> DbResult<i64>;
-    async fn count_history_cached(&self, user: &User) -> DbResult<i64>;
-
-    async fn delete_user(&self, u: &User) -> DbResult<()>;
-    async fn delete_history(&self, user: &User, id: String) -> DbResult<()>;
-    async fn deleted_history(&self, user: &User) -> DbResult<Vec<String>>;
-    async fn delete_store(&self, user: &User) -> DbResult<()>;
-
-    async fn add_records(&self, user: &User, record: &[Record<EncryptedData>]) -> DbResult<()>;
-    async fn next_records(
-        &self,
-        user: &User,
-        host: HostId,
-        tag: String,
-        start: Option<RecordIdx>,
-        count: u64,
-    ) -> DbResult<Vec<Record<EncryptedData>>>;
-
-    // Return the tail record ID for each store, so (HostID, Tag, TailRecordID)
-    async fn status(&self, user: &User) -> DbResult<RecordStatus>;
-
-    async fn count_history_range(&self, user: &User, range: Range<OffsetDateTime>)
-    -> DbResult<i64>;
-
-    async fn list_history(
-        &self,
-        user: &User,
-        created_after: OffsetDateTime,
-        since: OffsetDateTime,
-        host: &str,
-        page_size: i64,
-    ) -> DbResult<Vec<History>>;
-
-    async fn add_history(&self, history: &[NewHistory]) -> DbResult<()>;
-
-    async fn oldest_history(&self, user: &User) -> DbResult<History>;
-
-    #[instrument(skip_all)]
-    async fn calendar(
-        &self,
-        user: &User,
-        period: TimePeriod,
-        tz: UtcOffset,
-    ) -> DbResult<HashMap<u64, TimePeriodInfo>> {
-        let mut ret = HashMap::new();
-        let iter: Box<dyn Iterator<Item = DbResult<(u64, Range<Date>)>> + Send> = match period {
-            TimePeriod::Year => {
-                // First we need to work out how far back to calculate. Get the
-                // oldest history item
-                let oldest = self
-                    .oldest_history(user)
-                    .await?
-                    .timestamp
-                    .to_offset(tz)
-                    .year();
-                let current_year = OffsetDateTime::now_utc().to_offset(tz).year();
-
-                // All the years we need to get data for
-                // The upper bound is exclusive, so include current +1
-                let years = oldest..current_year + 1;
-
-                Box::new(years.map(|year| {
-                    let start = Date::from_calendar_date(year, time::Month::January, 1)?;
-                    let end = Date::from_calendar_date(year + 1, time::Month::January, 1)?;
-
-                    Ok((year as u64, start..end))
-                }))
-            }
-
-            TimePeriod::Month { year } => {
-                let months =
-                    std::iter::successors(Some(Month::January), |m| Some(m.next())).take(12);
-
-                Box::new(months.map(move |month| {
-                    let start = Date::from_calendar_date(year, month, 1)?;
-                    let days = start.month().length(year);
-                    let end = start + Duration::days(days as i64);
-
-                    Ok((month as u64, start..end))
-                }))
-            }
-
-            TimePeriod::Day { year, month } => {
-                let days = 1..month.length(year);
-                Box::new(days.map(move |day| {
-                    let start = Date::from_calendar_date(year, month, day)?;
-                    let end = start
-                        .next_day()
-                        .ok_or_else(|| DbError::Other(eyre::eyre!("no next day?")))?;
-
-                    Ok((day as u64, start..end))
-                }))
-            }
-        };
-
-        for x in iter {
-            let (index, range) = x?;
-
-            let start = range.start.with_time(Time::MIDNIGHT).assume_offset(tz);
-            let end = range.end.with_time(Time::MIDNIGHT).assume_offset(tz);
-
-            let count = self.count_history_range(user, start..end).await?;
-
-            ret.insert(
-                index,
-                TimePeriodInfo {
-                    count: count as u64,
-                    hash: "".to_string(),
-                },
-            );
-        }
-
-        Ok(ret)
-    }
-}
diff --git a/crates/atuin-server-database/src/models.rs b/crates/atuin-server-database/src/models.rs
deleted file mode 100644
index 894ac7f6..00000000
--- a/crates/atuin-server-database/src/models.rs
+++ /dev/null
@@ -1,53 +0,0 @@
-use time::OffsetDateTime;
-
-pub struct History {
-    pub id: i64,
-    pub client_id: String, // a client generated ID
-    pub user_id: i64,
-    pub hostname: String,
-    pub timestamp: OffsetDateTime,
-
-    /// All the data we have about this command, encrypted.
-    ///
-    /// Currently this is an encrypted msgpack object, but this may change in the future.
-    pub data: String,
-
-    pub created_at: OffsetDateTime,
-}
-
-pub struct NewHistory {
-    pub client_id: String,
-    pub user_id: i64,
-    pub hostname: String,
-    pub timestamp: OffsetDateTime,
-
-    /// All the data we have about this command, encrypted.
-    ///
-    /// Currently this is an encrypted msgpack object, but this may change in the future.
-    pub data: String,
-}
-
-pub struct User {
-    pub id: i64,
-    pub username: String,
-    pub email: String,
-    pub password: String,
-    pub verified: Option<OffsetDateTime>,
-}
-
-pub struct Session {
-    pub id: i64,
-    pub user_id: i64,
-    pub token: String,
-}
-
-pub struct NewUser {
-    pub username: String,
-    pub email: String,
-    pub password: String,
-}
-
-pub struct NewSession {
-    pub user_id: i64,
-    pub token: String,
-}
diff --git a/crates/atuin-server-postgres/Cargo.toml b/crates/atuin-server-postgres/Cargo.toml
deleted file mode 100644
index 9eccca50..00000000
--- a/crates/atuin-server-postgres/Cargo.toml
+++ /dev/null
@@ -1,25 +0,0 @@
-[package]
-name = "atuin-server-postgres"
-edition = "2024"
-description = "server postgres database library for atuin"
-
-version = { workspace = true }
-authors = { workspace = true }
-license = { workspace = true }
-homepage = { workspace = true }
-repository = { workspace = true }
-
-[dependencies]
-atuin-common = { path = "../atuin-common", version = "18.6.1" }
-atuin-server-database = { path = "../atuin-server-database", version = "18.6.1" }
-
-eyre = { workspace = true }
-tracing = { workspace = true }
-time = { workspace = true }
-serde = { workspace = true }
-sqlx = { workspace = true }
-async-trait = { workspace = true }
-uuid = { workspace = true }
-metrics = "0.21.1"
-futures-util = "0.3"
-url = "2.5.2"
diff --git a/crates/atuin-server-postgres/build.rs b/crates/atuin-server-postgres/build.rs
deleted file mode 100644
index d5068697..00000000
--- a/crates/atuin-server-postgres/build.rs
+++ /dev/null
@@ -1,5 +0,0 @@
-// generated by `sqlx migrate build-script`
-fn main() {
-    // trigger recompilation when a new migration is added
-    println!("cargo:rerun-if-changed=migrations");
-}
diff --git a/crates/atuin-server-postgres/migrations/20210425153745_create_history.sql b/crates/atuin-server-postgres/migrations/20210425153745_create_history.sql
deleted file mode 100644
index 2c2d17b0..00000000
--- a/crates/atuin-server-postgres/migrations/20210425153745_create_history.sql
+++ /dev/null
@@ -1,11 +0,0 @@
-create table history (
-	id bigserial primary key,
-	client_id text not null unique, -- the client-generated ID
-	user_id bigserial not null,     -- allow multiple users
-	hostname text not null,         -- a unique identifier from the client (can be hashed, random, whatever)
-	timestamp timestamp not null,   -- one of the few non-encrypted metadatas
-
-	data varchar(8192) not null,    -- store the actual history data, encrypted. I don't wanna know!
-
-	created_at timestamp not null default current_timestamp
-);
diff --git a/crates/atuin-server-postgres/migrations/20210425153757_create_users.sql b/crates/atuin-server-postgres/migrations/20210425153757_create_users.sql
deleted file mode 100644
index a25dcced..00000000
--- a/crates/atuin-server-postgres/migrations/20210425153757_create_users.sql
+++ /dev/null
@@ -1,10 +0,0 @@
-create table users (
-	id bigserial primary key,               -- also store our own ID
-	username varchar(32) not null unique,   -- being able to contact users is useful
-	email varchar(128) not null unique,     -- being able to contact users is useful
-	password varchar(128) not null unique
-);
-
--- the prior index is case sensitive :(
-CREATE UNIQUE INDEX email_unique_idx on users (LOWER(email));
-CREATE UNIQUE INDEX username_unique_idx on users (LOWER(username));
diff --git a/crates/atuin-server-postgres/migrations/20210425153800_create_sessions.sql b/crates/atuin-server-postgres/migrations/20210425153800_create_sessions.sql
deleted file mode 100644
index c2fb6559..00000000
--- a/crates/atuin-server-postgres/migrations/20210425153800_create_sessions.sql
+++ /dev/null
@@ -1,6 +0,0 @@
--- Add migration script here
-create table sessions (
-	id bigserial primary key,
-	user_id bigserial,
-	token varchar(128) unique not null
-);
diff --git a/crates/atuin-server-postgres/migrations/20220419082412_add_count_trigger.sql b/crates/atuin-server-postgres/migrations/20220419082412_add_count_trigger.sql
deleted file mode 100644
index dd1afa88..00000000
--- a/crates/atuin-server-postgres/migrations/20220419082412_add_count_trigger.sql
+++ /dev/null
@@ -1,51 +0,0 @@
--- Prior to this, the count endpoint was super naive and just ran COUNT(1). 
--- This is slow asf. Now that we have an amount of actual traffic, 
--- stop doing that!
--- This basically maintains a count, so we can read ONE row, instead of ALL the
--- rows. Much better.
--- Future optimisation could use some sort of cache so we don't even need to hit
--- postgres at all.
-
-create table total_history_count_user(
-	id bigserial primary key, 
-	user_id bigserial,
-	total integer -- try and avoid using keywords - hence total, not count
-);
-
-create or replace function user_history_count()
-returns trigger as 
-$func$
-begin
-	if (TG_OP='INSERT') then
-		update total_history_count_user set total = total + 1 where user_id = new.user_id;
-
-		if not found then
-			insert into total_history_count_user(user_id, total) 
-			values (
-				new.user_id, 
-				(select count(1) from history where user_id = new.user_id)
-			);
-		end if;
-		
-	elsif (TG_OP='DELETE') then
-		update total_history_count_user set total = total - 1 where user_id = new.user_id;
-
-		if not found then
-			insert into total_history_count_user(user_id, total) 
-			values (
-				new.user_id, 
-				(select count(1) from history where user_id = new.user_id)
-			);
-		end if;
-	end if;
-
-	return NEW; -- this is actually ignored for an after trigger, but oh well
-end;
-$func$
-language plpgsql volatile -- pldfplplpflh
-cost 100; -- default value
-
-create trigger tg_user_history_count 
-	after insert or delete on history 
-	for each row 
-	execute procedure user_history_count();
diff --git a/crates/atuin-server-postgres/migrations/20220421073605_fix_count_trigger_delete.sql b/crates/atuin-server-postgres/migrations/20220421073605_fix_count_trigger_delete.sql
deleted file mode 100644
index 6198f300..00000000
--- a/crates/atuin-server-postgres/migrations/20220421073605_fix_count_trigger_delete.sql
+++ /dev/null
@@ -1,35 +0,0 @@
--- the old version of this function used NEW in the delete part when it should
--- use OLD
-
-create or replace function user_history_count()
-returns trigger as 
-$func$
-begin
-	if (TG_OP='INSERT') then
-		update total_history_count_user set total = total + 1 where user_id = new.user_id;
-
-		if not found then
-			insert into total_history_count_user(user_id, total) 
-			values (
-				new.user_id, 
-				(select count(1) from history where user_id = new.user_id)
-			);
-		end if;
-		
-	elsif (TG_OP='DELETE') then
-		update total_history_count_user set total = total - 1 where user_id = old.user_id;
-
-		if not found then
-			insert into total_history_count_user(user_id, total) 
-			values (
-				old.user_id, 
-				(select count(1) from history where user_id = old.user_id)
-			);
-		end if;
-	end if;
-
-	return NEW; -- this is actually ignored for an after trigger, but oh well
-end;
-$func$
-language plpgsql volatile -- pldfplplpflh
-cost 100; -- default value
diff --git a/crates/atuin-server-postgres/migrations/20220421174016_larger-commands.sql b/crates/atuin-server-postgres/migrations/20220421174016_larger-commands.sql
deleted file mode 100644
index 0ac43433..00000000
--- a/crates/atuin-server-postgres/migrations/20220421174016_larger-commands.sql
+++ /dev/null
@@ -1,3 +0,0 @@
--- Make it 4x larger. Most commands are less than this, but as it's base64
--- SOME are more than 8192. Should be enough for now.
-ALTER TABLE history ALTER COLUMN data TYPE varchar(32768);
diff --git a/crates/atuin-server-postgres/migrations/20220426172813_user-created-at.sql b/crates/atuin-server-postgres/migrations/20220426172813_user-created-at.sql
deleted file mode 100644
index a9138194..00000000
--- a/crates/atuin-server-postgres/migrations/20220426172813_user-created-at.sql
+++ /dev/null
@@ -1 +0,0 @@
-alter table users add column created_at timestamp not null default now();
diff --git a/crates/atuin-server-postgres/migrations/20220505082442_create-events.sql b/crates/atuin-server-postgres/migrations/20220505082442_create-events.sql
deleted file mode 100644
index 57e16ec7..00000000
--- a/crates/atuin-server-postgres/migrations/20220505082442_create-events.sql
+++ /dev/null
@@ -1,14 +0,0 @@
-create type event_type as enum ('create', 'delete');
- 
-create table events (
-	id bigserial primary key,
-	client_id text not null unique, -- the client-generated ID
-	user_id bigserial not null,     -- allow multiple users
-	hostname text not null,         -- a unique identifier from the client (can be hashed, random, whatever)
-	timestamp timestamp not null,   -- one of the few non-encrypted metadatas
-
-	event_type event_type,
-	data text not null,    -- store the actual history data, encrypted. I don't wanna know!
-
-	created_at timestamp not null default current_timestamp
-);
diff --git a/crates/atuin-server-postgres/migrations/20220610074049_history-length.sql b/crates/atuin-server-postgres/migrations/20220610074049_history-length.sql
deleted file mode 100644
index b1c23016..00000000
--- a/crates/atuin-server-postgres/migrations/20220610074049_history-length.sql
+++ /dev/null
@@ -1,2 +0,0 @@
--- Add migration script here
-alter table history alter column data type text;
diff --git a/crates/atuin-server-postgres/migrations/20230315220537_drop-events.sql b/crates/atuin-server-postgres/migrations/20230315220537_drop-events.sql
deleted file mode 100644
index fe3cae17..00000000
--- a/crates/atuin-server-postgres/migrations/20230315220537_drop-events.sql
+++ /dev/null
@@ -1,2 +0,0 @@
--- Add migration script here
-drop table events;
diff --git a/crates/atuin-server-postgres/migrations/20230315224203_create-deleted.sql b/crates/atuin-server-postgres/migrations/20230315224203_create-deleted.sql
deleted file mode 100644
index 9a9e6263..00000000
--- a/crates/atuin-server-postgres/migrations/20230315224203_create-deleted.sql
+++ /dev/null
@@ -1,5 +0,0 @@
--- Add migration script here
-alter table history add column if not exists deleted_at timestamp;
-
--- queries will all be selecting the ids of history for a user, that has been deleted
-create index if not exists history_deleted_index on history(client_id, user_id, deleted_at);
diff --git a/crates/atuin-server-postgres/migrations/20230515221038_trigger-delete-only.sql b/crates/atuin-server-postgres/migrations/20230515221038_trigger-delete-only.sql
deleted file mode 100644
index 3d0bba52..00000000
--- a/crates/atuin-server-postgres/migrations/20230515221038_trigger-delete-only.sql
+++ /dev/null
@@ -1,30 +0,0 @@
--- We do not need to run the trigger on deletes, as the only time we are deleting history is when the user
--- has already been deleted
--- This actually slows down deleting all the history a good bit!
-
-create or replace function user_history_count()
-returns trigger as 
-$func$
-begin
-	if (TG_OP='INSERT') then
-		update total_history_count_user set total = total + 1 where user_id = new.user_id;
-
-		if not found then
-			insert into total_history_count_user(user_id, total) 
-			values (
-				new.user_id, 
-				(select count(1) from history where user_id = new.user_id)
-			);
-		end if;
-	end if;
-
-	return NEW; -- this is actually ignored for an after trigger, but oh well
-end;
-$func$
-language plpgsql volatile -- pldfplplpflh
-cost 100; -- default value
-
-create or replace trigger tg_user_history_count 
-	after insert on history 
-	for each row 
-	execute procedure user_history_count();
diff --git a/crates/atuin-server-postgres/migrations/20230623070418_records.sql b/crates/atuin-server-postgres/migrations/20230623070418_records.sql
deleted file mode 100644
index 22437595..00000000
--- a/crates/atuin-server-postgres/migrations/20230623070418_records.sql
+++ /dev/null
@@ -1,15 +0,0 @@
--- Add migration script here
-create table records (
-	id uuid primary key,            -- remember to use uuidv7 for happy indices <3
-    client_id uuid not null,        -- I am too uncomfortable with the idea of a client-generated primary key
-	host uuid not null,             -- a unique identifier for the host
-	parent uuid default null,       -- the ID of the parent record, bearing in mind this is a linked list
-	timestamp bigint not null,      -- not a timestamp type, as those do not have nanosecond precision
-	version text not null,
-	tag text not null,              -- what is this? history, kv, whatever. Remember clients get a log per tag per host
-	data text not null,            -- store the actual history data, encrypted. I don't wanna know!
-	cek text not null,            
-
-	user_id bigint not null,        -- allow multiple users
-	created_at timestamp not null default current_timestamp
-);
diff --git a/crates/atuin-server-postgres/migrations/20231202170508_create-store.sql b/crates/atuin-server-postgres/migrations/20231202170508_create-store.sql
deleted file mode 100644
index ffb57966..00000000
--- a/crates/atuin-server-postgres/migrations/20231202170508_create-store.sql
+++ /dev/null
@@ -1,15 +0,0 @@
--- Add migration script here
-create table store (
-	id uuid primary key,            -- remember to use uuidv7 for happy indices <3
-  client_id uuid not null,        -- I am too uncomfortable with the idea of a client-generated primary key, even though it's fine mathematically
-	host uuid not null,             -- a unique identifier for the host
-	idx bigint not null,       -- the index of the record in this store, identified by (host, tag)
-	timestamp bigint not null,      -- not a timestamp type, as those do not have nanosecond precision
-	version text not null,
-	tag text not null,              -- what is this? history, kv, whatever. Remember clients get a log per tag per host
-	data text not null,            -- store the actual history data, encrypted. I don't wanna know!
-	cek text not null,            
-
-	user_id bigint not null,        -- allow multiple users
-	created_at timestamp not null default current_timestamp
-);
diff --git a/crates/atuin-server-postgres/migrations/20231203124112_create-store-idx.sql b/crates/atuin-server-postgres/migrations/20231203124112_create-store-idx.sql
deleted file mode 100644
index 56d67145..00000000
--- a/crates/atuin-server-postgres/migrations/20231203124112_create-store-idx.sql
+++ /dev/null
@@ -1,2 +0,0 @@
--- Add migration script here
-create unique index record_uniq ON store(user_id, host, tag, idx);
diff --git a/crates/atuin-server-postgres/migrations/20240108124837_drop-some-defaults.sql b/crates/atuin-server-postgres/migrations/20240108124837_drop-some-defaults.sql
deleted file mode 100644
index ad2af5a1..00000000
--- a/crates/atuin-server-postgres/migrations/20240108124837_drop-some-defaults.sql
+++ /dev/null
@@ -1,4 +0,0 @@
--- Add migration script here
-alter table history alter column user_id drop default;
-alter table sessions alter column user_id drop default;
-alter table total_history_count_user alter column user_id drop default;
diff --git a/crates/atuin-server-postgres/migrations/20240614104159_idx-cache.sql b/crates/atuin-server-postgres/migrations/20240614104159_idx-cache.sql
deleted file mode 100644
index 76425ed7..00000000
--- a/crates/atuin-server-postgres/migrations/20240614104159_idx-cache.sql
+++ /dev/null
@@ -1,8 +0,0 @@
-create table store_idx_cache(
-  id bigserial primary key, 
-  user_id bigint,
-
-  host uuid,
-  tag text,
-  idx bigint
-);
diff --git a/crates/atuin-server-postgres/migrations/20240621110731_user-verified.sql b/crates/atuin-server-postgres/migrations/20240621110731_user-verified.sql
deleted file mode 100644
index 6eba02ec..00000000
--- a/crates/atuin-server-postgres/migrations/20240621110731_user-verified.sql
+++ /dev/null
@@ -1,8 +0,0 @@
-alter table users add verified_at timestamp with time zone default null;
-
-create table user_verification_token(
-  id bigserial primary key, 
-  user_id bigint unique references users(id), 
-  token text, 
-  valid_until timestamp with time zone
-);
diff --git a/crates/atuin-server-postgres/migrations/20240702094825_idx_cache_index.sql b/crates/atuin-server-postgres/migrations/20240702094825_idx_cache_index.sql
deleted file mode 100644
index d1a7b194..00000000
--- a/crates/atuin-server-postgres/migrations/20240702094825_idx_cache_index.sql
+++ /dev/null
@@ -1 +0,0 @@
-create unique index store_idx_cache_uniq on store_idx_cache(user_id, host, tag);
diff --git a/crates/atuin-server-postgres/src/lib.rs b/crates/atuin-server-postgres/src/lib.rs
deleted file mode 100644
index 7c6d8f9a..00000000
--- a/crates/atuin-server-postgres/src/lib.rs
+++ /dev/null
@@ -1,713 +0,0 @@
-use std::collections::HashMap;
-use std::fmt::Debug;
-use std::ops::Range;
-
-use async_trait::async_trait;
-use atuin_common::record::{EncryptedData, HostId, Record, RecordIdx, RecordStatus};
-use atuin_common::utils::crypto_random_string;
-use atuin_server_database::models::{History, NewHistory, NewSession, NewUser, Session, User};
-use atuin_server_database::{Database, DbError, DbResult};
-use futures_util::TryStreamExt;
-use metrics::counter;
-use serde::{Deserialize, Serialize};
-use sqlx::Row;
-use sqlx::postgres::PgPoolOptions;
-
-use time::{OffsetDateTime, PrimitiveDateTime, UtcOffset};
-use tracing::{instrument, trace};
-use uuid::Uuid;
-use wrappers::{DbHistory, DbRecord, DbSession, DbUser};
-
-mod wrappers;
-
-const MIN_PG_VERSION: u32 = 14;
-
-#[derive(Clone)]
-pub struct Postgres {
-    pool: sqlx::Pool<sqlx::postgres::Postgres>,
-}
-
-#[derive(Clone, Deserialize, Serialize)]
-pub struct PostgresSettings {
-    pub db_uri: String,
-}
-
-// Do our best to redact passwords so they're not logged in the event of an error.
-impl Debug for PostgresSettings {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        let redacted_uri = url::Url::parse(&self.db_uri)
-            .map(|mut url| {
-                let _ = url.set_password(Some("****"));
-                url.to_string()
-            })
-            .unwrap_or(self.db_uri.clone());
-        f.debug_struct("PostgresSettings")
-            .field("db_uri", &redacted_uri)
-            .finish()
-    }
-}
-
-fn fix_error(error: sqlx::Error) -> DbError {
-    match error {
-        sqlx::Error::RowNotFound => DbError::NotFound,
-        error => DbError::Other(error.into()),
-    }
-}
-
-#[async_trait]
-impl Database for Postgres {
-    type Settings = PostgresSettings;
-    async fn new(settings: &PostgresSettings) -> DbResult<Self> {
-        let pool = PgPoolOptions::new()
-            .max_connections(100)
-            .connect(settings.db_uri.as_str())
-            .await
-            .map_err(fix_error)?;
-
-        // Call server_version_num to get the DB server's major version number
-        // The call returns None for servers older than 8.x.
-        let pg_major_version: u32 = pool
-            .acquire()
-            .await
-            .map_err(fix_error)?
-            .server_version_num()
-            .ok_or(DbError::Other(eyre::Report::msg(
-                "could not get PostgreSQL version",
-            )))?
-            / 10000;
-
-        if pg_major_version < MIN_PG_VERSION {
-            return Err(DbError::Other(eyre::Report::msg(format!(
-                "unsupported PostgreSQL version {}, minimum required is {}",
-                pg_major_version, MIN_PG_VERSION
-            ))));
-        }
-
-        sqlx::migrate!("./migrations")
-            .run(&pool)
-            .await
-            .map_err(|error| DbError::Other(error.into()))?;
-
-        Ok(Self { pool })
-    }
-
-    #[instrument(skip_all)]
-    async fn get_session(&self, token: &str) -> DbResult<Session> {
-        sqlx::query_as("select id, user_id, token from sessions where token = $1")
-            .bind(token)
-            .fetch_one(&self.pool)
-            .await
-            .map_err(fix_error)
-            .map(|DbSession(session)| session)
-    }
-
-    #[instrument(skip_all)]
-    async fn get_user(&self, username: &str) -> DbResult<User> {
-        sqlx::query_as(
-            "select id, username, email, password, verified_at from users where username = $1",
-        )
-        .bind(username)
-        .fetch_one(&self.pool)
-        .await
-        .map_err(fix_error)
-        .map(|DbUser(user)| user)
-    }
-
-    #[instrument(skip_all)]
-    async fn user_verified(&self, id: i64) -> DbResult<bool> {
-        let res: (bool,) =
-            sqlx::query_as("select verified_at is not null from users where id = $1")
-                .bind(id)
-                .fetch_one(&self.pool)
-                .await
-                .map_err(fix_error)?;
-
-        Ok(res.0)
-    }
-
-    #[instrument(skip_all)]
-    async fn verify_user(&self, id: i64) -> DbResult<()> {
-        sqlx::query(
-            "update users set verified_at = (current_timestamp at time zone 'utc') where id=$1",
-        )
-        .bind(id)
-        .execute(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        Ok(())
-    }
-
-    /// Return a valid verification token for the user
-    /// If the user does not have any token, create one, insert it, and return
-    /// If the user has a token, but it's invalid, delete it, create a new one, return
-    /// If the user already has a valid token, return it
-    #[instrument(skip_all)]
-    async fn user_verification_token(&self, id: i64) -> DbResult<String> {
-        const TOKEN_VALID_MINUTES: i64 = 15;
-
-        // First we check if there is a verification token
-        let token: Option<(String, sqlx::types::time::OffsetDateTime)> = sqlx::query_as(
-            "select token, valid_until from user_verification_token where user_id = $1",
-        )
-        .bind(id)
-        .fetch_optional(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        let token = if let Some((token, valid_until)) = token {
-            trace!("Token for user {id} valid until {valid_until}");
-
-            // We have a token, AND it's still valid
-            if valid_until > time::OffsetDateTime::now_utc() {
-                token
-            } else {
-                // token has expired. generate a new one, return it
-                let token = crypto_random_string::<24>();
-
-                sqlx::query("update user_verification_token set token = $2, valid_until = $3 where user_id=$1")
-                    .bind(id)
-                    .bind(&token)
-                    .bind(time::OffsetDateTime::now_utc() + time::Duration::minutes(TOKEN_VALID_MINUTES))
-                    .execute(&self.pool)
-                    .await
-                    .map_err(fix_error)?;
-
-                token
-            }
-        } else {
-            // No token in the database! Generate one, insert it
-            let token = crypto_random_string::<24>();
-
-            sqlx::query("insert into user_verification_token (user_id, token, valid_until) values ($1, $2, $3)")
-                .bind(id)
-                .bind(&token)
-                .bind(time::OffsetDateTime::now_utc() + time::Duration::minutes(TOKEN_VALID_MINUTES))
-                .execute(&self.pool)
-                .await
-                .map_err(fix_error)?;
-
-            token
-        };
-
-        Ok(token)
-    }
-
-    #[instrument(skip_all)]
-    async fn get_session_user(&self, token: &str) -> DbResult<User> {
-        sqlx::query_as(
-            "select users.id, users.username, users.email, users.password, users.verified_at from users 
-            inner join sessions 
-            on users.id = sessions.user_id 
-            and sessions.token = $1",
-        )
-        .bind(token)
-        .fetch_one(&self.pool)
-        .await
-        .map_err(fix_error)
-        .map(|DbUser(user)| user)
-    }
-
-    #[instrument(skip_all)]
-    async fn count_history(&self, user: &User) -> DbResult<i64> {
-        // The cache is new, and the user might not yet have a cache value.
-        // They will have one as soon as they post up some new history, but handle that
-        // edge case.
-
-        let res: (i64,) = sqlx::query_as(
-            "select count(1) from history
-            where user_id = $1",
-        )
-        .bind(user.id)
-        .fetch_one(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        Ok(res.0)
-    }
-
-    #[instrument(skip_all)]
-    async fn total_history(&self) -> DbResult<i64> {
-        // The cache is new, and the user might not yet have a cache value.
-        // They will have one as soon as they post up some new history, but handle that
-        // edge case.
-
-        let res: (i64,) = sqlx::query_as("select sum(total) from total_history_count_user")
-            .fetch_optional(&self.pool)
-            .await
-            .map_err(fix_error)?
-            .unwrap_or((0,));
-
-        Ok(res.0)
-    }
-
-    #[instrument(skip_all)]
-    async fn count_history_cached(&self, user: &User) -> DbResult<i64> {
-        let res: (i32,) = sqlx::query_as(
-            "select total from total_history_count_user
-            where user_id = $1",
-        )
-        .bind(user.id)
-        .fetch_one(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        Ok(res.0 as i64)
-    }
-
-    async fn delete_store(&self, user: &User) -> DbResult<()> {
-        sqlx::query(
-            "delete from store
-            where user_id = $1",
-        )
-        .bind(user.id)
-        .execute(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        Ok(())
-    }
-
-    async fn delete_history(&self, user: &User, id: String) -> DbResult<()> {
-        sqlx::query(
-            "update history
-            set deleted_at = $3
-            where user_id = $1
-            and client_id = $2
-            and deleted_at is null", // don't just keep setting it
-        )
-        .bind(user.id)
-        .bind(id)
-        .bind(OffsetDateTime::now_utc())
-        .fetch_all(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        Ok(())
-    }
-
-    #[instrument(skip_all)]
-    async fn deleted_history(&self, user: &User) -> DbResult<Vec<String>> {
-        // The cache is new, and the user might not yet have a cache value.
-        // They will have one as soon as they post up some new history, but handle that
-        // edge case.
-
-        let res = sqlx::query(
-            "select client_id from history 
-            where user_id = $1
-            and deleted_at is not null",
-        )
-        .bind(user.id)
-        .fetch_all(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        let res = res
-            .iter()
-            .map(|row| row.get::<String, _>("client_id"))
-            .collect();
-
-        Ok(res)
-    }
-
-    #[instrument(skip_all)]
-    async fn count_history_range(
-        &self,
-        user: &User,
-        range: Range<OffsetDateTime>,
-    ) -> DbResult<i64> {
-        let res: (i64,) = sqlx::query_as(
-            "select count(1) from history
-            where user_id = $1
-            and timestamp >= $2::date
-            and timestamp < $3::date",
-        )
-        .bind(user.id)
-        .bind(into_utc(range.start))
-        .bind(into_utc(range.end))
-        .fetch_one(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        Ok(res.0)
-    }
-
-    #[instrument(skip_all)]
-    async fn list_history(
-        &self,
-        user: &User,
-        created_after: OffsetDateTime,
-        since: OffsetDateTime,
-        host: &str,
-        page_size: i64,
-    ) -> DbResult<Vec<History>> {
-        let res = sqlx::query_as(
-            "select id, client_id, user_id, hostname, timestamp, data, created_at from history 
-            where user_id = $1
-            and hostname != $2
-            and created_at >= $3
-            and timestamp >= $4
-            order by timestamp asc
-            limit $5",
-        )
-        .bind(user.id)
-        .bind(host)
-        .bind(into_utc(created_after))
-        .bind(into_utc(since))
-        .bind(page_size)
-        .fetch(&self.pool)
-        .map_ok(|DbHistory(h)| h)
-        .try_collect()
-        .await
-        .map_err(fix_error)?;
-
-        Ok(res)
-    }
-
-    #[instrument(skip_all)]
-    async fn add_history(&self, history: &[NewHistory]) -> DbResult<()> {
-        let mut tx = self.pool.begin().await.map_err(fix_error)?;
-
-        for i in history {
-            let client_id: &str = &i.client_id;
-            let hostname: &str = &i.hostname;
-            let data: &str = &i.data;
-
-            sqlx::query(
-                "insert into history
-                    (client_id, user_id, hostname, timestamp, data) 
-                values ($1, $2, $3, $4, $5)
-                on conflict do nothing
-                ",
-            )
-            .bind(client_id)
-            .bind(i.user_id)
-            .bind(hostname)
-            .bind(i.timestamp)
-            .bind(data)
-            .execute(&mut *tx)
-            .await
-            .map_err(fix_error)?;
-        }
-
-        tx.commit().await.map_err(fix_error)?;
-
-        Ok(())
-    }
-
-    #[instrument(skip_all)]
-    async fn delete_user(&self, u: &User) -> DbResult<()> {
-        sqlx::query("delete from sessions where user_id = $1")
-            .bind(u.id)
-            .execute(&self.pool)
-            .await
-            .map_err(fix_error)?;
-
-        sqlx::query("delete from history where user_id = $1")
-            .bind(u.id)
-            .execute(&self.pool)
-            .await
-            .map_err(fix_error)?;
-
-        sqlx::query("delete from store where user_id = $1")
-            .bind(u.id)
-            .execute(&self.pool)
-            .await
-            .map_err(fix_error)?;
-
-        sqlx::query("delete from user_verification_token where user_id = $1")
-            .bind(u.id)
-            .execute(&self.pool)
-            .await
-            .map_err(fix_error)?;
-
-        sqlx::query("delete from total_history_count_user where user_id = $1")
-            .bind(u.id)
-            .execute(&self.pool)
-            .await
-            .map_err(fix_error)?;
-
-        sqlx::query("delete from users where id = $1")
-            .bind(u.id)
-            .execute(&self.pool)
-            .await
-            .map_err(fix_error)?;
-
-        Ok(())
-    }
-
-    #[instrument(skip_all)]
-    async fn update_user_password(&self, user: &User) -> DbResult<()> {
-        sqlx::query(
-            "update users
-            set password = $1
-            where id = $2",
-        )
-        .bind(&user.password)
-        .bind(user.id)
-        .execute(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        Ok(())
-    }
-
-    #[instrument(skip_all)]
-    async fn add_user(&self, user: &NewUser) -> DbResult<i64> {
-        let email: &str = &user.email;
-        let username: &str = &user.username;
-        let password: &str = &user.password;
-
-        let res: (i64,) = sqlx::query_as(
-            "insert into users
-                (username, email, password)
-            values($1, $2, $3)
-            returning id",
-        )
-        .bind(username)
-        .bind(email)
-        .bind(password)
-        .fetch_one(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        Ok(res.0)
-    }
-
-    #[instrument(skip_all)]
-    async fn add_session(&self, session: &NewSession) -> DbResult<()> {
-        let token: &str = &session.token;
-
-        sqlx::query(
-            "insert into sessions
-                (user_id, token)
-            values($1, $2)",
-        )
-        .bind(session.user_id)
-        .bind(token)
-        .execute(&self.pool)
-        .await
-        .map_err(fix_error)?;
-
-        Ok(())
-    }
-
-    #[instrument(skip_all)]
-    async fn get_user_session(&self, u: &User) -> DbResult<Session> {
-        sqlx::query_as("select id, user_id, token from sessions where user_id = $1")
-            .bind(u.id)
-            .fetch_one(&self.pool)
-            .await
-            .map_err(fix_error)
-            .map(|DbSession(session)| session)
-    }
-
-    #[instrument(skip_all)]
-    async fn oldest_history(&self, user: &User) -> DbResult<History> {
-        sqlx::query_as(
-            "select id, client_id, user_id, hostname, timestamp, data, created_at from history 
-            where user_id = $1
-            order by timestamp asc
-            limit 1",
-        )
-        .bind(user.id)
-        .fetch_one(&self.pool)
-        .await
-        .map_err(fix_error)
-        .map(|DbHistory(h)| h)
-    }
-
-    #[instrument(skip_all)]
-    async fn add_records(&self, user: &User, records: &[Record<EncryptedData>]) -> DbResult<()> {
-        let mut tx = self.pool.begin().await.map_err(fix_error)?;
-
-        // We won't have uploaded this data if it wasn't the max. Therefore, we can deduce the max
-        // idx without having to make further database queries. Doing the query on this small
-        // amount of data should be much, much faster.
-        //
-        // Worst case, say we get this wrong. We end up caching data that isn't actually the max
-        // idx, so clients upload again. The cache logic can be verified with a sql query anyway :)
-
-        let mut heads = HashMap::<(HostId, &str), u64>::new();
-
-        for i in records {
-            let id = atuin_common::utils::uuid_v7();
-
-            sqlx::query(
-                "insert into store
-                    (id, client_id, host, idx, timestamp, version, tag, data, cek, user_id) 
-                values ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
-                on conflict do nothing
-                ",
-            )
-            .bind(id)
-            .bind(i.id)
-            .bind(i.host.id)
-            .bind(i.idx as i64)
-            .bind(i.timestamp as i64) // throwing away some data, but i64 is still big in terms of time
-            .bind(&i.version)
-            .bind(&i.tag)
-            .bind(&i.data.data)
-            .bind(&i.data.content_encryption_key)
-            .bind(user.id)
-            .execute(&mut *tx)
-            .await
-            .map_err(fix_error)?;
-
-            // we're already iterating sooooo
-            heads
-                .entry((i.host.id, &i.tag))
-                .and_modify(|e| {
-                    if i.idx > *e {
-                        *e = i.idx
-                    }
-                })
-                .or_insert(i.idx);
-        }
-
-        // we've built the map of heads for this push, so commit it to the database
-        for ((host, tag), idx) in heads {
-            sqlx::query(
-                "insert into store_idx_cache
-                    (user_id, host, tag, idx) 
-                values ($1, $2, $3, $4)
-                on conflict(user_id, host, tag) do update set idx = greatest(store_idx_cache.idx, $4)
-                ",
-            )
-            .bind(user.id)
-            .bind(host)
-            .bind(tag)
-            .bind(idx as i64)
-            .execute(&mut *tx)
-            .await
-            .map_err(fix_error)?;
-        }
-
-        tx.commit().await.map_err(fix_error)?;
-
-        Ok(())
-    }
-
-    #[instrument(skip_all)]
-    async fn next_records(
-        &self,
-        user: &User,
-        host: HostId,
-        tag: String,
-        start: Option<RecordIdx>,
-        count: u64,
-    ) -> DbResult<Vec<Record<EncryptedData>>> {
-        tracing::debug!("{:?} - {:?} - {:?}", host, tag, start);
-        let start = start.unwrap_or(0);
-
-        let records: Result<Vec<DbRecord>, DbError> = sqlx::query_as(
-            "select client_id, host, idx, timestamp, version, tag, data, cek from store
-                    where user_id = $1
-                    and tag = $2
-                    and host = $3
-                    and idx >= $4
-                    order by idx asc
-                    limit $5",
-        )
-        .bind(user.id)
-        .bind(tag.clone())
-        .bind(host)
-        .bind(start as i64)
-        .bind(count as i64)
-        .fetch_all(&self.pool)
-        .await
-        .map_err(fix_error);
-
-        let ret = match records {
-            Ok(records) => {
-                let records: Vec<Record<EncryptedData>> = records
-                    .into_iter()
-                    .map(|f| {
-                        let record: Record<EncryptedData> = f.into();
-                        record
-                    })
-                    .collect();
-
-                records
-            }
-            Err(DbError::NotFound) => {
-                tracing::debug!("no records found in store: {:?}/{}", host, tag);
-                return Ok(vec![]);
-            }
-            Err(e) => return Err(e),
-        };
-
-        Ok(ret)
-    }
-
-    async fn status(&self, user: &User) -> DbResult<RecordStatus> {
-        const STATUS_SQL: &str =
-            "select host, tag, max(idx) from store where user_id = $1 group by host, tag";
-
-        let mut res: Vec<(Uuid, String, i64)> = sqlx::query_as(STATUS_SQL)
-            .bind(user.id)
-            .fetch_all(&self.pool)
-            .await
-            .map_err(fix_error)?;
-        res.sort();
-
-        // We're temporarily increasing latency in order to improve confidence in the cache
-        // If it runs for a few days, and we confirm that cached values are equal to realtime, we
-        // can replace realtime with cached.
-        //
-        // But let's check so sync doesn't do Weird Things.
-
-        let mut cached_res: Vec<(Uuid, String, i64)> =
-            sqlx::query_as("select host, tag, idx from store_idx_cache where user_id = $1")
-                .bind(user.id)
-                .fetch_all(&self.pool)
-                .await
-                .map_err(fix_error)?;
-        cached_res.sort();
-
-        let mut status = RecordStatus::new();
-
-        let equal = res == cached_res;
-
-        if equal {
-            counter!("atuin_store_idx_cache_consistent", 1);
-        } else {
-            // log the values if we have an inconsistent cache
-            tracing::debug!(user = user.username, cache_match = equal, res = ?res, cached = ?cached_res, "record store index request");
-            counter!("atuin_store_idx_cache_inconsistent", 1);
-        };
-
-        for i in res.iter() {
-            status.set_raw(HostId(i.0), i.1.clone(), i.2 as u64);
-        }
-
-        Ok(status)
-    }
-}
-
-fn into_utc(x: OffsetDateTime) -> PrimitiveDateTime {
-    let x = x.to_offset(UtcOffset::UTC);
-    PrimitiveDateTime::new(x.date(), x.time())
-}
-
-#[cfg(test)]
-mod tests {
-    use time::macros::datetime;
-
-    use crate::into_utc;
-
-    #[test]
-    fn utc() {
-        let dt = datetime!(2023-09-26 15:11:02 +05:30);
-        assert_eq!(into_utc(dt), datetime!(2023-09-26 09:41:02));
-        assert_eq!(into_utc(dt).assume_utc(), dt);
-
-        let dt = datetime!(2023-09-26 15:11:02 -07:00);
-        assert_eq!(into_utc(dt), datetime!(2023-09-26 22:11:02));
-        assert_eq!(into_utc(dt).assume_utc(), dt);
-
-        let dt = datetime!(2023-09-26 15:11:02 +00:00);
-        assert_eq!(into_utc(dt), datetime!(2023-09-26 15:11:02));
-        assert_eq!(into_utc(dt).assume_utc(), dt);
-    }
-}
diff --git a/crates/atuin-server-postgres/src/wrappers.rs b/crates/atuin-server-postgres/src/wrappers.rs
deleted file mode 100644
index 0d6a0ee6..00000000
--- a/crates/atuin-server-postgres/src/wrappers.rs
+++ /dev/null
@@ -1,78 +0,0 @@
-use ::sqlx::{FromRow, Result};
-use atuin_common::record::{EncryptedData, Host, Record};
-use atuin_server_database::models::{History, Session, User};
-use sqlx::{Row, postgres::PgRow};
-use time::PrimitiveDateTime;
-
-pub struct DbUser(pub User);
-pub struct DbSession(pub Session);
-pub struct DbHistory(pub History);
-pub struct DbRecord(pub Record<EncryptedData>);
-
-impl<'a> FromRow<'a, PgRow> for DbUser {
-    fn from_row(row: &'a PgRow) -> Result<Self> {
-        Ok(Self(User {
-            id: row.try_get("id")?,
-            username: row.try_get("username")?,
-            email: row.try_get("email")?,
-            password: row.try_get("password")?,
-            verified: row.try_get("verified_at")?,
-        }))
-    }
-}
-
-impl<'a> ::sqlx::FromRow<'a, PgRow> for DbSession {
-    fn from_row(row: &'a PgRow) -> ::sqlx::Result<Self> {
-        Ok(Self(Session {
-            id: row.try_get("id")?,
-            user_id: row.try_get("user_id")?,
-            token: row.try_get("token")?,
-        }))
-    }
-}
-
-impl<'a> ::sqlx::FromRow<'a, PgRow> for DbHistory {
-    fn from_row(row: &'a PgRow) -> ::sqlx::Result<Self> {
-        Ok(Self(History {
-            id: row.try_get("id")?,
-            client_id: row.try_get("client_id")?,
-            user_id: row.try_get("user_id")?,
-            hostname: row.try_get("hostname")?,
-            timestamp: row
-                .try_get::<PrimitiveDateTime, _>("timestamp")?
-                .assume_utc(),
-            data: row.try_get("data")?,
-            created_at: row
-                .try_get::<PrimitiveDateTime, _>("created_at")?
-                .assume_utc(),
-        }))
-    }
-}
-
-impl<'a> ::sqlx::FromRow<'a, PgRow> for DbRecord {
-    fn from_row(row: &'a PgRow) -> ::sqlx::Result<Self> {
-        let timestamp: i64 = row.try_get("timestamp")?;
-        let idx: i64 = row.try_get("idx")?;
-
-        let data = EncryptedData {
-            data: row.try_get("data")?,
-            content_encryption_key: row.try_get("cek")?,
-        };
-
-        Ok(Self(Record {
-            id: row.try_get("client_id")?,
-            host: Host::new(row.try_get("host")?),
-            idx: idx as u64,
-            timestamp: timestamp as u64,
-            version: row.try_get("version")?,
-            tag: row.try_get("tag")?,
-            data,
-        }))
-    }
-}
-
-impl From<DbRecord> for Record<EncryptedData> {
-    fn from(other: DbRecord) -> Record<EncryptedData> {
-        Record { ..other.0 }
-    }
-}
diff --git a/crates/atuin-server/Cargo.toml b/crates/atuin-server/Cargo.toml
deleted file mode 100644
index 53f9d499..00000000
--- a/crates/atuin-server/Cargo.toml
+++ /dev/null
@@ -1,37 +0,0 @@
-[package]
-name = "atuin-server"
-edition = "2024"
-description = "server library for atuin"
-
-rust-version = { workspace = true }
-version = { workspace = true }
-authors = { workspace = true }
-license = { workspace = true }
-homepage = { workspace = true }
-repository = { workspace = true }
-
-[dependencies]
-atuin-common = { path = "../atuin-common", version = "18.6.1" }
-atuin-server-database = { path = "../atuin-server-database", version = "18.6.1" }
-
-tracing = { workspace = true }
-time = { workspace = true }
-eyre = { workspace = true }
-config = { workspace = true }
-serde = { workspace = true }
-serde_json = { workspace = true }
-rand = { workspace = true }
-tokio = { workspace = true }
-async-trait = { workspace = true }
-axum = "0.7"
-axum-server = { version = "0.7", features = ["tls-rustls-no-provider"] }
-fs-err = { workspace = true }
-tower = { workspace = true }
-tower-http = { version = "0.6", features = ["trace"] }
-reqwest = { workspace = true }
-rustls = { version = "0.23", features = ["ring"], default-features = false }
-argon2 = "0.5"
-semver = { workspace = true }
-metrics-exporter-prometheus = "0.12.1"
-metrics = "0.21.1"
-postmark = {version= "0.11", features=["reqwest", "reqwest-rustls-tls"]}
diff --git a/crates/atuin-server/server.toml b/crates/atuin-server/server.toml
deleted file mode 100644
index 946769c9..00000000
--- a/crates/atuin-server/server.toml
+++ /dev/null
@@ -1,34 +0,0 @@
-## host to bind, can also be passed via CLI args
-# host = "127.0.0.1"
-
-## port to bind, can also be passed via CLI args
-# port = 8888
-
-## whether to allow anyone to register an account
-# open_registration = false
-
-## URI for postgres (using development creds here)
-# db_uri="postgres://username:password@localhost/atuin"
-
-## Maximum size for one history entry
-# max_history_length = 8192
-
-## Maximum size for one record entry
-## 1024 * 1024 * 1024
-# max_record_size = 1073741824
-
-## Webhook to be called when user registers on the servers
-# register_webhook_username = ""
-
-## Default page size for requests
-# page_size = 1100
-
-# [metrics]
-# enable = false
-# host = 127.0.0.1
-# port = 9001
-
-# [tls]
-# enable = false
-# cert_path = ""
-# pkey_path = ""
diff --git a/crates/atuin-server/src/handlers/health.rs b/crates/atuin-server/src/handlers/health.rs
deleted file mode 100644
index aebd1e8f..00000000
--- a/crates/atuin-server/src/handlers/health.rs
+++ /dev/null
@@ -1,15 +0,0 @@
-use axum::{Json, http, response::IntoResponse};
-
-use serde::Serialize;
-
-#[derive(Serialize)]
-pub struct HealthResponse {
-    pub status: &'static str,
-}
-
-pub async fn health_check() -> impl IntoResponse {
-    (
-        http::StatusCode::OK,
-        Json(HealthResponse { status: "healthy" }),
-    )
-}
diff --git a/crates/atuin-server/src/handlers/history.rs b/crates/atuin-server/src/handlers/history.rs
deleted file mode 100644
index 5547a180..00000000
--- a/crates/atuin-server/src/handlers/history.rs
+++ /dev/null
@@ -1,237 +0,0 @@
-use std::{collections::HashMap, convert::TryFrom};
-
-use axum::{
-    Json,
-    extract::{Path, Query, State},
-    http::{HeaderMap, StatusCode},
-};
-use metrics::counter;
-use time::{Month, UtcOffset};
-use tracing::{debug, error, instrument};
-
-use super::{ErrorResponse, ErrorResponseStatus, RespExt};
-use crate::{
-    router::{AppState, UserAuth},
-    utils::client_version_min,
-};
-use atuin_server_database::{
-    Database,
-    calendar::{TimePeriod, TimePeriodInfo},
-    models::NewHistory,
-};
-
-use atuin_common::api::*;
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn count<DB: Database>(
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-) -> Result<Json<CountResponse>, ErrorResponseStatus<'static>> {
-    let db = &state.0.database;
-    match db.count_history_cached(&user).await {
-        // By default read out the cached value
-        Ok(count) => Ok(Json(CountResponse { count })),
-
-        // If that fails, fallback on a full COUNT. Cache is built on a POST
-        // only
-        Err(_) => match db.count_history(&user).await {
-            Ok(count) => Ok(Json(CountResponse { count })),
-            Err(_) => Err(ErrorResponse::reply("failed to query history count")
-                .with_status(StatusCode::INTERNAL_SERVER_ERROR)),
-        },
-    }
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn list<DB: Database>(
-    req: Query<SyncHistoryRequest>,
-    UserAuth(user): UserAuth,
-    headers: HeaderMap,
-    state: State<AppState<DB>>,
-) -> Result<Json<SyncHistoryResponse>, ErrorResponseStatus<'static>> {
-    let db = &state.0.database;
-
-    let agent = headers
-        .get("user-agent")
-        .map_or("", |v| v.to_str().unwrap_or(""));
-
-    let variable_page_size = client_version_min(agent, ">=15.0.0").unwrap_or(false);
-
-    let page_size = if variable_page_size {
-        state.settings.page_size
-    } else {
-        100
-    };
-
-    if req.sync_ts.unix_timestamp_nanos() < 0 || req.history_ts.unix_timestamp_nanos() < 0 {
-        error!("client asked for history from < epoch 0");
-        counter!("atuin_history_epoch_before_zero", 1);
-
-        return Err(
-            ErrorResponse::reply("asked for history from before epoch 0")
-                .with_status(StatusCode::BAD_REQUEST),
-        );
-    }
-
-    let history = db
-        .list_history(&user, req.sync_ts, req.history_ts, &req.host, page_size)
-        .await;
-
-    if let Err(e) = history {
-        error!("failed to load history: {}", e);
-        return Err(ErrorResponse::reply("failed to load history")
-            .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-    }
-
-    let history: Vec<String> = history
-        .unwrap()
-        .iter()
-        .map(|i| i.data.to_string())
-        .collect();
-
-    debug!(
-        "loaded {} items of history for user {}",
-        history.len(),
-        user.id
-    );
-
-    counter!("atuin_history_returned", history.len() as u64);
-
-    Ok(Json(SyncHistoryResponse { history }))
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn delete<DB: Database>(
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-    Json(req): Json<DeleteHistoryRequest>,
-) -> Result<Json<MessageResponse>, ErrorResponseStatus<'static>> {
-    let db = &state.0.database;
-
-    // user_id is the ID of the history, as set by the user (the server has its own ID)
-    let deleted = db.delete_history(&user, req.client_id).await;
-
-    if let Err(e) = deleted {
-        error!("failed to delete history: {}", e);
-        return Err(ErrorResponse::reply("failed to delete history")
-            .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-    }
-
-    Ok(Json(MessageResponse {
-        message: String::from("deleted OK"),
-    }))
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn add<DB: Database>(
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-    Json(req): Json<Vec<AddHistoryRequest>>,
-) -> Result<(), ErrorResponseStatus<'static>> {
-    let State(AppState { database, settings }) = state;
-
-    debug!("request to add {} history items", req.len());
-    counter!("atuin_history_uploaded", req.len() as u64);
-
-    let mut history: Vec<NewHistory> = req
-        .into_iter()
-        .map(|h| NewHistory {
-            client_id: h.id,
-            user_id: user.id,
-            hostname: h.hostname,
-            timestamp: h.timestamp,
-            data: h.data,
-        })
-        .collect();
-
-    history.retain(|h| {
-        // keep if within limit, or limit is 0 (unlimited)
-        let keep = h.data.len() <= settings.max_history_length || settings.max_history_length == 0;
-
-        // Don't return an error here. We want to insert as much of the
-        // history list as we can, so log the error and continue going.
-        if !keep {
-            counter!("atuin_history_too_long", 1);
-
-            tracing::warn!(
-                "history too long, got length {}, max {}",
-                h.data.len(),
-                settings.max_history_length
-            );
-        }
-
-        keep
-    });
-
-    if let Err(e) = database.add_history(&history).await {
-        error!("failed to add history: {}", e);
-
-        return Err(ErrorResponse::reply("failed to add history")
-            .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-    };
-
-    Ok(())
-}
-
-#[derive(serde::Deserialize, Debug)]
-pub struct CalendarQuery {
-    #[serde(default = "serde_calendar::zero")]
-    year: i32,
-    #[serde(default = "serde_calendar::one")]
-    month: u8,
-
-    #[serde(default = "serde_calendar::utc")]
-    tz: UtcOffset,
-}
-
-mod serde_calendar {
-    use time::UtcOffset;
-
-    pub fn zero() -> i32 {
-        0
-    }
-
-    pub fn one() -> u8 {
-        1
-    }
-
-    pub fn utc() -> UtcOffset {
-        UtcOffset::UTC
-    }
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn calendar<DB: Database>(
-    Path(focus): Path<String>,
-    Query(params): Query<CalendarQuery>,
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-) -> Result<Json<HashMap<u64, TimePeriodInfo>>, ErrorResponseStatus<'static>> {
-    let focus = focus.as_str();
-
-    let year = params.year;
-    let month = Month::try_from(params.month).map_err(|e| ErrorResponseStatus {
-        error: ErrorResponse {
-            reason: e.to_string().into(),
-        },
-        status: StatusCode::BAD_REQUEST,
-    })?;
-
-    let period = match focus {
-        "year" => TimePeriod::Year,
-        "month" => TimePeriod::Month { year },
-        "day" => TimePeriod::Day { year, month },
-        _ => {
-            return Err(ErrorResponse::reply("invalid focus: use year/month/day")
-                .with_status(StatusCode::BAD_REQUEST));
-        }
-    };
-
-    let db = &state.0.database;
-    let focus = db.calendar(&user, period, params.tz).await.map_err(|_| {
-        ErrorResponse::reply("failed to query calendar")
-            .with_status(StatusCode::INTERNAL_SERVER_ERROR)
-    })?;
-
-    Ok(Json(focus))
-}
diff --git a/crates/atuin-server/src/handlers/mod.rs b/crates/atuin-server/src/handlers/mod.rs
deleted file mode 100644
index 1b9fd162..00000000
--- a/crates/atuin-server/src/handlers/mod.rs
+++ /dev/null
@@ -1,65 +0,0 @@
-use atuin_common::api::{ErrorResponse, IndexResponse};
-use atuin_server_database::Database;
-use axum::{Json, extract::State, http, response::IntoResponse};
-
-use crate::router::AppState;
-
-pub mod health;
-pub mod history;
-pub mod record;
-pub mod status;
-pub mod user;
-pub mod v0;
-
-const VERSION: &str = env!("CARGO_PKG_VERSION");
-
-pub async fn index<DB: Database>(state: State<AppState<DB>>) -> Json<IndexResponse> {
-    let homage = r#""Through the fathomless deeps of space swims the star turtle Great A'Tuin, bearing on its back the four giant elephants who carry on their shoulders the mass of the Discworld." -- Sir Terry Pratchett"#;
-
-    // Error with a -1 response
-    // It's super unlikely this will happen
-    let count = state.database.total_history().await.unwrap_or(-1);
-
-    let version = state
-        .settings
-        .fake_version
-        .clone()
-        .unwrap_or(VERSION.to_string());
-
-    Json(IndexResponse {
-        homage: homage.to_string(),
-        total_history: count,
-        version,
-    })
-}
-
-impl IntoResponse for ErrorResponseStatus<'_> {
-    fn into_response(self) -> axum::response::Response {
-        (self.status, Json(self.error)).into_response()
-    }
-}
-
-pub struct ErrorResponseStatus<'a> {
-    pub error: ErrorResponse<'a>,
-    pub status: http::StatusCode,
-}
-
-pub trait RespExt<'a> {
-    fn with_status(self, status: http::StatusCode) -> ErrorResponseStatus<'a>;
-    fn reply(reason: &'a str) -> Self;
-}
-
-impl<'a> RespExt<'a> for ErrorResponse<'a> {
-    fn with_status(self, status: http::StatusCode) -> ErrorResponseStatus<'a> {
-        ErrorResponseStatus {
-            error: self,
-            status,
-        }
-    }
-
-    fn reply(reason: &'a str) -> ErrorResponse<'a> {
-        Self {
-            reason: reason.into(),
-        }
-    }
-}
diff --git a/crates/atuin-server/src/handlers/record.rs b/crates/atuin-server/src/handlers/record.rs
deleted file mode 100644
index 1400a923..00000000
--- a/crates/atuin-server/src/handlers/record.rs
+++ /dev/null
@@ -1,45 +0,0 @@
-use axum::{Json, http::StatusCode, response::IntoResponse};
-use serde_json::json;
-use tracing::instrument;
-
-use super::{ErrorResponse, ErrorResponseStatus, RespExt};
-use crate::router::UserAuth;
-use atuin_server_database::Database;
-
-use atuin_common::record::{EncryptedData, Record};
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn post<DB: Database>(
-    UserAuth(user): UserAuth,
-) -> Result<(), ErrorResponseStatus<'static>> {
-    // anyone who has actually used the old record store (a very small number) will see this error
-    // upon trying to sync.
-    // 1. The status endpoint will say that the server has nothing
-    // 2. The client will try to upload local records
-    // 3. Sync will fail with this error
-
-    // If the client has no local records, they will see the empty index and do nothing. For the
-    // vast majority of users, this is the case.
-    return Err(
-        ErrorResponse::reply("record store deprecated; please upgrade")
-            .with_status(StatusCode::BAD_REQUEST),
-    );
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn index<DB: Database>(UserAuth(user): UserAuth) -> axum::response::Response {
-    let ret = json!({
-        "hosts": {}
-    });
-
-    ret.to_string().into_response()
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn next(
-    UserAuth(user): UserAuth,
-) -> Result<Json<Vec<Record<EncryptedData>>>, ErrorResponseStatus<'static>> {
-    let records = Vec::new();
-
-    Ok(Json(records))
-}
diff --git a/crates/atuin-server/src/handlers/status.rs b/crates/atuin-server/src/handlers/status.rs
deleted file mode 100644
index 9c152d51..00000000
--- a/crates/atuin-server/src/handlers/status.rs
+++ /dev/null
@@ -1,45 +0,0 @@
-use axum::{Json, extract::State, http::StatusCode};
-use tracing::instrument;
-
-use super::{ErrorResponse, ErrorResponseStatus, RespExt};
-use crate::router::{AppState, UserAuth};
-use atuin_server_database::Database;
-
-use atuin_common::api::*;
-
-const VERSION: &str = env!("CARGO_PKG_VERSION");
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn status<DB: Database>(
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-) -> Result<Json<StatusResponse>, ErrorResponseStatus<'static>> {
-    let db = &state.0.database;
-
-    let deleted = db.deleted_history(&user).await.unwrap_or(vec![]);
-
-    let count = match db.count_history_cached(&user).await {
-        // By default read out the cached value
-        Ok(count) => count,
-
-        // If that fails, fallback on a full COUNT. Cache is built on a POST
-        // only
-        Err(_) => match db.count_history(&user).await {
-            Ok(count) => count,
-            Err(_) => {
-                return Err(ErrorResponse::reply("failed to query history count")
-                    .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-            }
-        },
-    };
-
-    tracing::debug!(user = user.username, "requested sync status");
-
-    Ok(Json(StatusResponse {
-        count,
-        deleted,
-        username: user.username,
-        version: VERSION.to_string(),
-        page_size: state.settings.page_size,
-    }))
-}
diff --git a/crates/atuin-server/src/handlers/user.rs b/crates/atuin-server/src/handlers/user.rs
deleted file mode 100644
index 60956e6e..00000000
--- a/crates/atuin-server/src/handlers/user.rs
+++ /dev/null
@@ -1,368 +0,0 @@
-use std::borrow::Borrow;
-use std::collections::HashMap;
-use std::time::Duration;
-
-use argon2::{
-    Algorithm, Argon2, Params, PasswordHash, PasswordHasher, PasswordVerifier, Version,
-    password_hash::SaltString,
-};
-use axum::{
-    Json,
-    extract::{Path, State},
-    http::StatusCode,
-};
-use metrics::counter;
-
-use postmark::{Query, reqwest::PostmarkClient};
-
-use rand::rngs::OsRng;
-use tracing::{debug, error, info, instrument};
-
-use super::{ErrorResponse, ErrorResponseStatus, RespExt};
-use crate::router::{AppState, UserAuth};
-use atuin_server_database::{
-    Database, DbError,
-    models::{NewSession, NewUser},
-};
-
-use reqwest::header::CONTENT_TYPE;
-
-use atuin_common::{api::*, utils::crypto_random_string};
-
-pub fn verify_str(hash: &str, password: &str) -> bool {
-    let arg2 = Argon2::new(Algorithm::Argon2id, Version::V0x13, Params::default());
-    let Ok(hash) = PasswordHash::new(hash) else {
-        return false;
-    };
-    arg2.verify_password(password.as_bytes(), &hash).is_ok()
-}
-
-// Try to send a Discord webhook once - if it fails, we don't retry. "At most once", and best effort.
-// Don't return the status because if this fails, we don't really care.
-async fn send_register_hook(url: &str, username: String, registered: String) {
-    let hook = HashMap::from([
-        ("username", username),
-        ("content", format!("{registered} has just signed up!")),
-    ]);
-
-    let client = reqwest::Client::new();
-
-    let resp = client
-        .post(url)
-        .timeout(Duration::new(5, 0))
-        .header(CONTENT_TYPE, "application/json")
-        .json(&hook)
-        .send()
-        .await;
-
-    match resp {
-        Ok(_) => info!("register webhook sent ok!"),
-        Err(e) => error!("failed to send register webhook: {}", e),
-    }
-}
-
-#[instrument(skip_all, fields(user.username = username.as_str()))]
-pub async fn get<DB: Database>(
-    Path(username): Path<String>,
-    state: State<AppState<DB>>,
-) -> Result<Json<UserResponse>, ErrorResponseStatus<'static>> {
-    let db = &state.0.database;
-    let user = match db.get_user(username.as_ref()).await {
-        Ok(user) => user,
-        Err(DbError::NotFound) => {
-            debug!("user not found: {}", username);
-            return Err(ErrorResponse::reply("user not found").with_status(StatusCode::NOT_FOUND));
-        }
-        Err(DbError::Other(err)) => {
-            error!("database error: {}", err);
-            return Err(ErrorResponse::reply("database error")
-                .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-        }
-    };
-
-    Ok(Json(UserResponse {
-        username: user.username,
-    }))
-}
-
-#[instrument(skip_all)]
-pub async fn register<DB: Database>(
-    state: State<AppState<DB>>,
-    Json(register): Json<RegisterRequest>,
-) -> Result<Json<RegisterResponse>, ErrorResponseStatus<'static>> {
-    if !state.settings.open_registration {
-        return Err(
-            ErrorResponse::reply("this server is not open for registrations")
-                .with_status(StatusCode::BAD_REQUEST),
-        );
-    }
-
-    for c in register.username.chars() {
-        match c {
-            'a'..='z' | 'A'..='Z' | '0'..='9' | '-' => {}
-            _ => {
-                return Err(ErrorResponse::reply(
-                    "Only alphanumeric and hyphens (-) are allowed in usernames",
-                )
-                .with_status(StatusCode::BAD_REQUEST));
-            }
-        }
-    }
-
-    let hashed = hash_secret(&register.password);
-
-    let new_user = NewUser {
-        email: register.email.clone(),
-        username: register.username.clone(),
-        password: hashed,
-    };
-
-    let db = &state.0.database;
-    let user_id = match db.add_user(&new_user).await {
-        Ok(id) => id,
-        Err(e) => {
-            error!("failed to add user: {}", e);
-            return Err(
-                ErrorResponse::reply("failed to add user").with_status(StatusCode::BAD_REQUEST)
-            );
-        }
-    };
-
-    // 24 bytes encoded as base64
-    let token = crypto_random_string::<24>();
-
-    let new_session = NewSession {
-        user_id,
-        token: (&token).into(),
-    };
-
-    if let Some(url) = &state.settings.register_webhook_url {
-        // Could probs be run on another thread, but it's ok atm
-        send_register_hook(
-            url,
-            state.settings.register_webhook_username.clone(),
-            register.username,
-        )
-        .await;
-    }
-
-    counter!("atuin_users_registered", 1);
-
-    match db.add_session(&new_session).await {
-        Ok(_) => Ok(Json(RegisterResponse { session: token })),
-        Err(e) => {
-            error!("failed to add session: {}", e);
-            Err(ErrorResponse::reply("failed to register user")
-                .with_status(StatusCode::BAD_REQUEST))
-        }
-    }
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn delete<DB: Database>(
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-) -> Result<Json<DeleteUserResponse>, ErrorResponseStatus<'static>> {
-    debug!("request to delete user {}", user.id);
-
-    let db = &state.0.database;
-    if let Err(e) = db.delete_user(&user).await {
-        error!("failed to delete user: {}", e);
-
-        return Err(ErrorResponse::reply("failed to delete user")
-            .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-    };
-
-    counter!("atuin_users_deleted", 1);
-
-    Ok(Json(DeleteUserResponse {}))
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn send_verification<DB: Database>(
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-) -> Result<Json<SendVerificationResponse>, ErrorResponseStatus<'static>> {
-    let settings = state.0.settings;
-    debug!("request to verify user {}", user.username);
-
-    if !settings.mail.enabled {
-        return Ok(Json(SendVerificationResponse {
-            email_sent: false,
-            verified: false,
-        }));
-    }
-
-    if user.verified.is_some() {
-        return Ok(Json(SendVerificationResponse {
-            email_sent: false,
-            verified: true,
-        }));
-    }
-
-    // TODO: if we ever add another mail provider, can match on them all here.
-    let postmark_token = match settings.mail.postmark.token {
-        Some(token) => token,
-        _ => {
-            error!("Failed to verify email: got None for postmark token");
-            return Err(ErrorResponse::reply("mail not configured")
-                .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-        }
-    };
-
-    let db = &state.0.database;
-
-    let verification_token = db
-        .user_verification_token(user.id)
-        .await
-        .expect("Failed to verify");
-
-    debug!("Generated verification token, emailing user");
-
-    let client = PostmarkClient::builder()
-        .base_url("https://api.postmarkapp.com/")
-        .server_token(postmark_token)
-        .build();
-
-    let req = postmark::api::email::SendEmailRequest::builder()
-        .from(settings.mail.verification.from)
-        .subject(settings.mail.verification.subject)
-        .to(user.email)
-        .body(postmark::api::Body::text(format!(
-            "Please run the following command to finalize your Atuin account verification. It is valid for 15 minutes:\n\natuin account verify --token '{}'",
-            verification_token
-        )))
-        .build();
-
-    req.execute(&client)
-        .await
-        .expect("postmark email request failed");
-
-    debug!("Email sent");
-
-    Ok(Json(SendVerificationResponse {
-        email_sent: true,
-        verified: false,
-    }))
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn verify_user<DB: Database>(
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-    Json(token_request): Json<VerificationTokenRequest>,
-) -> Result<Json<VerificationTokenResponse>, ErrorResponseStatus<'static>> {
-    let db = state.0.database;
-
-    if user.verified.is_some() {
-        return Ok(Json(VerificationTokenResponse { verified: true }));
-    }
-
-    let token = db.user_verification_token(user.id).await.map_err(|e| {
-        error!("Failed to read user token: {e}");
-
-        ErrorResponse::reply("Failed to verify").with_status(StatusCode::INTERNAL_SERVER_ERROR)
-    })?;
-
-    if token_request.token == token {
-        db.verify_user(user.id).await.map_err(|e| {
-            error!("Failed to verify user: {e}");
-
-            ErrorResponse::reply("Failed to verify").with_status(StatusCode::INTERNAL_SERVER_ERROR)
-        })?;
-    } else {
-        info!(
-            "Incorrect verification token {} vs {}",
-            token_request.token, token
-        );
-
-        return Ok(Json(VerificationTokenResponse { verified: false }));
-    }
-
-    Ok(Json(VerificationTokenResponse { verified: true }))
-}
-
-#[instrument(skip_all, fields(user.id = user.id, change_password))]
-pub async fn change_password<DB: Database>(
-    UserAuth(mut user): UserAuth,
-    state: State<AppState<DB>>,
-    Json(change_password): Json<ChangePasswordRequest>,
-) -> Result<Json<ChangePasswordResponse>, ErrorResponseStatus<'static>> {
-    let db = &state.0.database;
-
-    let verified = verify_str(
-        user.password.as_str(),
-        change_password.current_password.borrow(),
-    );
-    if !verified {
-        return Err(
-            ErrorResponse::reply("password is not correct").with_status(StatusCode::UNAUTHORIZED)
-        );
-    }
-
-    let hashed = hash_secret(&change_password.new_password);
-    user.password = hashed;
-
-    if let Err(e) = db.update_user_password(&user).await {
-        error!("failed to change user password: {}", e);
-
-        return Err(ErrorResponse::reply("failed to change user password")
-            .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-    };
-    Ok(Json(ChangePasswordResponse {}))
-}
-
-#[instrument(skip_all, fields(user.username = login.username.as_str()))]
-pub async fn login<DB: Database>(
-    state: State<AppState<DB>>,
-    login: Json<LoginRequest>,
-) -> Result<Json<LoginResponse>, ErrorResponseStatus<'static>> {
-    let db = &state.0.database;
-    let user = match db.get_user(login.username.borrow()).await {
-        Ok(u) => u,
-        Err(DbError::NotFound) => {
-            return Err(ErrorResponse::reply("user not found").with_status(StatusCode::NOT_FOUND));
-        }
-        Err(DbError::Other(e)) => {
-            error!("failed to get user {}: {}", login.username.clone(), e);
-
-            return Err(ErrorResponse::reply("database error")
-                .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-        }
-    };
-
-    let session = match db.get_user_session(&user).await {
-        Ok(u) => u,
-        Err(DbError::NotFound) => {
-            debug!("user session not found for user id={}", user.id);
-            return Err(ErrorResponse::reply("user not found").with_status(StatusCode::NOT_FOUND));
-        }
-        Err(DbError::Other(err)) => {
-            error!("database error for user {}: {}", login.username, err);
-            return Err(ErrorResponse::reply("database error")
-                .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-        }
-    };
-
-    let verified = verify_str(user.password.as_str(), login.password.borrow());
-
-    if !verified {
-        debug!(user = user.username, "login failed");
-        return Err(
-            ErrorResponse::reply("password is not correct").with_status(StatusCode::UNAUTHORIZED)
-        );
-    }
-
-    debug!(user = user.username, "login success");
-
-    Ok(Json(LoginResponse {
-        session: session.token,
-    }))
-}
-
-fn hash_secret(password: &str) -> String {
-    let arg2 = Argon2::new(Algorithm::Argon2id, Version::V0x13, Params::default());
-    let salt = SaltString::generate(&mut OsRng);
-    let hash = arg2.hash_password(password.as_bytes(), &salt).unwrap();
-    hash.to_string()
-}
diff --git a/crates/atuin-server/src/handlers/v0/me.rs b/crates/atuin-server/src/handlers/v0/me.rs
deleted file mode 100644
index 7960b479..00000000
--- a/crates/atuin-server/src/handlers/v0/me.rs
+++ /dev/null
@@ -1,16 +0,0 @@
-use axum::Json;
-use tracing::instrument;
-
-use crate::handlers::ErrorResponseStatus;
-use crate::router::UserAuth;
-
-use atuin_common::api::*;
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn get(
-    UserAuth(user): UserAuth,
-) -> Result<Json<MeResponse>, ErrorResponseStatus<'static>> {
-    Ok(Json(MeResponse {
-        username: user.username,
-    }))
-}
diff --git a/crates/atuin-server/src/handlers/v0/mod.rs b/crates/atuin-server/src/handlers/v0/mod.rs
deleted file mode 100644
index d6f880f2..00000000
--- a/crates/atuin-server/src/handlers/v0/mod.rs
+++ /dev/null
@@ -1,3 +0,0 @@
-pub(crate) mod me;
-pub(crate) mod record;
-pub(crate) mod store;
diff --git a/crates/atuin-server/src/handlers/v0/record.rs b/crates/atuin-server/src/handlers/v0/record.rs
deleted file mode 100644
index 01b91599..00000000
--- a/crates/atuin-server/src/handlers/v0/record.rs
+++ /dev/null
@@ -1,114 +0,0 @@
-use axum::{Json, extract::Query, extract::State, http::StatusCode};
-use metrics::counter;
-use serde::Deserialize;
-use tracing::{error, instrument};
-
-use crate::{
-    handlers::{ErrorResponse, ErrorResponseStatus, RespExt},
-    router::{AppState, UserAuth},
-};
-use atuin_server_database::Database;
-
-use atuin_common::record::{EncryptedData, HostId, Record, RecordIdx, RecordStatus};
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn post<DB: Database>(
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-    Json(records): Json<Vec<Record<EncryptedData>>>,
-) -> Result<(), ErrorResponseStatus<'static>> {
-    let State(AppState { database, settings }) = state;
-
-    tracing::debug!(
-        count = records.len(),
-        user = user.username,
-        "request to add records"
-    );
-
-    counter!("atuin_record_uploaded", records.len() as u64);
-
-    let keep = records
-        .iter()
-        .all(|r| r.data.data.len() <= settings.max_record_size || settings.max_record_size == 0);
-
-    if !keep {
-        counter!("atuin_record_too_large", 1);
-
-        return Err(
-            ErrorResponse::reply("could not add records; record too large")
-                .with_status(StatusCode::BAD_REQUEST),
-        );
-    }
-
-    if let Err(e) = database.add_records(&user, &records).await {
-        error!("failed to add record: {}", e);
-
-        return Err(ErrorResponse::reply("failed to add record")
-            .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-    };
-
-    Ok(())
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn index<DB: Database>(
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-) -> Result<Json<RecordStatus>, ErrorResponseStatus<'static>> {
-    let State(AppState {
-        database,
-        settings: _,
-    }) = state;
-
-    let record_index = match database.status(&user).await {
-        Ok(index) => index,
-        Err(e) => {
-            error!("failed to get record index: {}", e);
-
-            return Err(ErrorResponse::reply("failed to calculate record index")
-                .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-        }
-    };
-
-    tracing::debug!(user = user.username, "record index request");
-
-    Ok(Json(record_index))
-}
-
-#[derive(Deserialize)]
-pub struct NextParams {
-    host: HostId,
-    tag: String,
-    start: Option<RecordIdx>,
-    count: u64,
-}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn next<DB: Database>(
-    params: Query<NextParams>,
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-) -> Result<Json<Vec<Record<EncryptedData>>>, ErrorResponseStatus<'static>> {
-    let State(AppState {
-        database,
-        settings: _,
-    }) = state;
-    let params = params.0;
-
-    let records = match database
-        .next_records(&user, params.host, params.tag, params.start, params.count)
-        .await
-    {
-        Ok(records) => records,
-        Err(e) => {
-            error!("failed to get record index: {}", e);
-
-            return Err(ErrorResponse::reply("failed to calculate record index")
-                .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-        }
-    };
-
-    counter!("atuin_record_downloaded", records.len() as u64);
-
-    Ok(Json(records))
-}
diff --git a/crates/atuin-server/src/handlers/v0/store.rs b/crates/atuin-server/src/handlers/v0/store.rs
deleted file mode 100644
index 941f2487..00000000
--- a/crates/atuin-server/src/handlers/v0/store.rs
+++ /dev/null
@@ -1,37 +0,0 @@
-use axum::{extract::Query, extract::State, http::StatusCode};
-use metrics::counter;
-use serde::Deserialize;
-use tracing::{error, instrument};
-
-use crate::{
-    handlers::{ErrorResponse, ErrorResponseStatus, RespExt},
-    router::{AppState, UserAuth},
-};
-use atuin_server_database::Database;
-
-#[derive(Deserialize)]
-pub struct DeleteParams {}
-
-#[instrument(skip_all, fields(user.id = user.id))]
-pub async fn delete<DB: Database>(
-    _params: Query<DeleteParams>,
-    UserAuth(user): UserAuth,
-    state: State<AppState<DB>>,
-) -> Result<(), ErrorResponseStatus<'static>> {
-    let State(AppState {
-        database,
-        settings: _,
-    }) = state;
-
-    if let Err(e) = database.delete_store(&user).await {
-        counter!("atuin_store_delete_failed", 1);
-        error!("failed to delete store {e:?}");
-
-        return Err(ErrorResponse::reply("failed to delete store")
-            .with_status(StatusCode::INTERNAL_SERVER_ERROR));
-    }
-
-    counter!("atuin_store_deleted", 1);
-
-    Ok(())
-}
diff --git a/crates/atuin-server/src/lib.rs b/crates/atuin-server/src/lib.rs
deleted file mode 100644
index 7a0e982b..00000000
--- a/crates/atuin-server/src/lib.rs
+++ /dev/null
@@ -1,146 +0,0 @@
-#![forbid(unsafe_code)]
-
-use std::future::Future;
-use std::net::SocketAddr;
-
-use atuin_server_database::Database;
-use axum::{Router, serve};
-use axum_server::Handle;
-use axum_server::tls_rustls::RustlsConfig;
-use eyre::{Context, Result, eyre};
-
-mod handlers;
-mod metrics;
-mod router;
-mod utils;
-
-pub use settings::Settings;
-pub use settings::example_config;
-
-pub mod settings;
-
-use tokio::net::TcpListener;
-use tokio::signal;
-
-#[cfg(target_family = "unix")]
-async fn shutdown_signal() {
-    let mut term = signal::unix::signal(signal::unix::SignalKind::terminate())
-        .expect("failed to register signal handler");
-    let mut interrupt = signal::unix::signal(signal::unix::SignalKind::interrupt())
-        .expect("failed to register signal handler");
-
-    tokio::select! {
-        _ = term.recv() => {},
-        _ = interrupt.recv() => {},
-    };
-    eprintln!("Shutting down gracefully...");
-}
-
-#[cfg(target_family = "windows")]
-async fn shutdown_signal() {
-    signal::windows::ctrl_c()
-        .expect("failed to register signal handler")
-        .recv()
-        .await;
-    eprintln!("Shutting down gracefully...");
-}
-
-pub async fn launch<Db: Database>(
-    settings: Settings<Db::Settings>,
-    addr: SocketAddr,
-) -> Result<()> {
-    if settings.tls.enable {
-        launch_with_tls::<Db>(settings, addr, shutdown_signal()).await
-    } else {
-        launch_with_tcp_listener::<Db>(
-            settings,
-            TcpListener::bind(addr)
-                .await
-                .context("could not connect to socket")?,
-            shutdown_signal(),
-        )
-        .await
-    }
-}
-
-pub async fn launch_with_tcp_listener<Db: Database>(
-    settings: Settings<Db::Settings>,
-    listener: TcpListener,
-    shutdown: impl Future<Output = ()> + Send + 'static,
-) -> Result<()> {
-    let r = make_router::<Db>(settings).await?;
-
-    serve(listener, r.into_make_service())
-        .with_graceful_shutdown(shutdown)
-        .await?;
-
-    Ok(())
-}
-
-async fn launch_with_tls<Db: Database>(
-    settings: Settings<Db::Settings>,
-    addr: SocketAddr,
-    shutdown: impl Future<Output = ()>,
-) -> Result<()> {
-    let crypto_provider = rustls::crypto::ring::default_provider().install_default();
-    if crypto_provider.is_err() {
-        return Err(eyre!("Failed to install default crypto provider"));
-    }
-    let rustls_config = RustlsConfig::from_pem_file(
-        settings.tls.cert_path.clone(),
-        settings.tls.pkey_path.clone(),
-    )
-    .await;
-    if rustls_config.is_err() {
-        return Err(eyre!("Failed to load TLS key and/or certificate"));
-    }
-    let rustls_config = rustls_config.unwrap();
-
-    let r = make_router::<Db>(settings).await?;
-
-    let handle = Handle::new();
-
-    let server = axum_server::bind_rustls(addr, rustls_config)
-        .handle(handle.clone())
-        .serve(r.into_make_service());
-
-    tokio::select! {
-        _ = server => {}
-        _ = shutdown => {
-            handle.graceful_shutdown(None);
-        }
-    }
-
-    Ok(())
-}
-
-// The separate listener means it's much easier to ensure metrics are not accidentally exposed to
-// the public.
-pub async fn launch_metrics_server(host: String, port: u16) -> Result<()> {
-    let listener = TcpListener::bind((host, port))
-        .await
-        .context("failed to bind metrics tcp")?;
-
-    let recorder_handle = metrics::setup_metrics_recorder();
-
-    let router = Router::new().route(
-        "/metrics",
-        axum::routing::get(move || std::future::ready(recorder_handle.render())),
-    );
-
-    serve(listener, router.into_make_service())
-        .with_graceful_shutdown(shutdown_signal())
-        .await?;
-
-    Ok(())
-}
-
-async fn make_router<Db: Database>(
-    settings: Settings<<Db as Database>::Settings>,
-) -> Result<Router, eyre::Error> {
-    let db = Db::new(&settings.db_settings)
-        .await
-        .wrap_err_with(|| format!("failed to connect to db: {:?}", settings.db_settings))?;
-    let r = router::router(db, settings);
-    Ok(r)
-}
diff --git a/crates/atuin-server/src/metrics.rs b/crates/atuin-server/src/metrics.rs
deleted file mode 100644
index ff0fe925..00000000
--- a/crates/atuin-server/src/metrics.rs
+++ /dev/null
@@ -1,55 +0,0 @@
-use std::time::Instant;
-
-use axum::{
-    extract::{MatchedPath, Request},
-    middleware::Next,
-    response::IntoResponse,
-};
-use metrics_exporter_prometheus::{Matcher, PrometheusBuilder, PrometheusHandle};
-
-pub fn setup_metrics_recorder() -> PrometheusHandle {
-    const EXPONENTIAL_SECONDS: &[f64] = &[
-        0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0,
-    ];
-
-    PrometheusBuilder::new()
-        .set_buckets_for_metric(
-            Matcher::Full("http_requests_duration_seconds".to_string()),
-            EXPONENTIAL_SECONDS,
-        )
-        .unwrap()
-        .install_recorder()
-        .unwrap()
-}
-
-/// Middleware to record some common HTTP metrics
-/// Generic over B to allow for arbitrary body types (eg Vec<u8>, Streams, a deserialized thing, etc)
-/// Someday tower-http might provide a metrics middleware: https://github.com/tower-rs/tower-http/issues/57
-pub async fn track_metrics(req: Request, next: Next) -> impl IntoResponse {
-    let start = Instant::now();
-
-    let path = match req.extensions().get::<MatchedPath>() {
-        Some(matched_path) => matched_path.as_str().to_owned(),
-        _ => req.uri().path().to_owned(),
-    };
-
-    let method = req.method().clone();
-
-    // Run the rest of the request handling first, so we can measure it and get response
-    // codes.
-    let response = next.run(req).await;
-
-    let latency = start.elapsed().as_secs_f64();
-    let status = response.status().as_u16().to_string();
-
-    let labels = [
-        ("method", method.to_string()),
-        ("path", path),
-        ("status", status),
-    ];
-
-    metrics::increment_counter!("http_requests_total", &labels);
-    metrics::histogram!("http_requests_duration_seconds", latency, &labels);
-
-    response
-}
diff --git a/crates/atuin-server/src/router.rs b/crates/atuin-server/src/router.rs
deleted file mode 100644
index ae63e1e8..00000000
--- a/crates/atuin-server/src/router.rs
+++ /dev/null
@@ -1,155 +0,0 @@
-use async_trait::async_trait;
-use atuin_common::api::{ATUIN_CARGO_VERSION, ATUIN_HEADER_VERSION, ErrorResponse};
-use axum::{
-    Router,
-    extract::{FromRequestParts, Request},
-    http::{self, request::Parts},
-    middleware::Next,
-    response::{IntoResponse, Response},
-    routing::{delete, get, patch, post},
-};
-use eyre::Result;
-use tower::ServiceBuilder;
-use tower_http::trace::TraceLayer;
-
-use super::handlers;
-use crate::{
-    handlers::{ErrorResponseStatus, RespExt},
-    metrics,
-    settings::Settings,
-};
-use atuin_server_database::{Database, DbError, models::User};
-
-pub struct UserAuth(pub User);
-
-#[async_trait]
-impl<DB: Send + Sync> FromRequestParts<AppState<DB>> for UserAuth
-where
-    DB: Database,
-{
-    type Rejection = ErrorResponseStatus<'static>;
-
-    async fn from_request_parts(
-        req: &mut Parts,
-        state: &AppState<DB>,
-    ) -> Result<Self, Self::Rejection> {
-        let auth_header = req
-            .headers
-            .get(http::header::AUTHORIZATION)
-            .ok_or_else(|| {
-                ErrorResponse::reply("missing authorization header")
-                    .with_status(http::StatusCode::BAD_REQUEST)
-            })?;
-        let auth_header = auth_header.to_str().map_err(|_| {
-            ErrorResponse::reply("invalid authorization header encoding")
-                .with_status(http::StatusCode::BAD_REQUEST)
-        })?;
-        let (typ, token) = auth_header.split_once(' ').ok_or_else(|| {
-            ErrorResponse::reply("invalid authorization header encoding")
-                .with_status(http::StatusCode::BAD_REQUEST)
-        })?;
-
-        if typ != "Token" {
-            return Err(
-                ErrorResponse::reply("invalid authorization header encoding")
-                    .with_status(http::StatusCode::BAD_REQUEST),
-            );
-        }
-
-        let user = state
-            .database
-            .get_session_user(token)
-            .await
-            .map_err(|e| match e {
-                DbError::NotFound => ErrorResponse::reply("session not found")
-                    .with_status(http::StatusCode::FORBIDDEN),
-                DbError::Other(e) => {
-                    tracing::error!(error = ?e, "could not query user session");
-                    ErrorResponse::reply("could not query user session")
-                        .with_status(http::StatusCode::INTERNAL_SERVER_ERROR)
-                }
-            })?;
-
-        Ok(UserAuth(user))
-    }
-}
-
-async fn teapot() -> impl IntoResponse {
-    // This used to return 418: 
-    // Much as it was fun, it wasn't as useful or informative as it should be
-    (http::StatusCode::NOT_FOUND, "404 not found")
-}
-
-async fn clacks_overhead(request: Request, next: Next) -> Response {
-    let mut response = next.run(request).await;
-
-    let gnu_terry_value = "GNU Terry Pratchett, Kris Nova";
-    let gnu_terry_header = "X-Clacks-Overhead";
-
-    response
-        .headers_mut()
-        .insert(gnu_terry_header, gnu_terry_value.parse().unwrap());
-    response
-}
-
-/// Ensure that we only try and sync with clients on the same major version
-async fn semver(request: Request, next: Next) -> Response {
-    let mut response = next.run(request).await;
-    response
-        .headers_mut()
-        .insert(ATUIN_HEADER_VERSION, ATUIN_CARGO_VERSION.parse().unwrap());
-
-    response
-}
-
-#[derive(Clone)]
-pub struct AppState<DB: Database> {
-    pub database: DB,
-    pub settings: Settings<DB::Settings>,
-}
-
-pub fn router<DB: Database>(database: DB, settings: Settings<DB::Settings>) -> Router {
-    let routes = Router::new()
-        .route("/", get(handlers::index))
-        .route("/healthz", get(handlers::health::health_check))
-        .route("/sync/count", get(handlers::history::count))
-        .route("/sync/history", get(handlers::history::list))
-        .route("/sync/calendar/:focus", get(handlers::history::calendar))
-        .route("/sync/status", get(handlers::status::status))
-        .route("/history", post(handlers::history::add))
-        .route("/history", delete(handlers::history::delete))
-        .route("/user/:username", get(handlers::user::get))
-        .route("/account", delete(handlers::user::delete))
-        .route("/account/password", patch(handlers::user::change_password))
-        .route("/register", post(handlers::user::register))
-        .route("/login", post(handlers::user::login))
-        .route("/record", post(handlers::record::post::<DB>))
-        .route("/record", get(handlers::record::index::<DB>))
-        .route("/record/next", get(handlers::record::next))
-        .route("/api/v0/me", get(handlers::v0::me::get))
-        .route("/api/v0/account/verify", post(handlers::user::verify_user))
-        .route(
-            "/api/v0/account/send-verification",
-            post(handlers::user::send_verification),
-        )
-        .route("/api/v0/record", post(handlers::v0::record::post))
-        .route("/api/v0/record", get(handlers::v0::record::index))
-        .route("/api/v0/record/next", get(handlers::v0::record::next))
-        .route("/api/v0/store", delete(handlers::v0::store::delete));
-
-    let path = settings.path.as_str();
-    if path.is_empty() {
-        routes
-    } else {
-        Router::new().nest(path, routes)
-    }
-    .fallback(teapot)
-    .with_state(AppState { database, settings })
-    .layer(
-        ServiceBuilder::new()
-            .layer(axum::middleware::from_fn(clacks_overhead))
-            .layer(TraceLayer::new_for_http())
-            .layer(axum::middleware::from_fn(metrics::track_metrics))
-            .layer(axum::middleware::from_fn(semver)),
-    )
-}
diff --git a/crates/atuin-server/src/settings.rs b/crates/atuin-server/src/settings.rs
deleted file mode 100644
index d5070dae..00000000
--- a/crates/atuin-server/src/settings.rs
+++ /dev/null
@@ -1,149 +0,0 @@
-use std::{io::prelude::*, path::PathBuf};
-
-use config::{Config, Environment, File as ConfigFile, FileFormat};
-use eyre::{Result, eyre};
-use fs_err::{File, create_dir_all};
-use serde::{Deserialize, Serialize, de::DeserializeOwned};
-
-static EXAMPLE_CONFIG: &str = include_str!("../server.toml");
-
-#[derive(Default, Clone, Debug, Deserialize, Serialize)]
-pub struct Mail {
-    #[serde(alias = "enable")]
-    pub enabled: bool,
-
-    /// Configuration for the postmark api client
-    /// This is what we use for Atuin Cloud, the forum, etc.
-    #[serde(default)]
-    pub postmark: Postmark,
-
-    #[serde(default)]
-    pub verification: MailVerification,
-}
-
-#[derive(Default, Clone, Debug, Deserialize, Serialize)]
-pub struct Postmark {
-    #[serde(alias = "token")]
-    pub token: Option<String>,
-}
-
-#[derive(Default, Clone, Debug, Deserialize, Serialize)]
-pub struct MailVerification {
-    #[serde(alias = "enable")]
-    pub from: String,
-    pub subject: String,
-}
-
-#[derive(Clone, Debug, Deserialize, Serialize)]
-pub struct Metrics {
-    #[serde(alias = "enabled")]
-    pub enable: bool,
-    pub host: String,
-    pub port: u16,
-}
-
-impl Default for Metrics {
-    fn default() -> Self {
-        Self {
-            enable: false,
-            host: String::from("127.0.0.1"),
-            port: 9001,
-        }
-    }
-}
-
-#[derive(Clone, Debug, Deserialize, Serialize)]
-pub struct Settings<DbSettings> {
-    pub host: String,
-    pub port: u16,
-    pub path: String,
-    pub open_registration: bool,
-    pub max_history_length: usize,
-    pub max_record_size: usize,
-    pub page_size: i64,
-    pub register_webhook_url: Option<String>,
-    pub register_webhook_username: String,
-    pub metrics: Metrics,
-    pub tls: Tls,
-    pub mail: Mail,
-
-    /// Advertise a version that is not what we are _actually_ running
-    /// Many clients compare their version with api.atuin.sh, and if they differ, notify the user
-    /// that an update is available.
-    /// Now that we take beta releases, we should be able to advertise a different version to avoid
-    /// notifying users when the server runs something that is not a stable release.
-    pub fake_version: Option<String>,
-
-    #[serde(flatten)]
-    pub db_settings: DbSettings,
-}
-
-impl<DbSettings: DeserializeOwned> Settings<DbSettings> {
-    pub fn new() -> Result<Self> {
-        let mut config_file = if let Ok(p) = std::env::var("ATUIN_CONFIG_DIR") {
-            PathBuf::from(p)
-        } else {
-            let mut config_file = PathBuf::new();
-            let config_dir = atuin_common::utils::config_dir();
-            config_file.push(config_dir);
-            config_file
-        };
-
-        config_file.push("server.toml");
-
-        // create the config file if it does not exist
-        let mut config_builder = Config::builder()
-            .set_default("host", "127.0.0.1")?
-            .set_default("port", 8888)?
-            .set_default("open_registration", false)?
-            .set_default("max_history_length", 8192)?
-            .set_default("max_record_size", 1024 * 1024 * 1024)? // pretty chonky
-            .set_default("path", "")?
-            .set_default("register_webhook_username", "")?
-            .set_default("page_size", 1100)?
-            .set_default("metrics.enable", false)?
-            .set_default("metrics.host", "127.0.0.1")?
-            .set_default("metrics.port", 9001)?
-            .set_default("mail.enable", false)?
-            .set_default("tls.enable", false)?
-            .set_default("tls.cert_path", "")?
-            .set_default("tls.pkey_path", "")?
-            .add_source(
-                Environment::with_prefix("atuin")
-                    .prefix_separator("_")
-                    .separator("__"),
-            );
-
-        config_builder = if config_file.exists() {
-            config_builder.add_source(ConfigFile::new(
-                config_file.to_str().unwrap(),
-                FileFormat::Toml,
-            ))
-        } else {
-            create_dir_all(config_file.parent().unwrap())?;
-            let mut file = File::create(config_file)?;
-            file.write_all(EXAMPLE_CONFIG.as_bytes())?;
-
-            config_builder
-        };
-
-        let config = config_builder.build()?;
-
-        config
-            .try_deserialize()
-            .map_err(|e| eyre!("failed to deserialize: {}", e))
-    }
-}
-
-pub fn example_config() -> &'static str {
-    EXAMPLE_CONFIG
-}
-
-#[derive(Clone, Debug, Default, Deserialize, Serialize)]
-pub struct Tls {
-    #[serde(alias = "enabled")]
-    pub enable: bool,
-
-    pub cert_path: PathBuf,
-    pub pkey_path: PathBuf,
-}
diff --git a/crates/atuin-server/src/utils.rs b/crates/atuin-server/src/utils.rs
deleted file mode 100644
index 12e9ac1b..00000000
--- a/crates/atuin-server/src/utils.rs
+++ /dev/null
@@ -1,15 +0,0 @@
-use eyre::Result;
-use semver::{Version, VersionReq};
-
-pub fn client_version_min(user_agent: &str, req: &str) -> Result<bool> {
-    if user_agent.is_empty() {
-        return Ok(false);
-    }
-
-    let version = user_agent.replace("atuin/", "");
-
-    let req = VersionReq::parse(req)?;
-    let version = Version::parse(version.as_str())?;
-
-    Ok(req.matches(&version))
-}
diff --git a/crates/atuin/Cargo.toml b/crates/atuin/Cargo.toml
index 487df85a..743ec0eb 100644
--- a/crates/atuin/Cargo.toml
+++ b/crates/atuin/Cargo.toml
@@ -33,17 +33,14 @@ buildflags = ["--release"]
 atuin = { path = "/usr/bin/atuin" }
 
 [features]
-default = ["client", "sync", "server", "clipboard", "check-update", "daemon"]
+default = ["client", "clipboard", "check-update", "daemon"]
 client = ["atuin-client"]
 sync = ["atuin-client/sync"]
 daemon = ["atuin-client/daemon", "atuin-daemon"]
-server = ["atuin-server", "atuin-server-postgres"]
 clipboard = ["arboard"]
 check-update = ["atuin-client/check-update"]
 
 [dependencies]
-atuin-server-postgres = { path = "../atuin-server-postgres", version = "18.6.1", optional = true }
-atuin-server = { path = "../atuin-server", version = "18.6.1", optional = true }
 atuin-client = { path = "../atuin-client", version = "18.6.1", optional = true, default-features = false }
 atuin-common = { path = "../atuin-common", version = "18.6.1" }
 atuin-dotfiles = { path = "../atuin-dotfiles", version = "18.6.1" }
diff --git a/crates/atuin/src/command/client.rs b/crates/atuin/src/command/client.rs
index 330fef0c..0d8140e8 100644
--- a/crates/atuin/src/command/client.rs
+++ b/crates/atuin/src/command/client.rs
@@ -8,12 +8,6 @@ use atuin_client::{
 };
 use tracing_subscriber::{filter::EnvFilter, fmt, prelude::*};
 
-#[cfg(feature = "sync")]
-mod sync;
-
-#[cfg(feature = "sync")]
-mod account;
-
 #[cfg(feature = "daemon")]
 mod daemon;
 
@@ -48,14 +42,6 @@ pub enum Cmd {
     /// Interactive history search
     Search(search::Cmd),
 
-    #[cfg(feature = "sync")]
-    #[command(flatten)]
-    Sync(sync::Cmd),
-
-    /// Manage your sync account
-    #[cfg(feature = "sync")]
-    Account(account::Cmd),
-
     /// Get or set small key-value pairs
     #[command(subcommand)]
     Kv(kv::Cmd),
@@ -152,12 +138,6 @@ impl Cmd {
             Self::Stats(stats) => stats.run(&db, &settings, theme).await,
             Self::Search(search) => search.run(db, &mut settings, sqlite_store, theme).await,
 
-            #[cfg(feature = "sync")]
-            Self::Sync(sync) => sync.run(settings, &db, sqlite_store).await,
-
-            #[cfg(feature = "sync")]
-            Self::Account(account) => account.run(settings, sqlite_store).await,
-
             Self::Kv(kv) => kv.run(&settings, &sqlite_store).await,
 
             Self::Store(store) => store.run(&settings, &db, sqlite_store).await,
diff --git a/crates/atuin/src/command/client/account.rs b/crates/atuin/src/command/client/account.rs
deleted file mode 100644
index 011d7c69..00000000
--- a/crates/atuin/src/command/client/account.rs
+++ /dev/null
@@ -1,52 +0,0 @@
-use clap::{Args, Subcommand};
-use eyre::Result;
-
-use atuin_client::record::sqlite_store::SqliteStore;
-use atuin_client::settings::Settings;
-
-pub mod change_password;
-pub mod delete;
-pub mod login;
-pub mod logout;
-pub mod register;
-pub mod verify;
-
-#[derive(Args, Debug)]
-pub struct Cmd {
-    #[command(subcommand)]
-    command: Commands,
-}
-
-#[derive(Subcommand, Debug)]
-pub enum Commands {
-    /// Login to the configured server
-    Login(login::Cmd),
-
-    /// Register a new account
-    Register(register::Cmd),
-
-    /// Log out
-    Logout,
-
-    /// Delete your account, and all synced data
-    Delete,
-
-    /// Change your password
-    ChangePassword(change_password::Cmd),
-
-    /// Verify your account
-    Verify(verify::Cmd),
-}
-
-impl Cmd {
-    pub async fn run(self, settings: Settings, store: SqliteStore) -> Result<()> {
-        match self.command {
-            Commands::Login(l) => l.run(&settings, &store).await,
-            Commands::Register(r) => r.run(&settings).await,
-            Commands::Logout => logout::run(&settings),
-            Commands::Delete => delete::run(&settings).await,
-            Commands::ChangePassword(c) => c.run(&settings).await,
-            Commands::Verify(c) => c.run(&settings).await,
-        }
-    }
-}
diff --git a/crates/atuin/src/command/client/account/change_password.rs b/crates/atuin/src/command/client/account/change_password.rs
deleted file mode 100644
index a91495db..00000000
--- a/crates/atuin/src/command/client/account/change_password.rs
+++ /dev/null
@@ -1,57 +0,0 @@
-use clap::Parser;
-use eyre::{Result, bail};
-
-use atuin_client::{api_client, settings::Settings};
-use rpassword::prompt_password;
-
-#[derive(Parser, Debug)]
-pub struct Cmd {
-    #[clap(long, short)]
-    pub current_password: Option<String>,
-
-    #[clap(long, short)]
-    pub new_password: Option<String>,
-}
-
-impl Cmd {
-    pub async fn run(self, settings: &Settings) -> Result<()> {
-        run(settings, self.current_password, self.new_password).await
-    }
-}
-
-pub async fn run(
-    settings: &Settings,
-    current_password: Option<String>,
-    new_password: Option<String>,
-) -> Result<()> {
-    let client = api_client::Client::new(
-        &settings.sync_address,
-        settings.session_token()?.as_str(),
-        settings.network_connect_timeout,
-        settings.network_timeout,
-    )?;
-
-    let current_password = current_password.clone().unwrap_or_else(|| {
-        prompt_password("Please enter the current password: ").expect("Failed to read from input")
-    });
-
-    if current_password.is_empty() {
-        bail!("please provide the current password");
-    }
-
-    let new_password = new_password.clone().unwrap_or_else(|| {
-        prompt_password("Please enter the new password: ").expect("Failed to read from input")
-    });
-
-    if new_password.is_empty() {
-        bail!("please provide a new password");
-    }
-
-    client
-        .change_password(current_password, new_password)
-        .await?;
-
-    println!("Account password successfully changed!");
-
-    Ok(())
-}
diff --git a/crates/atuin/src/command/client/account/delete.rs b/crates/atuin/src/command/client/account/delete.rs
deleted file mode 100644
index 3e2db33c..00000000
--- a/crates/atuin/src/command/client/account/delete.rs
+++ /dev/null
@@ -1,30 +0,0 @@
-use atuin_client::{api_client, settings::Settings};
-use eyre::{Result, bail};
-use std::fs::remove_file;
-use std::path::PathBuf;
-
-pub async fn run(settings: &Settings) -> Result<()> {
-    let session_path = settings.session_path.as_str();
-
-    if !PathBuf::from(session_path).exists() {
-        bail!("You are not logged in");
-    }
-
-    let client = api_client::Client::new(
-        &settings.sync_address,
-        settings.session_token()?.as_str(),
-        settings.network_connect_timeout,
-        settings.network_timeout,
-    )?;
-
-    client.delete().await?;
-
-    // Fixes stale session+key when account is deleted via CLI.
-    if PathBuf::from(session_path).exists() {
-        remove_file(PathBuf::from(session_path))?;
-    }
-
-    println!("Your account is deleted");
-
-    Ok(())
-}
diff --git a/crates/atuin/src/command/client/account/login.rs b/crates/atuin/src/command/client/account/login.rs
deleted file mode 100644
index f569fe25..00000000
--- a/crates/atuin/src/command/client/account/login.rs
+++ /dev/null
@@ -1,199 +0,0 @@
-use std::{io, path::PathBuf};
-
-use clap::Parser;
-use eyre::{Context, Result, bail};
-use tokio::{fs::File, io::AsyncWriteExt};
-
-use atuin_client::{
-    api_client,
-    encryption::{Key, decode_key, encode_key, load_key},
-    record::sqlite_store::SqliteStore,
-    record::store::Store,
-    settings::Settings,
-};
-use atuin_common::api::LoginRequest;
-use rpassword::prompt_password;
-
-#[derive(Parser, Debug)]
-pub struct Cmd {
-    #[clap(long, short)]
-    pub username: Option<String>,
-
-    #[clap(long, short)]
-    pub password: Option<String>,
-
-    /// The encryption key for your account
-    #[clap(long, short)]
-    pub key: Option<String>,
-}
-
-fn get_input() -> Result<String> {
-    let mut input = String::new();
-    io::stdin().read_line(&mut input)?;
-    Ok(input.trim_end_matches(&['\r', '\n'][..]).to_string())
-}
-
-impl Cmd {
-    pub async fn run(&self, settings: &Settings, store: &SqliteStore) -> Result<()> {
-        // TODO(ellie): Replace this with a call to atuin_client::login::login
-        // The reason I haven't done this yet is that this implementation allows for
-        // an empty key. This will use an existing key file.
-        //
-        // I'd quite like to ditch that behaviour, so have not brought it into the library
-        // function.
-        if settings.logged_in() {
-            println!(
-                "You are already logged in! Please run 'atuin logout' if you wish to login again"
-            );
-
-            return Ok(());
-        }
-
-        let username = or_user_input(self.username.clone(), "username");
-        let password = self.password.clone().unwrap_or_else(read_user_password);
-
-        let key_path = settings.key_path.as_str();
-        let key_path = PathBuf::from(key_path);
-
-        println!("IMPORTANT");
-        println!(
-            "If you are already logged in on another machine, you must ensure that the key you use here is the same as the key you used there."
-        );
-        println!("You can find your key by running 'atuin key' on the other machine");
-        println!("Do not share this key with anyone");
-        println!("\nRead more here: https://docs.atuin.sh/guide/sync/#login \n");
-
-        let key = or_user_input(
-            self.key.clone(),
-            "encryption key [blank to use existing key file]",
-        );
-
-        // if provided, the key may be EITHER base64, or a bip mnemonic
-        // try to normalize on base64
-        let key = if key.is_empty() {
-            key
-        } else {
-            // try parse the key as a mnemonic...
-            match bip39::Mnemonic::from_phrase(&key, bip39::Language::English) {
-                Ok(mnemonic) => encode_key(Key::from_slice(mnemonic.entropy()))?,
-                Err(err) => {
-                    match err.downcast_ref::<bip39::ErrorKind>() {
-                        Some(err) => {
-                            match err {
-                                // assume they copied in the base64 key
-                                bip39::ErrorKind::InvalidWord => key,
-                                bip39::ErrorKind::InvalidChecksum => {
-                                    bail!("key mnemonic was not valid")
-                                }
-                                bip39::ErrorKind::InvalidKeysize(_)
-                                | bip39::ErrorKind::InvalidWordLength(_)
-                                | bip39::ErrorKind::InvalidEntropyLength(_, _) => {
-                                    bail!("key was not the correct length")
-                                }
-                            }
-                        }
-                        _ => {
-                            // unknown error. assume they copied the base64 key
-                            key
-                        }
-                    }
-                }
-            }
-        };
-
-        // I've simplified this a little, but it could really do with a refactor
-        // Annoyingly, it's also very important to get it correct
-        if key.is_empty() {
-            if key_path.exists() {
-                let bytes = fs_err::read_to_string(key_path)
-                    .context("existing key file couldn't be read")?;
-                if decode_key(bytes).is_err() {
-                    bail!("the key in existing key file was invalid");
-                }
-            } else {
-                panic!(
-                    "No key provided. Please use 'atuin key' on your other machine, or recover your key from a backup."
-                )
-            }
-        } else if !key_path.exists() {
-            if decode_key(key.clone()).is_err() {
-                bail!("the specified key was invalid");
-            }
-
-            let mut file = File::create(key_path).await?;
-            file.write_all(key.as_bytes()).await?;
-        } else {
-            // we now know that the user has logged in specifying a key, AND that the key path
-            // exists
-
-            // 1. check if the saved key and the provided key match. if so, nothing to do.
-            // 2. if not, re-encrypt the local history and overwrite the key
-            let current_key: [u8; 32] = load_key(settings)?.into();
-
-            let encoded = key.clone(); // gonna want to save it in a bit
-            let new_key: [u8; 32] = decode_key(key)
-                .context("could not decode provided key - is not valid base64")?
-                .into();
-
-            if new_key != current_key {
-                println!("\nRe-encrypting local store with new key");
-
-                store.re_encrypt(&current_key, &new_key).await?;
-
-                println!("Writing new key");
-                let mut file = File::create(key_path).await?;
-                file.write_all(encoded.as_bytes()).await?;
-            }
-        }
-
-        let session = api_client::login(
-            settings.sync_address.as_str(),
-            LoginRequest { username, password },
-        )
-        .await?;
-
-        let session_path = settings.session_path.as_str();
-        let mut file = File::create(session_path).await?;
-        file.write_all(session.session.as_bytes()).await?;
-
-        println!("Logged in!");
-
-        Ok(())
-    }
-}
-
-pub(super) fn or_user_input(value: Option<String>, name: &'static str) -> String {
-    value.unwrap_or_else(|| read_user_input(name))
-}
-
-pub(super) fn read_user_password() -> String {
-    let password = prompt_password("Please enter password: ");
-    password.expect("Failed to read from input")
-}
-
-fn read_user_input(name: &'static str) -> String {
-    eprint!("Please enter {name}: ");
-    get_input().expect("Failed to read from input")
-}
-
-#[cfg(test)]
-mod tests {
-    use atuin_client::encryption::Key;
-
-    #[test]
-    fn mnemonic_round_trip() {
-        let key = Key::from([
-            3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9, 3, 2, 3, 8, 4, 6, 2, 6, 4, 3, 3, 8, 3, 2,
-            7, 9, 5,
-        ]);
-        let phrase = bip39::Mnemonic::from_entropy(&key, bip39::Language::English)
-            .unwrap()
-            .into_phrase();
-        let mnemonic = bip39::Mnemonic::from_phrase(&phrase, bip39::Language::English).unwrap();
-        assert_eq!(mnemonic.entropy(), key.as_slice());
-        assert_eq!(
-            phrase,
-            "adapt amused able anxiety mother adapt beef gaze amount else seat alcohol cage lottery avoid scare alcohol cactus school avoid coral adjust catch pink"
-        );
-    }
-}
diff --git a/crates/atuin/src/command/client/account/logout.rs b/crates/atuin/src/command/client/account/logout.rs
deleted file mode 100644
index 836360a1..00000000
--- a/crates/atuin/src/command/client/account/logout.rs
+++ /dev/null
@@ -1,6 +0,0 @@
-use atuin_client::settings::Settings;
-use eyre::Result;
-
-pub fn run(settings: &Settings) -> Result<()> {
-    atuin_client::logout::logout(settings)
-}
diff --git a/crates/atuin/src/command/client/account/register.rs b/crates/atuin/src/command/client/account/register.rs
deleted file mode 100644
index 80c4c29c..00000000
--- a/crates/atuin/src/command/client/account/register.rs
+++ /dev/null
@@ -1,62 +0,0 @@
-use clap::Parser;
-use eyre::{Result, bail};
-use tokio::{fs::File, io::AsyncWriteExt};
-
-use atuin_client::{api_client, settings::Settings};
-
-#[derive(Parser, Debug)]
-pub struct Cmd {
-    #[clap(long, short)]
-    pub username: Option<String>,
-
-    #[clap(long, short)]
-    pub password: Option<String>,
-
-    #[clap(long, short)]
-    pub email: Option<String>,
-}
-
-impl Cmd {
-    pub async fn run(self, settings: &Settings) -> Result<()> {
-        run(settings, self.username, self.email, self.password).await
-    }
-}
-
-pub async fn run(
-    settings: &Settings,
-    username: Option<String>,
-    email: Option<String>,
-    password: Option<String>,
-) -> Result<()> {
-    use super::login::or_user_input;
-    println!("Registering for an Atuin Sync account");
-
-    let username = or_user_input(username, "username");
-    let email = or_user_input(email, "email");
-
-    let password = password
-        .clone()
-        .unwrap_or_else(super::login::read_user_password);
-
-    if password.is_empty() {
-        bail!("please provide a password");
-    }
-
-    let session =
-        api_client::register(settings.sync_address.as_str(), &username, &email, &password).await?;
-
-    let path = settings.session_path.as_str();
-    let mut file = File::create(path).await?;
-    file.write_all(session.session.as_bytes()).await?;
-
-    let _key = atuin_client::encryption::load_key(settings)?;
-
-    println!(
-        "Registration successful! Please make a note of your key (run 'atuin key') and keep it safe."
-    );
-    println!(
-        "You will need it to log in on other devices, and we cannot help recover it if you lose it."
-    );
-
-    Ok(())
-}
diff --git a/crates/atuin/src/command/client/account/verify.rs b/crates/atuin/src/command/client/account/verify.rs
deleted file mode 100644
index 7c707117..00000000
--- a/crates/atuin/src/command/client/account/verify.rs
+++ /dev/null
@@ -1,51 +0,0 @@
-use clap::Parser;
-use eyre::Result;
-
-use atuin_client::{api_client, settings::Settings};
-
-#[derive(Parser, Debug)]
-pub struct Cmd {
-    #[clap(long, short)]
-    pub token: Option<String>,
-}
-
-impl Cmd {
-    pub async fn run(self, settings: &Settings) -> Result<()> {
-        run(settings, self.token).await
-    }
-}
-
-pub async fn run(settings: &Settings, token: Option<String>) -> Result<()> {
-    let client = api_client::Client::new(
-        &settings.sync_address,
-        settings.session_token()?.as_str(),
-        settings.network_connect_timeout,
-        settings.network_timeout,
-    )?;
-
-    let (email_sent, verified) = client.verify(token).await?;
-
-    match (email_sent, verified) {
-        (true, false) => {
-            println!("Verification sent! Please check your inbox");
-        }
-
-        (false, true) => {
-            println!("Your account is verified");
-        }
-
-        (false, false) => {
-            println!(
-                "Your Atuin server does not have mail setup. This is not required, though your account cannot be verified. Speak to your admin."
-            );
-        }
-
-        _ => {
-            println!(
-                "Invalid email and verification status. This is a bug. Please open an issue: https://github.com/atuinsh/atuin"
-            );
-        }
-    }
-
-    Ok(())
-}
diff --git a/crates/atuin/src/command/client/doctor.rs b/crates/atuin/src/command/client/doctor.rs
index 24e41d3d..693d74ad 100644
--- a/crates/atuin/src/command/client/doctor.rs
+++ b/crates/atuin/src/command/client/doctor.rs
@@ -1,5 +1,5 @@
 use std::process::Command;
-use std::{env, path::PathBuf, str::FromStr};
+use std::{env, str::FromStr};
 
 use atuin_client::database::Sqlite;
 use atuin_client::settings::Settings;
@@ -234,34 +234,11 @@ impl SystemInfo {
     }
 }
 
-#[derive(Debug, Serialize)]
-struct SyncInfo {
-    /// Whether the main Atuin sync server is in use
-    /// I'm just calling it Atuin Cloud for lack of a better name atm
-    pub cloud: bool,
-    pub records: bool,
-    pub auto_sync: bool,
-
-    pub last_sync: String,
-}
-
-impl SyncInfo {
-    pub fn new(settings: &Settings) -> Self {
-        Self {
-            cloud: settings.sync_address == "https://api.atuin.sh",
-            auto_sync: settings.auto_sync,
-            records: settings.sync.records,
-            last_sync: Settings::last_sync().map_or("no last sync".to_string(), |v| v.to_string()),
-        }
-    }
-}
-
 #[derive(Debug)]
 struct SettingPaths {
     db: String,
     record_store: String,
     key: String,
-    session: String,
 }
 
 impl SettingPaths {
@@ -270,7 +247,6 @@ impl SettingPaths {
             db: settings.db_path.clone(),
             record_store: settings.record_store_path.clone(),
             key: settings.key_path.clone(),
-            session: settings.session_path.clone(),
         }
     }
 
@@ -279,7 +255,6 @@ impl SettingPaths {
             ("ATUIN_DB_PATH", &self.db),
             ("ATUIN_RECORD_STORE", &self.record_store),
             ("ATUIN_KEY", &self.key),
-            ("ATUIN_SESSION", &self.session),
         ];
 
         for (path_env_var, path) in paths {
@@ -296,10 +271,6 @@ impl SettingPaths {
 struct AtuinInfo {
     pub version: String,
 
-    /// Whether the main Atuin sync server is in use
-    /// I'm just calling it Atuin Cloud for lack of a better name atm
-    pub sync: Option<SyncInfo>,
-
     pub sqlite_version: String,
 
     #[serde(skip)] // probably unnecessary to expose this
@@ -308,15 +279,6 @@ struct AtuinInfo {
 
 impl AtuinInfo {
     pub async fn new(settings: &Settings) -> Self {
-        let session_path = settings.session_path.as_str();
-        let logged_in = PathBuf::from(session_path).exists();
-
-        let sync = if logged_in {
-            Some(SyncInfo::new(settings))
-        } else {
-            None
-        };
-
         let sqlite_version = match Sqlite::new("sqlite::memory:", 0.1).await {
             Ok(db) => db
                 .sqlite_version()
@@ -327,7 +289,6 @@ impl AtuinInfo {
 
         Self {
             version: crate::VERSION.to_string(),
-            sync,
             sqlite_version,
             setting_paths: SettingPaths::new(settings),
         }
diff --git a/crates/atuin/src/command/client/history.rs b/crates/atuin/src/command/client/history.rs
index 80d40f58..1101366e 100644
--- a/crates/atuin/src/command/client/history.rs
+++ b/crates/atuin/src/command/client/history.rs
@@ -21,9 +21,6 @@ use atuin_client::{
     },
 };
 
-#[cfg(feature = "sync")]
-use atuin_client::{record, sync};
-
 use log::{debug, warn};
 use time::{OffsetDateTime, macros::format_description};
 
@@ -421,25 +418,6 @@ impl Cmd {
         db.update(&h).await?;
         history_store.push(h).await?;
 
-        if settings.should_sync()? {
-            #[cfg(feature = "sync")]
-            {
-                if settings.sync.records {
-                    let (_, downloaded) = record::sync::sync(settings, &store).await?;
-                    Settings::save_sync_time()?;
-
-                    crate::sync::build(settings, &store, db, Some(&downloaded)).await?;
-                } else {
-                    debug!("running periodic background sync");
-                    sync::sync(settings, false, db).await?;
-                }
-            }
-            #[cfg(not(feature = "sync"))]
-            debug!("not compiled with sync support");
-        } else {
-            debug!("sync disabled! not syncing");
-        }
-
         Ok(())
     }
 
@@ -548,12 +526,7 @@ impl Cmd {
 
             for entry in matches {
                 eprintln!("deleting {}", entry.id);
-                if settings.sync.records {
-                    let (id, _) = history_store.delete(entry.id.clone()).await?;
-                    history_store.incremental_build(db, &[id]).await?;
-                } else {
-                    db.delete(entry.clone()).await?;
-                }
+                db.delete(entry.clone()).await?;
             }
         }
         Ok(())
@@ -588,20 +561,9 @@ impl Cmd {
                 settings.timezone,
             );
         } else {
-            let encryption_key: [u8; 32] = encryption::load_key(settings)
-                .context("could not load encryption key")?
-                .into();
-            let host_id = Settings::host_id().expect("failed to get host_id");
-            let history_store = HistoryStore::new(store.clone(), host_id, encryption_key);
-
             for entry in matches {
                 eprintln!("deleting {}", entry.id);
-                if settings.sync.records {
-                    let (id, _) = history_store.delete(entry.id).await?;
-                    history_store.incremental_build(db, &[id]).await?;
-                } else {
-                    db.delete(entry).await?;
-                }
+                db.delete(entry).await?;
             }
         }
         Ok(())
diff --git a/crates/atuin/src/command/client/info.rs b/crates/atuin/src/command/client/info.rs
index 60ba1fe6..82d80a64 100644
--- a/crates/atuin/src/command/client/info.rs
+++ b/crates/atuin/src/command/client/info.rs
@@ -10,12 +10,11 @@ pub fn run(settings: &Settings) {
     sever_config.push("server.toml");
 
     let config_paths = format!(
-        "Config files:\nclient config: {:?}\nserver config: {:?}\nclient db path: {:?}\nkey path: {:?}\nsession path: {:?}",
+        "Config files:\nclient config: {:?}\nserver config: {:?}\nclient db path: {:?}\nkey path: {:?}",
         config_file.to_string_lossy(),
         sever_config.to_string_lossy(),
         settings.db_path,
         settings.key_path,
-        settings.session_path
     );
 
     let env_vars = format!(
diff --git a/crates/atuin/src/command/client/search.rs b/crates/atuin/src/command/client/search.rs
index 8c864e77..81a9eb63 100644
--- a/crates/atuin/src/command/client/search.rs
+++ b/crates/atuin/src/command/client/search.rs
@@ -247,13 +247,7 @@ impl Cmd {
                 while !entries.is_empty() {
                     for entry in &entries {
                         eprintln!("deleting {}", entry.id);
-
-                        if settings.sync.records {
-                            let (id, _) = history_store.delete(entry.id.clone()).await?;
-                            history_store.incremental_build(&db, &[id]).await?;
-                        } else {
-                            db.delete(entry.clone()).await?;
-                        }
+                        db.delete(entry.clone()).await?;
                     }
 
                     entries =
diff --git a/crates/atuin/src/command/client/search/interactive.rs b/crates/atuin/src/command/client/search/interactive.rs
index 0fd7cbb6..571a0fdb 100644
--- a/crates/atuin/src/command/client/search/interactive.rs
+++ b/crates/atuin/src/command/client/search/interactive.rs
@@ -1155,12 +1155,7 @@ pub async fn history(
 
                                 let entry = results.remove(index);
 
-                                if settings.sync.records {
-                                    let (id, _) = history_store.delete(entry.id).await?;
-                                    history_store.incremental_build(&db, &[id]).await?;
-                                } else {
-                                    db.delete(entry.clone()).await?;
-                                }
+                                db.delete(entry.clone()).await?;
 
                                 app.tab_index  = 0;
                             },
diff --git a/crates/atuin/src/command/client/store.rs b/crates/atuin/src/command/client/store.rs
index 63029ee1..01d8fe8c 100644
--- a/crates/atuin/src/command/client/store.rs
+++ b/crates/atuin/src/command/client/store.rs
@@ -9,12 +9,6 @@ use atuin_client::{
 use itertools::Itertools;
 use time::{OffsetDateTime, UtcOffset};
 
-#[cfg(feature = "sync")]
-mod push;
-
-#[cfg(feature = "sync")]
-mod pull;
-
 mod purge;
 mod rebuild;
 mod rekey;
@@ -37,14 +31,6 @@ pub enum Cmd {
 
     /// Verify that all records in the store can be decrypted with the current key
     Verify(verify::Verify),
-
-    /// Push all records to the remote sync server (one way sync)
-    #[cfg(feature = "sync")]
-    Push(push::Push),
-
-    /// Pull records from the remote sync server (one way sync)
-    #[cfg(feature = "sync")]
-    Pull(pull::Pull),
 }
 
 impl Cmd {
@@ -60,12 +46,6 @@ impl Cmd {
             Self::Rekey(rekey) => rekey.run(settings, store).await,
             Self::Verify(verify) => verify.run(settings, store).await,
             Self::Purge(purge) => purge.run(settings, store).await,
-
-            #[cfg(feature = "sync")]
-            Self::Push(push) => push.run(settings, store).await,
-
-            #[cfg(feature = "sync")]
-            Self::Pull(pull) => pull.run(settings, store, database).await,
         }
     }
 
diff --git a/crates/atuin/src/command/client/store/pull.rs b/crates/atuin/src/command/client/store/pull.rs
deleted file mode 100644
index 36450fbf..00000000
--- a/crates/atuin/src/command/client/store/pull.rs
+++ /dev/null
@@ -1,78 +0,0 @@
-use clap::Args;
-use eyre::Result;
-
-use atuin_client::{
-    database::Database,
-    record::store::Store,
-    record::sync::Operation,
-    record::{sqlite_store::SqliteStore, sync},
-    settings::Settings,
-};
-
-#[derive(Args, Debug)]
-pub struct Pull {
-    /// The tag to push (eg, 'history'). Defaults to all tags
-    #[arg(long, short)]
-    pub tag: Option<String>,
-
-    /// Force push records
-    /// This will first wipe the local store, and then download all records from the remote
-    #[arg(long, default_value = "false")]
-    pub force: bool,
-}
-
-impl Pull {
-    pub async fn run(
-        &self,
-        settings: &Settings,
-        store: SqliteStore,
-        db: &dyn Database,
-    ) -> Result<()> {
-        if self.force {
-            println!("Forcing local overwrite!");
-            println!("Clearing local store");
-
-            store.delete_all().await?;
-        }
-
-        // We can actually just use the existing diff/etc to push
-        // 1. Diff
-        // 2. Get operations
-        // 3. Filter operations by
-        //  a) are they a download op?
-        //  b) are they for the host/tag we are pushing here?
-        let (diff, _) = sync::diff(settings, &store).await?;
-        let operations = sync::operations(diff, &store).await?;
-
-        let operations = operations
-            .into_iter()
-            .filter(|op| match op {
-                // No noops or downloads thx
-                Operation::Noop { .. } | Operation::Upload { .. } => false,
-
-                // pull, so yes plz to downloads!
-                Operation::Download { tag, .. } => {
-                    if self.force {
-                        return true;
-                    }
-
-                    if let Some(t) = self.tag.clone() {
-                        if t != *tag {
-                            return false;
-                        }
-                    }
-
-                    true
-                }
-            })
-            .collect();
-
-        let (_, downloaded) = sync::sync_remote(operations, &store, settings).await?;
-
-        println!("Downloaded {} records", downloaded.len());
-
-        crate::sync::build(settings, &store, db, Some(&downloaded)).await?;
-
-        Ok(())
-    }
-}
diff --git a/crates/atuin/src/command/client/store/push.rs b/crates/atuin/src/command/client/store/push.rs
deleted file mode 100644
index e1d80ef7..00000000
--- a/crates/atuin/src/command/client/store/push.rs
+++ /dev/null
@@ -1,96 +0,0 @@
-use atuin_common::record::HostId;
-use clap::Args;
-use eyre::Result;
-use uuid::Uuid;
-
-use atuin_client::{
-    api_client::Client,
-    record::sync::Operation,
-    record::{sqlite_store::SqliteStore, sync},
-    settings::Settings,
-};
-
-#[derive(Args, Debug)]
-pub struct Push {
-    /// The tag to push (eg, 'history'). Defaults to all tags
-    #[arg(long, short)]
-    pub tag: Option<String>,
-
-    /// The host to push, in the form of a UUID host ID. Defaults to the current host.
-    #[arg(long)]
-    pub host: Option<Uuid>,
-
-    /// Force push records
-    /// This will override both host and tag, to be all hosts and all tags. First clear the remote store, then upload all of the
-    /// local store
-    #[arg(long, default_value = "false")]
-    pub force: bool,
-}
-
-impl Push {
-    pub async fn run(&self, settings: &Settings, store: SqliteStore) -> Result<()> {
-        let host_id = Settings::host_id().expect("failed to get host_id");
-
-        if self.force {
-            println!("Forcing remote store overwrite!");
-            println!("Clearing remote store");
-
-            let client = Client::new(
-                &settings.sync_address,
-                settings.session_token()?.as_str(),
-                settings.network_connect_timeout,
-                settings.network_timeout * 10, // we may be deleting a lot of data... so up the
-                                               // timeout
-            )
-            .expect("failed to create client");
-
-            client.delete_store().await?;
-        }
-
-        // We can actually just use the existing diff/etc to push
-        // 1. Diff
-        // 2. Get operations
-        // 3. Filter operations by
-        //  a) are they an upload op?
-        //  b) are they for the host/tag we are pushing here?
-        let (diff, _) = sync::diff(settings, &store).await?;
-        let operations = sync::operations(diff, &store).await?;
-
-        let operations = operations
-            .into_iter()
-            .filter(|op| match op {
-                // No noops or downloads thx
-                Operation::Noop { .. } | Operation::Download { .. } => false,
-
-                // push, so yes plz to uploads!
-                Operation::Upload { host, tag, .. } => {
-                    if self.force {
-                        return true;
-                    }
-
-                    if let Some(h) = self.host {
-                        if HostId(h) != *host {
-                            return false;
-                        }
-                    } else if *host != host_id {
-                        return false;
-                    }
-
-                    if let Some(t) = self.tag.clone() {
-                        if t != *tag {
-                            return false;
-                        }
-                    }
-
-                    true
-                }
-            })
-            .collect();
-
-        let (uploaded, _) = sync::sync_remote(operations, &store, settings).await?;
-
-        println!("Uploaded {uploaded} records");
-
-        Ok(())
-    }
-}
diff --git a/crates/atuin/src/command/client/sync.rs b/crates/atuin/src/command/client/sync.rs
deleted file mode 100644
index be1bf6d2..00000000
--- a/crates/atuin/src/command/client/sync.rs
+++ /dev/null
@@ -1,131 +0,0 @@
-use clap::Subcommand;
-use eyre::{Result, WrapErr};
-
-use atuin_client::{
-    database::Database,
-    encryption,
-    history::store::HistoryStore,
-    record::{sqlite_store::SqliteStore, store::Store, sync},
-    settings::Settings,
-};
-
-mod status;
-
-use crate::command::client::account;
-
-#[derive(Subcommand, Debug)]
-#[command(infer_subcommands = true)]
-pub enum Cmd {
-    /// Sync with the configured server
-    Sync {
-        /// Force re-download everything
-        #[arg(long, short)]
-        force: bool,
-    },
-
-    /// Login to the configured server
-    Login(account::login::Cmd),
-
-    /// Log out
-    Logout,
-
-    /// Register with the configured server
-    Register(account::register::Cmd),
-
-    /// Print the encryption key for transfer to another machine
-    Key {
-        /// Switch to base64 output of the key
-        #[arg(long)]
-        base64: bool,
-    },
-
-    /// Display the sync status
-    Status,
-}
-
-impl Cmd {
-    pub async fn run(
-        self,
-        settings: Settings,
-        db: &impl Database,
-        store: SqliteStore,
-    ) -> Result<()> {
-        match self {
-            Self::Sync { force } => run(&settings, force, db, store).await,
-            Self::Login(l) => l.run(&settings, &store).await,
-            Self::Logout => account::logout::run(&settings),
-            Self::Register(r) => r.run(&settings).await,
-            Self::Status => status::run(&settings, db).await,
-            Self::Key { base64 } => {
-                use atuin_client::encryption::{encode_key, load_key};
-                let key = load_key(&settings).wrap_err("could not load encryption key")?;
-
-                if base64 {
-                    let encode = encode_key(&key).wrap_err("could not encode encryption key")?;
-                    println!("{encode}");
-                } else {
-                    let mnemonic = bip39::Mnemonic::from_entropy(&key, bip39::Language::English)
-                        .map_err(|_| eyre::eyre!("invalid key"))?;
-                    println!("{mnemonic}");
-                }
-                Ok(())
-            }
-        }
-    }
-}
-
-async fn run(
-    settings: &Settings,
-    force: bool,
-    db: &impl Database,
-    store: SqliteStore,
-) -> Result<()> {
-    if settings.sync.records {
-        let encryption_key: [u8; 32] = encryption::load_key(settings)
-            .context("could not load encryption key")?
-            .into();
-
-        let host_id = Settings::host_id().expect("failed to get host_id");
-        let history_store = HistoryStore::new(store.clone(), host_id, encryption_key);
-
-        let (uploaded, downloaded) = sync::sync(settings, &store).await?;
-
-        crate::sync::build(settings, &store, db, Some(&downloaded)).await?;
-
-        println!("{uploaded}/{} up/down to record store", downloaded.len());
-
-        let history_length = db.history_count(true).await?;
-        let store_history_length = store.len_tag("history").await?;
-
-        #[allow(clippy::cast_sign_loss)]
-        if history_length as u64 > store_history_length {
-            println!(
-                "{history_length} in history index, but {store_history_length} in history store"
-            );
-            println!("Running automatic history store init...");
-
-            // Internally we use the global filter mode, so this context is ignored.
-            // don't recurse or loop here.
-            history_store.init_store(db).await?;
-
-            println!("Re-running sync due to new records locally");
-
-            // we'll want to run sync once more, as there will now be stuff to upload
-            let (uploaded, downloaded) = sync::sync(settings, &store).await?;
-
-            crate::sync::build(settings, &store, db, Some(&downloaded)).await?;
-
-            println!("{uploaded}/{} up/down to record store", downloaded.len());
-        }
-    } else {
-        atuin_client::sync::sync(settings, force, db).await?;
-    }
-
-    println!(
-        "Sync complete! {} items in history database, force: {}",
-        db.history_count(true).await?,
-        force
-    );
-
-    Ok(())
-}
diff --git a/crates/atuin/src/command/client/sync/status.rs b/crates/atuin/src/command/client/sync/status.rs
deleted file mode 100644
index f59ef5b9..00000000
--- a/crates/atuin/src/command/client/sync/status.rs
+++ /dev/null
@@ -1,51 +0,0 @@
-use std::path::PathBuf;
-
-use crate::{SHA, VERSION};
-use atuin_client::{api_client, database::Database, settings::Settings};
-use colored::Colorize;
-use eyre::Result;
-
-pub async fn run(settings: &Settings, db: &impl Database) -> Result<()> {
-    let session_path = settings.session_path.as_str();
-
-    if !PathBuf::from(session_path).exists() {
-        println!("You are not logged in to a sync server - cannot show sync status");
-
-        return Ok(());
-    }
-
-    let client = api_client::Client::new(
-        &settings.sync_address,
-        settings.session_token()?.as_str(),
-        settings.network_connect_timeout,
-        settings.network_timeout,
-    )?;
-
-    let status = client.status().await?;
-    let last_sync = Settings::last_sync()?;
-
-    println!("Atuin v{VERSION} - Build rev {SHA}\n");
-
-    println!("{}", "[Local]".green());
-
-    if settings.auto_sync {
-        println!("Sync frequency: {}", settings.sync_frequency);
-        println!("Last sync: {}", last_sync.to_offset(settings.timezone.0));
-    }
-
-    if !settings.sync.records {
-        let local_count = db.history_count(false).await?;
-        let deleted_count = db.history_count(true).await? - local_count;
-
-        println!("History count: {local_count}");
-        println!("Deleted history count: {deleted_count}\n");
-    }
-
-    if settings.auto_sync {
-        println!("{}", "[Remote]".green());
-        println!("Address: {}", settings.sync_address);
-        println!("Username: {}", status.username);
-    }
-
-    Ok(())
-}
diff --git a/crates/atuin/src/main.rs b/crates/atuin/src/main.rs
index eaa58664..0dfe6fae 100644
--- a/crates/atuin/src/main.rs
+++ b/crates/atuin/src/main.rs
@@ -8,9 +8,6 @@ use command::AtuinCmd;
 
 mod command;
 
-#[cfg(feature = "sync")]
-mod sync;
-
 const VERSION: &str = env!("CARGO_PKG_VERSION");
 const SHA: &str = env!("GIT_HASH");
 
diff --git a/crates/atuin/src/sync.rs b/crates/atuin/src/sync.rs
deleted file mode 100644
index b95b78b9..00000000
--- a/crates/atuin/src/sync.rs
+++ /dev/null
@@ -1,50 +0,0 @@
-use atuin_dotfiles::store::{AliasStore, var::VarStore};
-use atuin_scripts::store::ScriptStore;
-use eyre::{Context, Result};
-
-use atuin_client::{
-    database::Database, history::store::HistoryStore, record::sqlite_store::SqliteStore,
-    settings::Settings,
-};
-use atuin_common::record::RecordId;
-use atuin_kv::store::KvStore;
-
-// This is the only crate that ties together all other crates.
-// Therefore, it's the only crate where functions tying together all stores can live
-
-/// Rebuild all stores after a sync
-/// Note: for history, this only does an _incremental_ sync. Hence the need to specify downloaded
-/// records.
-pub async fn build(
-    settings: &Settings,
-    store: &SqliteStore,
-    db: &dyn Database,
-    downloaded: Option<&[RecordId]>,
-) -> Result<()> {
-    let encryption_key: [u8; 32] = atuin_client::encryption::load_key(settings)
-        .context("could not load encryption key")?
-        .into();
-
-    let host_id = Settings::host_id().expect("failed to get host_id");
-
-    let downloaded = downloaded.unwrap_or(&[]);
-
-    let kv_db = atuin_kv::database::Database::new(settings.kv.db_path.clone(), 1.0).await?;
-
-    let history_store = HistoryStore::new(store.clone(), host_id, encryption_key);
-    let alias_store = AliasStore::new(store.clone(), host_id, encryption_key);
-    let var_store = VarStore::new(store.clone(), host_id, encryption_key);
-    let kv_store = KvStore::new(store.clone(), kv_db, host_id, encryption_key);
-    let script_store = ScriptStore::new(store.clone(), host_id, encryption_key);
-
-    history_store.incremental_build(db, downloaded).await?;
-
-    alias_store.build().await?;
-    var_store.build().await?;
-    kv_store.build().await?;
-
-    let script_db =
-        atuin_scripts::database::Database::new(settings.scripts.db_path.clone(), 1.0).await?;
-    script_store.build(script_db).await?;
-    Ok(())
-}
diff --git a/crates/atuin/tests/common/mod.rs b/crates/atuin/tests/common/mod.rs
deleted file mode 100644
index f947d164..00000000
--- a/crates/atuin/tests/common/mod.rs
+++ /dev/null
@@ -1,102 +0,0 @@
-use std::{env, time::Duration};
-
-use atuin_client::api_client;
-use atuin_common::utils::uuid_v7;
-use atuin_server::{Settings as ServerSettings, launch_with_tcp_listener};
-use atuin_server_postgres::{Postgres, PostgresSettings};
-use futures_util::TryFutureExt;
-use tokio::{net::TcpListener, sync::oneshot, task::JoinHandle};
-use tracing::{Dispatch, dispatcher};
-use tracing_subscriber::{EnvFilter, layer::SubscriberExt};
-
-pub async fn start_server(path: &str) -> (String, oneshot::Sender<()>, JoinHandle<()>) {
-    let formatting_layer = tracing_tree::HierarchicalLayer::default()
-        .with_writer(tracing_subscriber::fmt::TestWriter::new())
-        .with_indent_lines(true)
-        .with_ansi(true)
-        .with_targets(true)
-        .with_indent_amount(2);
-
-    let dispatch: Dispatch = tracing_subscriber::registry()
-        .with(formatting_layer)
-        .with(EnvFilter::new("atuin_server=debug,atuin_client=debug,info"))
-        .into();
-
-    let db_uri = env::var("ATUIN_DB_URI")
-        .unwrap_or_else(|_| "postgres://atuin:pass@localhost:5432/atuin".to_owned());
-
-    let server_settings = ServerSettings {
-        host: "127.0.0.1".to_owned(),
-        port: 0,
-        path: path.to_owned(),
-        open_registration: true,
-        max_history_length: 8192,
-        max_record_size: 1024 * 1024 * 1024,
-        page_size: 1100,
-        register_webhook_url: None,
-        register_webhook_username: String::new(),
-        db_settings: PostgresSettings { db_uri },
-        metrics: atuin_server::settings::Metrics::default(),
-        tls: atuin_server::settings::Tls::default(),
-        mail: atuin_server::settings::Mail::default(),
-        fake_version: None,
-    };
-
-    let (shutdown_tx, shutdown_rx) = tokio::sync::oneshot::channel();
-    let listener = TcpListener::bind("127.0.0.1:0").await.unwrap();
-    let addr = listener.local_addr().unwrap();
-    let server = tokio::spawn(async move {
-        let _tracing_guard = dispatcher::set_default(&dispatch);
-
-        if let Err(e) = launch_with_tcp_listener::<Postgres>(
-            server_settings,
-            listener,
-            shutdown_rx.unwrap_or_else(|_| ()),
-        )
-        .await
-        {
-            tracing::error!(error=?e, "server error");
-            panic!("error running server: {e:?}");
-        }
-    });
-
-    // let the server come online
-    tokio::time::sleep(Duration::from_millis(200)).await;
-
-    (format!("http://{addr}{path}"), shutdown_tx, server)
-}
-
-pub async fn register_inner<'a>(
-    address: &'a str,
-    username: &str,
-    password: &str,
-) -> api_client::Client<'a> {
-    let email = format!("{}@example.com", uuid_v7().as_simple());
-
-    // registration works
-    let registration_response = api_client::register(address, username, &email, password)
-        .await
-        .unwrap();
-
-    api_client::Client::new(address, &registration_response.session, 5, 30).unwrap()
-}
-
-#[allow(dead_code)]
-pub async fn login(address: &str, username: String, password: String) -> api_client::Client<'_> {
-    // registration works
-    let login_response = api_client::login(
-        address,
-        atuin_common::api::LoginRequest { username, password },
-    )
-    .await
-    .unwrap();
-
-    api_client::Client::new(address, &login_response.session, 5, 30).unwrap()
-}
-
-#[allow(dead_code)]
-pub async fn register(address: &str) -> api_client::Client<'_> {
-    let username = uuid_v7().as_simple().to_string();
-    let password = uuid_v7().as_simple().to_string();
-    register_inner(address, &username, &password).await
-}
diff --git a/crates/atuin/tests/sync.rs b/crates/atuin/tests/sync.rs
deleted file mode 100644
index 7e25d1c2..00000000
--- a/crates/atuin/tests/sync.rs
+++ /dev/null
@@ -1,45 +0,0 @@
-use atuin_common::{api::AddHistoryRequest, utils::uuid_v7};
-use time::OffsetDateTime;
-
-mod common;
-
-#[tokio::test]
-async fn sync() {
-    let path = format!("/{}", uuid_v7().as_simple());
-    let (address, shutdown, server) = common::start_server(&path).await;
-
-    let client = common::register(&address).await;
-    let hostname = uuid_v7().as_simple().to_string();
-    let now = OffsetDateTime::now_utc();
-
-    let data1 = uuid_v7().as_simple().to_string();
-    let data2 = uuid_v7().as_simple().to_string();
-
-    client
-        .post_history(&[
-            AddHistoryRequest {
-                id: uuid_v7().as_simple().to_string(),
-                timestamp: now,
-                data: data1.clone(),
-                hostname: hostname.clone(),
-            },
-            AddHistoryRequest {
-                id: uuid_v7().as_simple().to_string(),
-                timestamp: now,
-                data: data2.clone(),
-                hostname: hostname.clone(),
-            },
-        ])
-        .await
-        .unwrap();
-
-    let history = client
-        .get_history(OffsetDateTime::UNIX_EPOCH, OffsetDateTime::UNIX_EPOCH, None)
-        .await
-        .unwrap();
-
-    assert_eq!(history.history, vec![data1, data2]);
-
-    shutdown.send(()).unwrap();
-    server.await.unwrap();
-}
diff --git a/crates/atuin/tests/users.rs b/crates/atuin/tests/users.rs
deleted file mode 100644
index 95fb533b..00000000
--- a/crates/atuin/tests/users.rs
+++ /dev/null
@@ -1,121 +0,0 @@
-use atuin_common::utils::uuid_v7;
-
-mod common;
-
-#[tokio::test]
-async fn registration() {
-    let path = format!("/{}", uuid_v7().as_simple());
-    let (address, shutdown, server) = common::start_server(&path).await;
-    dbg!(&address);
-
-    // -- REGISTRATION --
-
-    let username = uuid_v7().as_simple().to_string();
-    let password = uuid_v7().as_simple().to_string();
-    let client = common::register_inner(&address, &username, &password).await;
-
-    // the session token works
-    let status = client.status().await.unwrap();
-    assert_eq!(status.username, username);
-
-    // -- LOGIN --
-
-    let client = common::login(&address, username.clone(), password).await;
-
-    // the session token works
-    let status = client.status().await.unwrap();
-    assert_eq!(status.username, username);
-
-    shutdown.send(()).unwrap();
-    server.await.unwrap();
-}
-
-#[tokio::test]
-async fn change_password() {
-    let path = format!("/{}", uuid_v7().as_simple());
-    let (address, shutdown, server) = common::start_server(&path).await;
-
-    // -- REGISTRATION --
-
-    let username = uuid_v7().as_simple().to_string();
-    let password = uuid_v7().as_simple().to_string();
-    let client = common::register_inner(&address, &username, &password).await;
-
-    // the session token works
-    let status = client.status().await.unwrap();
-    assert_eq!(status.username, username);
-
-    // -- PASSWORD CHANGE --
-
-    let current_password = password;
-    let new_password = uuid_v7().as_simple().to_string();
-    let result = client
-        .change_password(current_password, new_password.clone())
-        .await;
-
-    // the password change request succeeded
-    assert!(result.is_ok());
-
-    // -- LOGIN --
-
-    let client = common::login(&address, username.clone(), new_password).await;
-
-    // login with new password yields a working token
-    let status = client.status().await.unwrap();
-    assert_eq!(status.username, username);
-
-    shutdown.send(()).unwrap();
-    server.await.unwrap();
-}
-
-#[tokio::test]
-async fn multi_user_test() {
-    let path = format!("/{}", uuid_v7().as_simple());
-    let (address, shutdown, server) = common::start_server(&path).await;
-    dbg!(&address);
-
-    // -- REGISTRATION --
-
-    let user_one = uuid_v7().as_simple().to_string();
-    let password_one = uuid_v7().as_simple().to_string();
-    let client_one = common::register_inner(&address, &user_one, &password_one).await;
-
-    // the session token works
-    let status = client_one.status().await.unwrap();
-    assert_eq!(status.username, user_one);
-
-    let user_two = uuid_v7().as_simple().to_string();
-    let password_two = uuid_v7().as_simple().to_string();
-    let client_two = common::register_inner(&address, &user_two, &password_two).await;
-
-    // the session token works
-    let status = client_two.status().await.unwrap();
-    assert_eq!(status.username, user_two);
-
-    // check that we can change user one's password, and _this does not affect user two_
-
-    let current_password = password_one;
-    let new_password = uuid_v7().as_simple().to_string();
-    let result = client_one
-        .change_password(current_password, new_password.clone())
-        .await;
-
-    // the password change request succeeded
-    assert!(result.is_ok());
-
-    // -- LOGIN --
-
-    let client_one = common::login(&address, user_one.clone(), new_password).await;
-    let client_two = common::login(&address, user_two.clone(), password_two).await;
-
-    // login with new password yields a working token
-    let status = client_one.status().await.unwrap();
-    assert_eq!(status.username, user_one);
-    assert_ne!(status.username, user_two);
-
-    let status = client_two.status().await.unwrap();
-    assert_eq!(status.username, user_two);
-
-    shutdown.send(()).unwrap();
-    server.await.unwrap();
-}
